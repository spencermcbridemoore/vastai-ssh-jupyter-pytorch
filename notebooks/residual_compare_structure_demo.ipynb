{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Residual compare structure demo\n",
        "\n",
        "This notebook shows light-weight inspection patterns for the large\n",
        "`notebooks/h200_long_outputs/physics_A/qwen-0_5b/residual_compare_20251129_053550_Qwen_Qwen2_5_0_5B_Instruct.json`\n",
        "file. The goal is to understand its structure using Python helpers while\n",
        "avoiding massive prints or full data dumps.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import json\n",
        "import itertools as it\n",
        "from pprint import pprint\n",
        "from typing import Any, Mapping, Sequence\n",
        "\n",
        "from src.analysis.residual_results import loader\n",
        "\n",
        "DATA_PATH = Path(\n",
        "    \"notebooks/h200_long_outputs/physics_A/qwen-0_5b/\"\n",
        "    \"residual_compare_20251129_053550_Qwen_Qwen2_5_0_5B_Instruct.json\"\n",
        ").resolve()\n",
        "\n",
        "if not DATA_PATH.exists():\n",
        "    raise FileNotFoundError(DATA_PATH)\n",
        "\n",
        "size_mb = DATA_PATH.stat().st_size / (1024 ** 2)\n",
        "print(f\"Data path: {DATA_PATH}\")\n",
        "print(f\"File size: {size_mb:.2f} MiB\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Raw JSON peek\n",
        "\n",
        "The next helpers read a tiny slice of the JSON file and summarize what is\n",
        "available (keys, token counts, metadata keys, etc.). They explicitly cap\n",
        "how many tokens or layers are displayed so the notebook stays readable.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_small_sample(path: Path, max_items: int = 2):\n",
        "    \"\"\"Return up to `max_items` entries plus the total record count.\"\"\"\n",
        "    with path.open(\"r\", encoding=\"utf-8\") as fh:\n",
        "        payload = json.load(fh)\n",
        "    if not isinstance(payload, list):\n",
        "        raise TypeError(\"Expected a list of records at the top level.\")\n",
        "    return payload[:max_items], len(payload)\n",
        "\n",
        "\n",
        "def describe_record(record: Mapping[str, Any], token_limit: int = 8) -> Mapping[str, Any]:\n",
        "    \"\"\"Extract a compact summary from a single raw record.\"\"\"\n",
        "    tokens = record.get(\"tokens\", [])\n",
        "    metadata = record.get(\"metadata\", {})\n",
        "    layers = record.get(\"layers\", [])\n",
        "    base_swap = record.get(\"base_swap\", {})\n",
        "    sft_swap = record.get(\"sft_swap\", {})\n",
        "    return {\n",
        "        \"available_keys\": sorted(record.keys()),\n",
        "        \"token_count\": len(tokens),\n",
        "        \"token_preview\": tokens[:token_limit],\n",
        "        \"metadata_keys\": sorted(metadata.keys()),\n",
        "        \"layer_count\": len(layers),\n",
        "        \"layer_indices\": [layer.get(\"layer_index\") for layer in layers[:3]],\n",
        "        \"base_swap_sources\": {\n",
        "            \"embedding\": base_swap.get(\"embedding_source\"),\n",
        "            \"unembedding\": base_swap.get(\"unembedding_source\"),\n",
        "        },\n",
        "        \"sft_swap_sources\": {\n",
        "            \"embedding\": sft_swap.get(\"embedding_source\"),\n",
        "            \"unembedding\": sft_swap.get(\"unembedding_source\"),\n",
        "        },\n",
        "    }\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "raw_sample, total_records = load_small_sample(DATA_PATH, max_items=1)\n",
        "print(f\"Top-level entries: {total_records}\")\n",
        "print(\"\\nFirst record summary:\")\n",
        "pprint(describe_record(raw_sample[0]), width=100, compact=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "record = raw_sample[0]\n",
        "metadata_preview = list(it.islice(record.get(\"metadata\", {}).items(), 5))\n",
        "print(\"Metadata sample (first 5 entries):\")\n",
        "pprint(metadata_preview, width=100, compact=True)\n",
        "\n",
        "first_layer = record.get(\"layers\", [])[0]\n",
        "layer_positions = first_layer.get(\"positions\", [])\n",
        "print(\"\\nLayer 0 summary:\")\n",
        "pprint(\n",
        "    {\n",
        "        \"layer_index\": first_layer.get(\"layer_index\"),\n",
        "        \"num_positions\": len(layer_positions),\n",
        "        \"token_names\": [pos.get(\"token\") for pos in layer_positions[:5]],\n",
        "    },\n",
        "    width=100,\n",
        "    compact=True,\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Structured dataclass view\n",
        "\n",
        "The `src.analysis.residual_results.loader` module exposes iterators and\n",
        "summaries that convert each JSON entry into typed dataclasses. This keeps\n",
        "the parsing lazy and provides convenience accessors for prompts, tokens,\n",
        "and layers.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "file_summary = loader.summarize_file(DATA_PATH)\n",
        "print(\"ResidualResult summary:\")\n",
        "pprint(\n",
        "    {\n",
        "        \"path\": str(file_summary.path),\n",
        "        \"num_results\": file_summary.num_results,\n",
        "        \"total_tokens\": file_summary.total_tokens,\n",
        "        \"avg_tokens\": round(file_summary.avg_tokens, 2),\n",
        "    },\n",
        "    width=100,\n",
        "    compact=True,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "result_iter = loader.iter_results(DATA_PATH)\n",
        "first_result = next(result_iter)\n",
        "\n",
        "structured_summary = {\n",
        "    \"prompt_chars\": len(first_result.prompt),\n",
        "    \"prompt_preview\": first_result.prompt[:120].replace(\"\\n\", \" \") + (\n",
        "        \"â€¦\" if len(first_result.prompt) > 120 else \"\"\n",
        "    ),\n",
        "    \"token_count\": first_result.num_tokens(),\n",
        "    \"token_preview\": list(first_result.tokens[:8]),\n",
        "    \"num_layers\": first_result.num_layers(),\n",
        "}\n",
        "\n",
        "print(\"First ResidualResult summary:\")\n",
        "pprint(structured_summary, width=100, compact=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Reusable query helpers\n",
        "\n",
        "Wrapping repeated inspection patterns into tiny functions keeps the\n",
        "notebook tidy. The helpers below take dataclass instances and emit\n",
        "summaries limited by caller-provided caps.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def summarize_tokens(result: loader.ResidualResult, max_tokens: int = 10) -> Mapping[str, Any]:\n",
        "    return {\n",
        "        \"total\": result.num_tokens(),\n",
        "        \"preview\": list(result.tokens[:max_tokens]),\n",
        "    }\n",
        "\n",
        "\n",
        "def list_layers(result: loader.ResidualResult, max_layers: int = 3, max_positions: int = 5):\n",
        "    for layer in it.islice(result.layers, max_layers):\n",
        "        yield {\n",
        "            \"layer_index\": layer.layer_index,\n",
        "            \"positions\": [pos.token for pos in layer.positions[:max_positions]],\n",
        "        }\n",
        "\n",
        "\n",
        "def layer_norm_deltas(result: loader.ResidualResult, max_layers: int = 3, max_positions: int = 5):\n",
        "    for layer in it.islice(result.layers, max_layers):\n",
        "        stats = []\n",
        "        for pos in layer.positions[:max_positions]:\n",
        "            stats.append(\n",
        "                {\n",
        "                    \"token\": pos.token,\n",
        "                    \"norm_diff\": pos.norm_diff,\n",
        "                    \"kl_div\": pos.kl_div,\n",
        "                }\n",
        "            )\n",
        "        yield layer.layer_index, stats\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Token preview via summarize_tokens():\")\n",
        "pprint(summarize_tokens(first_result, max_tokens=6), width=100, compact=True)\n",
        "\n",
        "print(\"\\nLayer token overview (first 2 layers):\")\n",
        "for info in list_layers(first_result, max_layers=2, max_positions=6):\n",
        "    pprint(info, width=100, compact=True)\n",
        "\n",
        "print(\"\\nLayer norm / KL samples:\")\n",
        "for layer_idx, stats in layer_norm_deltas(first_result, max_layers=1, max_positions=5):\n",
        "    print(f\"Layer {layer_idx}\")\n",
        "    pprint(stats, width=100, compact=True)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
