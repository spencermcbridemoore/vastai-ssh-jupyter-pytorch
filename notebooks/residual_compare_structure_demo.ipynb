{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Residual compare structure demo\n",
        "\n",
        "This notebook shows light-weight inspection patterns for the large\n",
        "`notebooks/h200_long_outputs/physics_A/qwen-0_5b/residual_compare_20251129_053550_Qwen_Qwen2_5_0_5B_Instruct.json`\n",
        "file. The goal is to understand its structure using Python helpers while\n",
        "avoiding massive prints or full data dumps.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "C:\\Users\\spenc\\Cursor Repos\\vastai-ssh-jupyter-pytorch\\notebooks\\notebooks\\h200_long_outputs\\physics_A\\qwen-0_5b\\residual_compare_20251129_053550_Qwen_Qwen2_5_0_5B_Instruct.json",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 33\u001b[39m\n\u001b[32m     27\u001b[39m DATA_PATH = Path(\n\u001b[32m     28\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mnotebooks/h200_long_outputs/physics_A/qwen-0_5b/\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     29\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mresidual_compare_20251129_053550_Qwen_Qwen2_5_0_5B_Instruct.json\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     30\u001b[39m ).resolve()\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m DATA_PATH.exists():\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(DATA_PATH)\n\u001b[32m     35\u001b[39m size_mb = DATA_PATH.stat().st_size / (\u001b[32m1024\u001b[39m ** \u001b[32m2\u001b[39m)\n\u001b[32m     36\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mData path: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDATA_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
            "\u001b[31mFileNotFoundError\u001b[39m: C:\\Users\\spenc\\Cursor Repos\\vastai-ssh-jupyter-pytorch\\notebooks\\notebooks\\h200_long_outputs\\physics_A\\qwen-0_5b\\residual_compare_20251129_053550_Qwen_Qwen2_5_0_5B_Instruct.json"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import sys\n",
        "import json\n",
        "import itertools as it\n",
        "from pprint import pprint\n",
        "from typing import Any, Mapping, Sequence\n",
        "\n",
        "\n",
        "def resolve_project_root() -> Path:\n",
        "    \"\"\"Locate the repository root (directory containing the src package).\"\"\"\n",
        "    candidates = [Path.cwd().resolve()]\n",
        "    if \"__file__\" in globals():\n",
        "        candidates.append(Path(__file__).resolve().parent)\n",
        "    candidates.extend(candidate.parent for candidate in list(candidates))\n",
        "    for candidate in candidates:\n",
        "        if (candidate / \"src\").exists():\n",
        "            return candidate\n",
        "    raise RuntimeError(\"Unable to locate project root containing 'src'.\")\n",
        "\n",
        "\n",
        "PROJECT_ROOT = resolve_project_root()\n",
        "if str(PROJECT_ROOT) not in sys.path:\n",
        "    sys.path.insert(0, str(PROJECT_ROOT))\n",
        "\n",
        "from src.analysis.residual_results import loader\n",
        "\n",
        "DATA_PATH = (\n",
        "    PROJECT_ROOT\n",
        "    / \"notebooks/h200_long_outputs/physics_A/qwen-0_5b/\"\n",
        "    / \"residual_compare_20251129_053550_Qwen_Qwen2_5_0_5B_Instruct.json\"\n",
        ").resolve()\n",
        "\n",
        "if not DATA_PATH.exists():\n",
        "    raise FileNotFoundError(DATA_PATH)\n",
        "\n",
        "size_mb = DATA_PATH.stat().st_size / (1024 ** 2)\n",
        "print(f\"Data path: {DATA_PATH}\")\n",
        "print(f\"File size: {size_mb:.2f} MiB\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Raw JSON peek\n",
        "\n",
        "The next helpers read a tiny slice of the JSON file and summarize what is\n",
        "available (keys, token counts, metadata keys, etc.). They explicitly cap\n",
        "how many tokens or layers are displayed so the notebook stays readable.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "from typing import Any, Mapping\n",
        "\n",
        "def load_small_sample(path: Path, max_items: int = 2):\n",
        "    \"\"\"Return up to `max_items` entries plus the total record count.\"\"\"\n",
        "    with path.open(\"r\", encoding=\"utf-8\") as fh:\n",
        "        payload = json.load(fh)\n",
        "    if not isinstance(payload, list):\n",
        "        raise TypeError(\"Expected a list of records at the top level.\")\n",
        "    return payload[:max_items], len(payload)\n",
        "\n",
        "\n",
        "def describe_record(record: Mapping[str, Any], token_limit: int = 8) -> Mapping[str, Any]:\n",
        "    \"\"\"Extract a compact summary from a single raw record.\"\"\"\n",
        "    tokens = record.get(\"tokens\", [])\n",
        "    metadata = record.get(\"metadata\", {})\n",
        "    layers = record.get(\"layers\", [])\n",
        "    base_swap = record.get(\"base_swap\", {})\n",
        "    sft_swap = record.get(\"sft_swap\", {})\n",
        "    return {\n",
        "        \"available_keys\": sorted(record.keys()),\n",
        "        \"token_count\": len(tokens),\n",
        "        \"token_preview\": tokens[:token_limit],\n",
        "        \"metadata_keys\": sorted(metadata.keys()),\n",
        "        \"layer_count\": len(layers),\n",
        "        \"layer_indices\": [layer.get(\"layer_index\") for layer in layers[:3]],\n",
        "        \"base_swap_sources\": {\n",
        "            \"embedding\": base_swap.get(\"embedding_source\"),\n",
        "            \"unembedding\": base_swap.get(\"unembedding_source\"),\n",
        "        },\n",
        "        \"sft_swap_sources\": {\n",
        "            \"embedding\": sft_swap.get(\"embedding_source\"),\n",
        "            \"unembedding\": sft_swap.get(\"unembedding_source\"),\n",
        "        },\n",
        "    }\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'DATA_PATH' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m raw_sample, total_records = load_small_sample(\u001b[43mDATA_PATH\u001b[49m, max_items=\u001b[32m1\u001b[39m)\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTop-level entries: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_records\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mFirst record summary:\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[31mNameError\u001b[39m: name 'DATA_PATH' is not defined"
          ]
        }
      ],
      "source": [
        "if \"DATA_PATH\" not in globals():\n",
        "    from pathlib import Path\n",
        "\n",
        "    def _locate_root() -> Path:\n",
        "        candidates = [Path.cwd().resolve()]\n",
        "        if \"__file__\" in globals():\n",
        "            candidates.append(Path(__file__).resolve().parent)\n",
        "        candidates.extend(candidate.parent for candidate in list(candidates))\n",
        "        for candidate in candidates:\n",
        "            if (candidate / \"src\").exists():\n",
        "                return candidate\n",
        "        raise RuntimeError(\"Unable to locate repository root containing 'src'.\")\n",
        "\n",
        "    _PROJECT_ROOT = _locate_root()\n",
        "    DATA_PATH = (\n",
        "        _PROJECT_ROOT\n",
        "        / \"notebooks/h200_long_outputs/physics_A/qwen-0_5b/\"\n",
        "        / \"residual_compare_20251129_053550_Qwen_Qwen2_5_0_5B_Instruct.json\"\n",
        "    ).resolve()\n",
        "\n",
        "raw_sample, total_records = load_small_sample(DATA_PATH, max_items=1)\n",
        "print(f\"Top-level entries: {total_records}\")\n",
        "print(\"\\nFirst record summary:\")\n",
        "pprint(describe_record(raw_sample[0]), width=100, compact=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "record = raw_sample[0]\n",
        "metadata_preview = list(it.islice(record.get(\"metadata\", {}).items(), 5))\n",
        "print(\"Metadata sample (first 5 entries):\")\n",
        "pprint(metadata_preview, width=100, compact=True)\n",
        "\n",
        "first_layer = record.get(\"layers\", [])[0]\n",
        "layer_positions = first_layer.get(\"positions\", [])\n",
        "print(\"\\nLayer 0 summary:\")\n",
        "pprint(\n",
        "    {\n",
        "        \"layer_index\": first_layer.get(\"layer_index\"),\n",
        "        \"num_positions\": len(layer_positions),\n",
        "        \"token_names\": [pos.get(\"token\") for pos in layer_positions[:5]],\n",
        "    },\n",
        "    width=100,\n",
        "    compact=True,\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Structured dataclass view\n",
        "\n",
        "The `src.analysis.residual_results.loader` module exposes iterators and\n",
        "summaries that convert each JSON entry into typed dataclasses. This keeps\n",
        "the parsing lazy and provides convenience accessors for prompts, tokens,\n",
        "and layers.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "file_summary = loader.summarize_file(DATA_PATH)\n",
        "print(\"ResidualResult summary:\")\n",
        "pprint(\n",
        "    {\n",
        "        \"path\": str(file_summary.path),\n",
        "        \"num_results\": file_summary.num_results,\n",
        "        \"total_tokens\": file_summary.total_tokens,\n",
        "        \"avg_tokens\": round(file_summary.avg_tokens, 2),\n",
        "    },\n",
        "    width=100,\n",
        "    compact=True,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "result_iter = loader.iter_results(DATA_PATH)\n",
        "first_result = next(result_iter)\n",
        "\n",
        "structured_summary = {\n",
        "    \"prompt_chars\": len(first_result.prompt),\n",
        "    \"prompt_preview\": first_result.prompt[:120].replace(\"\\n\", \" \") + (\n",
        "        \"â€¦\" if len(first_result.prompt) > 120 else \"\"\n",
        "    ),\n",
        "    \"token_count\": first_result.num_tokens(),\n",
        "    \"token_preview\": list(first_result.tokens[:8]),\n",
        "    \"num_layers\": first_result.num_layers(),\n",
        "}\n",
        "\n",
        "print(\"First ResidualResult summary:\")\n",
        "pprint(structured_summary, width=100, compact=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Reusable query helpers\n",
        "\n",
        "Wrapping repeated inspection patterns into tiny functions keeps the\n",
        "notebook tidy. The helpers below take dataclass instances and emit\n",
        "summaries limited by caller-provided caps.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def summarize_tokens(result: loader.ResidualResult, max_tokens: int = 10) -> Mapping[str, Any]:\n",
        "    return {\n",
        "        \"total\": result.num_tokens(),\n",
        "        \"preview\": list(result.tokens[:max_tokens]),\n",
        "    }\n",
        "\n",
        "\n",
        "def list_layers(result: loader.ResidualResult, max_layers: int = 3, max_positions: int = 5):\n",
        "    for layer in it.islice(result.layers, max_layers):\n",
        "        yield {\n",
        "            \"layer_index\": layer.layer_index,\n",
        "            \"positions\": [pos.token for pos in layer.positions[:max_positions]],\n",
        "        }\n",
        "\n",
        "\n",
        "def layer_norm_deltas(result: loader.ResidualResult, max_layers: int = 3, max_positions: int = 5):\n",
        "    for layer in it.islice(result.layers, max_layers):\n",
        "        stats = []\n",
        "        for pos in layer.positions[:max_positions]:\n",
        "            stats.append(\n",
        "                {\n",
        "                    \"token\": pos.token,\n",
        "                    \"norm_diff\": pos.norm_diff,\n",
        "                    \"kl_div\": pos.kl_div,\n",
        "                }\n",
        "            )\n",
        "        yield layer.layer_index, stats\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Token preview via summarize_tokens():\")\n",
        "pprint(summarize_tokens(first_result, max_tokens=6), width=100, compact=True)\n",
        "\n",
        "print(\"\\nLayer token overview (first 2 layers):\")\n",
        "for info in list_layers(first_result, max_layers=2, max_positions=6):\n",
        "    pprint(info, width=100, compact=True)\n",
        "\n",
        "print(\"\\nLayer norm / KL samples:\")\n",
        "for layer_idx, stats in layer_norm_deltas(first_result, max_layers=1, max_positions=5):\n",
        "    print(f\"Layer {layer_idx}\")\n",
        "    pprint(stats, width=100, compact=True)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "VASTAI-SSH-JUPYTER-PYTORCH-ENV",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
