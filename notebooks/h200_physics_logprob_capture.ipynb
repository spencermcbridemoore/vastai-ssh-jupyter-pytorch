{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# H200 Physics Prompt Logprob Capture\n",
        "\n",
        "This notebook orchestrates log-probability sweeps for the Physics A/B prompt suites on a Vast H200 instance. It mirrors the residual workflows but focuses solely on per-token logprobs so that perplexity for any prefix length can be reconstructed offline.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Usage Overview\n",
        "\n",
        "1. Detect or specify the target Vast H200 instance.\n",
        "2. Prefetch all required checkpoints before running any prompts.\n",
        "3. Filter the Physics A/B prompt files to drop items under 30 tokens.\n",
        "4. Stage the filtered prompts and remote helper script on the instance.\n",
        "5. For each model (base + SFT), attempt 1000/800/600/400 character truncations until a run succeeds, collecting JSONL logprob traces under `notebooks/h200_outputs_perplexity/`.\n",
        "6. Pull artifacts back locally and update the manifest to track which models/groups completed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Repo root: C:\\Users\\spenc\\Cursor Repos\\vastai-ssh-jupyter-pytorch\n",
            "Local output root: C:\\Users\\spenc\\Cursor Repos\\vastai-ssh-jupyter-pytorch\\notebooks\\h200_outputs_perplexity\n",
            "Vast instance: 28352791 -> root@208.64.254.75:16693\n"
          ]
        }
      ],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import json\n",
        "import os\n",
        "import shlex\n",
        "import subprocess\n",
        "import textwrap\n",
        "from dataclasses import dataclass\n",
        "from pathlib import Path, PurePosixPath\n",
        "from string import Template\n",
        "from typing import Dict, List, Optional, Sequence\n",
        "\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "REPO_ROOT = Path(\"..\").resolve()\n",
        "if not (REPO_ROOT / \"experiments\").exists():\n",
        "    REPO_ROOT = Path.cwd().resolve()\n",
        "\n",
        "NOTEBOOK_DIR = REPO_ROOT / \"notebooks\"\n",
        "LOCAL_OUTPUT_ROOT = NOTEBOOK_DIR / \"h200_outputs_perplexity\"\n",
        "LOCAL_OUTPUT_ROOT.mkdir(parents=True, exist_ok=True)\n",
        "LOCAL_PROMPT_CACHE_DIR = LOCAL_OUTPUT_ROOT / \"_prompt_cache\"\n",
        "LOCAL_PROMPT_CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "REMOTE_REPO = os.environ.get(\"H200_REMOTE_REPO\", \"/workspace/vastai-ssh-jupyter-pytorch\")\n",
        "REMOTE_REPO_PATH = PurePosixPath(REMOTE_REPO)\n",
        "REMOTE_OUTPUT_ROOT = str(REMOTE_REPO_PATH / \"notebooks\" / \"h200_outputs_perplexity\")\n",
        "REMOTE_PROMPT_DIR = f\"{REMOTE_OUTPUT_ROOT}/prompts\"\n",
        "REMOTE_SCRIPT_PATH = str(REMOTE_REPO_PATH / \"scripts\" / \"physics_logprob_remote.py\")\n",
        "REMOTE_PYTHON = os.environ.get(\"H200_REMOTE_PYTHON\", \"python3\")\n",
        "\n",
        "ATTEMPT_CHAR_LIMITS = [1000, 800, 600, 400]\n",
        "MIN_PROMPT_TOKENS = 30\n",
        "PROMPT_LIMIT = int(os.environ.get(\"H200_PHYSICS_PROMPT_LIMIT\", \"100\"))\n",
        "PROMPT_SOURCES = {\n",
        "    \"physics_A\": REPO_ROOT / \"experiments\" / \"prompts\" / \"physics_answers_A.txt\",\n",
        "    \"physics_B\": REPO_ROOT / \"experiments\" / \"prompts\" / \"physics_answers_B.txt\",\n",
        "}\n",
        "\n",
        "\n",
        "def detect_vast_instance(preferred_id: Optional[str]) -> Dict[str, Optional[str]]:\n",
        "    try:\n",
        "        result = subprocess.run(\n",
        "            [\"vastai\", \"show\", \"instances\"],\n",
        "            capture_output=True,\n",
        "            text=True,\n",
        "            check=True,\n",
        "        )\n",
        "    except (FileNotFoundError, subprocess.CalledProcessError):\n",
        "        return {\"instance_id\": preferred_id, \"ssh_host\": None, \"ssh_port\": None}\n",
        "\n",
        "    lines = [line.strip() for line in result.stdout.splitlines() if line.strip()]\n",
        "    rows = [line for line in lines if line and line[0].isdigit()]\n",
        "    target_row = None\n",
        "    for row in rows:\n",
        "        parts = row.split()\n",
        "        if not parts:\n",
        "            continue\n",
        "        row_id = parts[0]\n",
        "        if preferred_id and row_id == preferred_id:\n",
        "            target_row = parts\n",
        "            break\n",
        "        if target_row is None:\n",
        "            target_row = parts\n",
        "    if not target_row or len(target_row) < 11:\n",
        "        return {\n",
        "            \"instance_id\": preferred_id or (target_row[0] if target_row else None),\n",
        "            \"ssh_host\": None,\n",
        "            \"ssh_port\": None,\n",
        "        }\n",
        "\n",
        "    return {\n",
        "        \"instance_id\": target_row[0],\n",
        "        \"ssh_host\": target_row[9],\n",
        "        \"ssh_port\": target_row[10],\n",
        "    }\n",
        "\n",
        "\n",
        "def fetch_ssh_settings(instance_id: Optional[str]) -> Dict[str, Optional[str]]:\n",
        "    if not instance_id:\n",
        "        return {}\n",
        "    try:\n",
        "        result = subprocess.run(\n",
        "            [\"vastai\", \"ssh-url\", instance_id],\n",
        "            capture_output=True,\n",
        "            text=True,\n",
        "            check=True,\n",
        "        )\n",
        "    except (FileNotFoundError, subprocess.CalledProcessError):\n",
        "        return {}\n",
        "    from urllib.parse import urlparse\n",
        "\n",
        "    url = result.stdout.strip()\n",
        "    if not url:\n",
        "        return {}\n",
        "    parsed = urlparse(url)\n",
        "    if not parsed.hostname:\n",
        "        return {}\n",
        "    return {\n",
        "        \"ssh_user\": parsed.username or \"root\",\n",
        "        \"ssh_host\": parsed.hostname,\n",
        "        \"ssh_port\": str(parsed.port) if parsed.port else None,\n",
        "    }\n",
        "\n",
        "\n",
        "detected = detect_vast_instance(os.environ.get(\"H200_INSTANCE_ID\"))\n",
        "INSTANCE_ID = os.environ.get(\"H200_INSTANCE_ID\") or detected.get(\"instance_id\")\n",
        "ssh_url = fetch_ssh_settings(INSTANCE_ID)\n",
        "SSH_USER = os.environ.get(\"H200_SSH_USER\") or ssh_url.get(\"ssh_user\") or \"root\"\n",
        "SSH_HOST = os.environ.get(\"H200_SSH_HOST\") or ssh_url.get(\"ssh_host\") or detected.get(\"ssh_host\") or \"ssh.vast.ai\"\n",
        "SSH_PORT = int(os.environ.get(\"H200_SSH_PORT\") or ssh_url.get(\"ssh_port\") or detected.get(\"ssh_port\") or \"22\")\n",
        "_identity_env = os.environ.get(\"H200_SSH_IDENTITY\")\n",
        "default_identity = Path.home() / \".ssh\" / \"id_rsa\"\n",
        "if _identity_env:\n",
        "    SSH_IDENTITY: Optional[Path] = Path(_identity_env).expanduser()\n",
        "elif default_identity.exists():\n",
        "    SSH_IDENTITY = default_identity\n",
        "else:\n",
        "    SSH_IDENTITY = None\n",
        "\n",
        "print(f\"Repo root: {REPO_ROOT}\")\n",
        "print(f\"Local output root: {LOCAL_OUTPUT_ROOT}\")\n",
        "print(f\"Vast instance: {INSTANCE_ID} -> {SSH_USER}@{SSH_HOST}:{SSH_PORT}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tracking 10 model pairs.\n"
          ]
        }
      ],
      "source": [
        "@dataclass(frozen=True)\n",
        "class ModelSpec:\n",
        "    name: str\n",
        "    base: str\n",
        "    sft: str\n",
        "    tokenizer: str\n",
        "    dtype: str\n",
        "    device: str = \"cuda:0\"\n",
        "    notes: str = \"\"\n",
        "\n",
        "\n",
        "MODEL_SPECS: List[ModelSpec] = [\n",
        "    ModelSpec(\n",
        "        name=\"qwen-0_5b\",\n",
        "        base=\"Qwen/Qwen2.5-0.5B\",\n",
        "        sft=\"Qwen/Qwen2.5-0.5B-Instruct\",\n",
        "        tokenizer=\"Qwen/Qwen2.5-0.5B-Instruct\",\n",
        "        dtype=\"float16\",\n",
        "        notes=\"Tiny sanity check; should finish within minutes.\",\n",
        "    ),\n",
        "    ModelSpec(\n",
        "        name=\"qwen-1_5b\",\n",
        "        base=\"Qwen/Qwen2.5-1.5B\",\n",
        "        sft=\"Qwen/Qwen2.5-1.5B-Instruct\",\n",
        "        tokenizer=\"Qwen/Qwen2.5-1.5B-Instruct\",\n",
        "        dtype=\"float16\",\n",
        "        notes=\"Requires ~12-16 GB; still lightweight on H200.\",\n",
        "    ),\n",
        "    ModelSpec(\n",
        "        name=\"qwen-3b\",\n",
        "        base=\"Qwen/Qwen2.5-3B\",\n",
        "        sft=\"Qwen/Qwen2.5-3B-Instruct\",\n",
        "        tokenizer=\"Qwen/Qwen2.5-3B-Instruct\",\n",
        "        dtype=\"float16\",\n",
        "        notes=\"Comfortably fits on an H200 in fp16.\",\n",
        "    ),\n",
        "    ModelSpec(\n",
        "        name=\"qwen-7b\",\n",
        "        base=\"Qwen/Qwen2.5-7B\",\n",
        "        sft=\"Qwen/Qwen2.5-7B-Instruct\",\n",
        "        tokenizer=\"Qwen/Qwen2.5-7B-Instruct\",\n",
        "        dtype=\"bfloat16\",\n",
        "        notes=\"Use bf16 for stability on longer contexts.\",\n",
        "    ),\n",
        "    ModelSpec(\n",
        "        name=\"qwen-14b\",\n",
        "        base=\"Qwen/Qwen2.5-14B\",\n",
        "        sft=\"Qwen/Qwen2.5-14B-Instruct\",\n",
        "        tokenizer=\"Qwen/Qwen2.5-14B-Instruct\",\n",
        "        dtype=\"bfloat16\",\n",
        "        notes=\"Large but still single-GPU on H200.\",\n",
        "    ),\n",
        "    ModelSpec(\n",
        "        name=\"qwen-32b\",\n",
        "        base=\"Qwen/Qwen2.5-32B\",\n",
        "        sft=\"Qwen/Qwen2.5-32B-Instruct\",\n",
        "        tokenizer=\"Qwen/Qwen2.5-32B-Instruct\",\n",
        "        dtype=\"bfloat16\",\n",
        "        notes=\"Heavyweight pair; expect slower throughput.\",\n",
        "    ),\n",
        "    ModelSpec(\n",
        "        name=\"qwen-72b\",\n",
        "        base=\"Qwen/Qwen2.5-72B\",\n",
        "        sft=\"Qwen/Qwen2.5-72B-Instruct\",\n",
        "        tokenizer=\"Qwen/Qwen2.5-72B-Instruct\",\n",
        "        dtype=\"bfloat16\",\n",
        "        notes=\"Pushes memory; keep batch size at 1.\",\n",
        "    ),\n",
        "    ModelSpec(\n",
        "        name=\"llama-8b\",\n",
        "        base=\"meta-llama/Llama-3.1-8B\",\n",
        "        sft=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
        "        tokenizer=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
        "        dtype=\"bfloat16\",\n",
        "        notes=\"Requires HF auth; set HUGGINGFACE_TOKEN remotely.\",\n",
        "    ),\n",
        "    ModelSpec(\n",
        "        name=\"llama-70b\",\n",
        "        base=\"meta-llama/Llama-3.1-70B\",\n",
        "        sft=\"meta-llama/Llama-3.1-70B-Instruct\",\n",
        "        tokenizer=\"meta-llama/Llama-3.1-70B-Instruct\",\n",
        "        dtype=\"bfloat16\",\n",
        "        notes=\"Plan for long download time (~200 GB).\",\n",
        "    ),\n",
        "    ModelSpec(\n",
        "        name=\"mistral-8b\",\n",
        "        base=\"nvidia/Mistral-NeMo-Minitron-8B-Base\",\n",
        "        sft=\"nvidia/Mistral-NeMo-Minitron-8B-Instruct\",\n",
        "        tokenizer=\"nvidia/Mistral-NeMo-Minitron-8B-Instruct\",\n",
        "        dtype=\"bfloat16\",\n",
        "        notes=\"NVIDIA NeMo release; requires NGC credential if private.\",\n",
        "    ),\n",
        "]\n",
        "\n",
        "print(f\"Tracking {len(MODEL_SPECS)} model pairs.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Group physics_A: kept 100 prompts (>= 30 tokens)\n",
            "Group physics_B: kept 100 prompts (>= 30 tokens)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'physics_A': {'local_path': WindowsPath('C:/Users/spenc/Cursor Repos/vastai-ssh-jupyter-pytorch/notebooks/h200_outputs_perplexity/_prompt_cache/physics_A.txt'),\n",
              "  'count': 100},\n",
              " 'physics_B': {'local_path': WindowsPath('C:/Users/spenc/Cursor Repos/vastai-ssh-jupyter-pytorch/notebooks/h200_outputs_perplexity/_prompt_cache/physics_B.txt'),\n",
              "  'count': 100}}"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "FALLBACK_TOKENIZER_NAME = os.environ.get(\"H200_FALLBACK_TOKENIZER\", \"Qwen/Qwen2.5-0.5B-Instruct\")\n",
        "FALLBACK_TOKENIZER = AutoTokenizer.from_pretrained(FALLBACK_TOKENIZER_NAME)\n",
        "if FALLBACK_TOKENIZER.pad_token is None:\n",
        "    FALLBACK_TOKENIZER.pad_token = FALLBACK_TOKENIZER.eos_token\n",
        "\n",
        "PROMPT_CACHE: Dict[str, Dict[str, object]] = {}\n",
        "\n",
        "\n",
        "def truncate_to_char_limit(text: str, limit: int) -> str:\n",
        "    if len(text) <= limit:\n",
        "        return text\n",
        "    return text[:limit]\n",
        "\n",
        "\n",
        "def load_and_filter_prompts(path: Path, limit: int) -> List[str]:\n",
        "    filtered: List[str] = []\n",
        "    with path.open(\"r\", encoding=\"utf-8\") as handle:\n",
        "        for line in handle:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "            tokenized = FALLBACK_TOKENIZER(line, add_special_tokens=False)\n",
        "            if len(tokenized[\"input_ids\"]) < MIN_PROMPT_TOKENS:\n",
        "                continue\n",
        "            filtered.append(line)\n",
        "            if len(filtered) >= limit:\n",
        "                break\n",
        "    return filtered\n",
        "\n",
        "\n",
        "def refresh_prompt_cache() -> None:\n",
        "    PROMPT_CACHE.clear()\n",
        "    for group, src_path in PROMPT_SOURCES.items():\n",
        "        prompts = load_and_filter_prompts(src_path, PROMPT_LIMIT)\n",
        "        cache_path = LOCAL_PROMPT_CACHE_DIR / f\"{group}.txt\"\n",
        "        with cache_path.open(\"w\", encoding=\"utf-8\") as handle:\n",
        "            for prompt in prompts:\n",
        "                handle.write(prompt + \"\\n\")\n",
        "        PROMPT_CACHE[group] = {\n",
        "            \"local_path\": cache_path,\n",
        "            \"count\": len(prompts),\n",
        "        }\n",
        "        print(f\"Group {group}: kept {len(prompts)} prompts (>= {MIN_PROMPT_TOKENS} tokens)\")\n",
        "\n",
        "\n",
        "refresh_prompt_cache()\n",
        "PROMPT_CACHE\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Remote directories ready.\n"
          ]
        }
      ],
      "source": [
        "def _ssh_base() -> List[str]:\n",
        "    parts = [\"ssh\", \"-p\", str(SSH_PORT)]\n",
        "    if SSH_IDENTITY:\n",
        "        parts += [\"-i\", str(SSH_IDENTITY)]\n",
        "    parts += [f\"{SSH_USER}@{SSH_HOST}\"]\n",
        "    return parts\n",
        "\n",
        "\n",
        "def run_ssh(command: str, *, check: bool = True, capture_output: bool = False) -> subprocess.CompletedProcess:\n",
        "    full_cmd = _ssh_base() + [command]\n",
        "    return subprocess.run(\n",
        "        full_cmd,\n",
        "        check=check,\n",
        "        capture_output=capture_output,\n",
        "        text=True,\n",
        "    )\n",
        "\n",
        "\n",
        "def run_local(command: Sequence[str], *, check: bool = True) -> subprocess.CompletedProcess:\n",
        "    return subprocess.run(command, check=check)\n",
        "\n",
        "\n",
        "def ensure_remote_dir(path: str) -> None:\n",
        "    run_ssh(f\"mkdir -p {shlex.quote(path)}\")\n",
        "\n",
        "\n",
        "def push_file(local_path: Path, remote_path: str) -> None:\n",
        "    cmd = [\"scp\", \"-P\", str(SSH_PORT)]\n",
        "    if SSH_IDENTITY:\n",
        "        cmd += [\"-i\", str(SSH_IDENTITY)]\n",
        "    cmd += [str(local_path), f\"{SSH_USER}@{SSH_HOST}:{remote_path}\"]\n",
        "    run_local(cmd)\n",
        "\n",
        "\n",
        "def pull_remote_tree(remote_path: str, local_path: Path) -> None:\n",
        "    local_path.mkdir(parents=True, exist_ok=True)\n",
        "    cmd = [\"scp\", \"-r\", \"-P\", str(SSH_PORT)]\n",
        "    if SSH_IDENTITY:\n",
        "        cmd += [\"-i\", str(SSH_IDENTITY)]\n",
        "    cmd += [f\"{SSH_USER}@{SSH_HOST}:{remote_path}\", str(local_path)]\n",
        "    run_local(cmd)\n",
        "\n",
        "\n",
        "ensure_remote_dir(REMOTE_OUTPUT_ROOT)\n",
        "ensure_remote_dir(REMOTE_PROMPT_DIR)\n",
        "print(\"Remote directories ready.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0130cc8a89554a9390f7d358c85c1ad7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Staging prompt files:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "physics_A: uploading physics_A.txt → /workspace/vastai-ssh-jupyter-pytorch/notebooks/h200_outputs_perplexity/prompts/physics_A.txt\n",
            "physics_A: upload complete (100 prompts)\n",
            "physics_B: uploading physics_B.txt → /workspace/vastai-ssh-jupyter-pytorch/notebooks/h200_outputs_perplexity/prompts/physics_B.txt\n",
            "physics_B: upload complete (100 prompts)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'physics_A': {'remote_path': '/workspace/vastai-ssh-jupyter-pytorch/notebooks/h200_outputs_perplexity/prompts/physics_A.txt',\n",
              "  'local_path': 'C:\\\\Users\\\\spenc\\\\Cursor Repos\\\\vastai-ssh-jupyter-pytorch\\\\notebooks\\\\h200_outputs_perplexity\\\\_prompt_cache\\\\physics_A.txt',\n",
              "  'count': 100},\n",
              " 'physics_B': {'remote_path': '/workspace/vastai-ssh-jupyter-pytorch/notebooks/h200_outputs_perplexity/prompts/physics_B.txt',\n",
              "  'local_path': 'C:\\\\Users\\\\spenc\\\\Cursor Repos\\\\vastai-ssh-jupyter-pytorch\\\\notebooks\\\\h200_outputs_perplexity\\\\_prompt_cache\\\\physics_B.txt',\n",
              "  'count': 100}}"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from tqdm.auto import tqdm\n",
        "\n",
        "def stage_prompts_remote(show_progress: bool = True) -> Dict[str, Dict[str, str]]:\n",
        "    ensure_remote_dir(REMOTE_PROMPT_DIR)\n",
        "    summary: Dict[str, Dict[str, str]] = {}\n",
        "\n",
        "    iterator = PROMPT_CACHE.items()\n",
        "    if show_progress:\n",
        "        iterator = tqdm(\n",
        "            iterator,\n",
        "            desc=\"Staging prompt files\",\n",
        "            total=len(PROMPT_CACHE),\n",
        "            leave=True,\n",
        "        )\n",
        "\n",
        "    for group, meta in iterator:\n",
        "        remote_path = f\"{REMOTE_PROMPT_DIR}/{group}.txt\"\n",
        "        message = f\"{group}: uploading {meta['local_path'].name} → {remote_path}\"\n",
        "        if show_progress:\n",
        "            tqdm.write(message)\n",
        "        else:\n",
        "            print(message)\n",
        "\n",
        "        push_file(meta[\"local_path\"], remote_path)\n",
        "\n",
        "        done_msg = f\"{group}: upload complete ({meta['count']} prompts)\"\n",
        "        if show_progress:\n",
        "            tqdm.write(done_msg)\n",
        "        else:\n",
        "            print(done_msg)\n",
        "\n",
        "        summary[group] = {\n",
        "            \"remote_path\": remote_path,\n",
        "            \"local_path\": str(meta[\"local_path\"]),\n",
        "            \"count\": meta[\"count\"],\n",
        "        }\n",
        "\n",
        "    return summary\n",
        "\n",
        "\n",
        "REMOTE_PROMPT_MAP = stage_prompts_remote()\n",
        "REMOTE_PROMPT_MAP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'args' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 41\u001b[39m\n\u001b[32m      1\u001b[39m REMOTE_SCRIPT_BODY = textwrap.dedent(\n\u001b[32m      2\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[33m#!/usr/bin/env python\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[33mimport argparse\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[33mimport json\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[33mimport time\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[33mfrom datetime import datetime\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[33mfrom pathlib import Path\u001b[39m\n\u001b[32m      9\u001b[39m \n\u001b[32m     10\u001b[39m \u001b[33mimport torch\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[33mfrom transformers import AutoModelForCausalLM, AutoTokenizer\u001b[39m\n\u001b[32m     12\u001b[39m \n\u001b[32m     13\u001b[39m \n\u001b[32m     14\u001b[39m \u001b[33mdef resolve_dtype(name: str) -> torch.dtype:\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[33m    if not name:\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[33m        return torch.float16\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[33m    lowered = name.lower()\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[33m    if lowered in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33mbfloat16\u001b[39m\u001b[33m\"\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[33m\"\u001b[39m\u001b[33mbf16\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m:\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[33m        return torch.bfloat16\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[33m    return torch.float16\u001b[39m\n\u001b[32m     21\u001b[39m \n\u001b[32m     22\u001b[39m \n\u001b[32m     23\u001b[39m \u001b[33mdef main() -> None:\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[33m    parser = argparse.ArgumentParser(description=\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPhysics prompt logprob capture\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m)\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[33m    parser.add_argument(\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m--model-name\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m, required=True)\u001b[39m\n\u001b[32m     26\u001b[39m \u001b[33m    parser.add_argument(\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m--tokenizer-name\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m, required=True)\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[33m    parser.add_argument(\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m--prompt-file\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m, required=True)\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[33m    parser.add_argument(\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m--output-dir\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m, required=True)\u001b[39m\n\u001b[32m     29\u001b[39m \u001b[33m    parser.add_argument(\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m--attempt-char-limits\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m, default=\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m1000,800,600,400\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m)\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[33m    parser.add_argument(\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m--min-tokens\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m, type=int, default=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMIN_PROMPT_TOKENS\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\n\u001b[32m     31\u001b[39m \u001b[33m    parser.add_argument(\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m--dtype\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m, default=\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mfloat16\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m)\u001b[39m\n\u001b[32m     32\u001b[39m \u001b[33m    parser.add_argument(\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m--device\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m, default=\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mcuda:0\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m)\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[33m    parser.add_argument(\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m--run-id\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m, required=True)\u001b[39m\n\u001b[32m     34\u001b[39m \u001b[33m    parser.add_argument(\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m--group-name\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m, required=True)\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[33m    parser.add_argument(\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m--variant-name\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m, required=True)\u001b[39m\n\u001b[32m     36\u001b[39m \u001b[33m    args = parser.parse_args()\u001b[39m\n\u001b[32m     37\u001b[39m \n\u001b[32m     38\u001b[39m \u001b[33m    attempt_limits = [int(part.strip()) for part in args.attempt_char_limits.split(\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m,\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m) if part.strip()]\u001b[39m\n\u001b[32m     39\u001b[39m \u001b[33m    output_dir = Path(args.output_dir)\u001b[39m\n\u001b[32m     40\u001b[39m \u001b[33m    output_dir.mkdir(parents=True, exist_ok=True)\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m \u001b[33m    jsonl_path = output_dir / f\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mlogprobs_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43margs\u001b[49m.run_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.jsonl\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     42\u001b[39m \u001b[33m    summary_path = output_dir / f\u001b[39m\u001b[33m\"\u001b[39m\u001b[33msummary_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs.run_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.json\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     43\u001b[39m \u001b[33m    log_path = output_dir / f\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mrun_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs.run_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.log\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     44\u001b[39m \n\u001b[32m     45\u001b[39m \u001b[33m    dtype = resolve_dtype(args.dtype)\u001b[39m\n\u001b[32m     46\u001b[39m \u001b[33m    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name)\u001b[39m\n\u001b[32m     47\u001b[39m \u001b[33m    if tokenizer.pad_token is None:\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[33m        tokenizer.pad_token = tokenizer.eos_token\u001b[39m\n\u001b[32m     49\u001b[39m \u001b[33m    model = AutoModelForCausalLM.from_pretrained(\u001b[39m\n\u001b[32m     50\u001b[39m \u001b[33m        args.model_name,\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[33m        torch_dtype=dtype,\u001b[39m\n\u001b[32m     52\u001b[39m \u001b[33m        low_cpu_mem_usage=True,\u001b[39m\n\u001b[32m     53\u001b[39m \u001b[33m    ).to(args.device)\u001b[39m\n\u001b[32m     54\u001b[39m \u001b[33m    model.eval()\u001b[39m\n\u001b[32m     55\u001b[39m \n\u001b[32m     56\u001b[39m \u001b[33m    with open(args.prompt_file, \u001b[39m\u001b[33m\"\u001b[39m\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m, encoding=\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m) as handle:\u001b[39m\n\u001b[32m     57\u001b[39m \u001b[33m        prompts = [line.strip() for line in handle if line.strip()]\u001b[39m\n\u001b[32m     58\u001b[39m \n\u001b[32m     59\u001b[39m \u001b[33m    def log(message: str) -> None:\u001b[39m\n\u001b[32m     60\u001b[39m \u001b[33m        timestamp = datetime.utcnow().isoformat() + \u001b[39m\u001b[33m\"\u001b[39m\u001b[33mZ\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     61\u001b[39m \u001b[33m        payload = f\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[\u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[38;5;130;01m}}\u001b[39;00m\u001b[33m] \u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[38;5;130;01m}}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m.format(timestamp, message)\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[33m        with open(log_path, \u001b[39m\u001b[33m\"\u001b[39m\u001b[33ma\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m, encoding=\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m) as log_handle:\u001b[39m\n\u001b[32m     63\u001b[39m \u001b[33m            log_handle.write(payload)\u001b[39m\n\u001b[32m     64\u001b[39m \u001b[33m        print(payload, end=\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m)\u001b[39m\n\u001b[32m     65\u001b[39m \n\u001b[32m     66\u001b[39m \u001b[33m    records = []\u001b[39m\n\u001b[32m     67\u001b[39m \u001b[33m    failures = []\u001b[39m\n\u001b[32m     68\u001b[39m \u001b[33m    start_time = time.time()\u001b[39m\n\u001b[32m     69\u001b[39m \n\u001b[32m     70\u001b[39m \u001b[33m    for idx, prompt in enumerate(prompts):\u001b[39m\n\u001b[32m     71\u001b[39m \u001b[33m        success = False\u001b[39m\n\u001b[32m     72\u001b[39m \u001b[33m        for limit in attempt_limits:\u001b[39m\n\u001b[32m     73\u001b[39m \u001b[33m            truncated = prompt if len(prompt) <= limit else prompt[:limit]\u001b[39m\n\u001b[32m     74\u001b[39m \u001b[33m            encoded = tokenizer(\u001b[39m\n\u001b[32m     75\u001b[39m \u001b[33m                truncated,\u001b[39m\n\u001b[32m     76\u001b[39m \u001b[33m                return_tensors=\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m,\u001b[39m\n\u001b[32m     77\u001b[39m \u001b[33m                padding=False,\u001b[39m\n\u001b[32m     78\u001b[39m \u001b[33m                add_special_tokens=False,\u001b[39m\n\u001b[32m     79\u001b[39m \u001b[33m            )\u001b[39m\n\u001b[32m     80\u001b[39m \u001b[33m            seq_len = int(encoded[\u001b[39m\u001b[33m\"\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m].shape[-1])\u001b[39m\n\u001b[32m     81\u001b[39m \u001b[33m            if seq_len < max(2, args.min_tokens):\u001b[39m\n\u001b[32m     82\u001b[39m \u001b[33m                continue\u001b[39m\n\u001b[32m     83\u001b[39m \u001b[33m            encoded = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m value.to(args.device) for key, value in encoded.items()\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[32m     84\u001b[39m \u001b[33m            try:\u001b[39m\n\u001b[32m     85\u001b[39m \u001b[33m                with torch.no_grad():\u001b[39m\n\u001b[32m     86\u001b[39m \u001b[33m                    outputs = model(**encoded)\u001b[39m\n\u001b[32m     87\u001b[39m \u001b[33m                logits = outputs.logits[:, :-1, :]\u001b[39m\n\u001b[32m     88\u001b[39m \u001b[33m                target_ids = encoded[\u001b[39m\u001b[33m\"\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m][:, 1:]\u001b[39m\n\u001b[32m     89\u001b[39m \u001b[33m                log_probs = torch.log_softmax(logits, dim=-1)\u001b[39m\n\u001b[32m     90\u001b[39m \u001b[33m                gathered = log_probs.gather(-1, target_ids.unsqueeze(-1)).squeeze(-1)\u001b[39m\n\u001b[32m     91\u001b[39m \u001b[33m                token_log_probs = gathered[0].tolist()\u001b[39m\n\u001b[32m     92\u001b[39m \u001b[33m                cumulative_nll = (-torch.cumsum(gathered[0], dim=0)).tolist()\u001b[39m\n\u001b[32m     93\u001b[39m \u001b[33m                token_ids = target_ids[0].tolist()\u001b[39m\n\u001b[32m     94\u001b[39m \u001b[33m                tokens = tokenizer.convert_ids_to_tokens(token_ids)\u001b[39m\n\u001b[32m     95\u001b[39m \u001b[33m                record = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\n\u001b[32m     96\u001b[39m \u001b[38;5;250m                    \u001b[39m\u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m args.model_name,\u001b[39m\n\u001b[32m     97\u001b[39m \u001b[33m                    \u001b[39m\u001b[33m\"\u001b[39m\u001b[33mvariant\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m: args.variant_name,\u001b[39m\n\u001b[32m     98\u001b[39m \u001b[33m                    \u001b[39m\u001b[33m\"\u001b[39m\u001b[33mgroup\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m: args.group_name,\u001b[39m\n\u001b[32m     99\u001b[39m \u001b[33m                    \u001b[39m\u001b[33m\"\u001b[39m\u001b[33mprompt_idx\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m: idx,\u001b[39m\n\u001b[32m    100\u001b[39m \u001b[33m                    \u001b[39m\u001b[33m\"\u001b[39m\u001b[33mchar_limit\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m: limit,\u001b[39m\n\u001b[32m    101\u001b[39m \u001b[33m                    \u001b[39m\u001b[33m\"\u001b[39m\u001b[33mchars_used\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m: len(truncated),\u001b[39m\n\u001b[32m    102\u001b[39m \u001b[33m                    \u001b[39m\u001b[33m\"\u001b[39m\u001b[33mtoken_count\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m: len(token_log_probs),\u001b[39m\n\u001b[32m    103\u001b[39m \u001b[33m                    \u001b[39m\u001b[33m\"\u001b[39m\u001b[33mtoken_log_probs\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m: token_log_probs,\u001b[39m\n\u001b[32m    104\u001b[39m \u001b[33m                    \u001b[39m\u001b[33m\"\u001b[39m\u001b[33mcumulative_nll\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m: cumulative_nll,\u001b[39m\n\u001b[32m    105\u001b[39m \u001b[33m                    \u001b[39m\u001b[33m\"\u001b[39m\u001b[33mtokens\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m: tokens,\u001b[39m\n\u001b[32m    106\u001b[39m \u001b[33m                \u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[32m    107\u001b[39m \u001b[33m                records.append(record)\u001b[39m\n\u001b[32m    108\u001b[39m \u001b[33m                success = True\u001b[39m\n\u001b[32m    109\u001b[39m \u001b[33m                log(f\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mprompt \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: success at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlimit\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m chars (token_count=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrecord[\u001b[33m'\u001b[39m\u001b[33mtoken_count\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m)\u001b[39m\n\u001b[32m    110\u001b[39m \u001b[33m                break\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[33m            except torch.cuda.OutOfMemoryError:\u001b[39m\n\u001b[32m    112\u001b[39m \u001b[33m                torch.cuda.empty_cache()\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[33m                log(f\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mprompt \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: OOM at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlimit\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m chars, retrying shorter\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m)\u001b[39m\n\u001b[32m    114\u001b[39m \u001b[33m                continue\u001b[39m\n\u001b[32m    115\u001b[39m \u001b[33m        if not success:\u001b[39m\n\u001b[32m    116\u001b[39m \u001b[33m            failures.append(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33mprompt_idx\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m idx, \u001b[39m\u001b[33m\"\u001b[39m\u001b[33mreason\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m: \u001b[39m\u001b[33m\"\u001b[39m\u001b[33mall_attempts_failed\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[33m            log(f\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mprompt \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: FAILED after attempts \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattempt_limits\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m)\u001b[39m\n\u001b[32m    118\u001b[39m \n\u001b[32m    119\u001b[39m \u001b[33m    elapsed = time.time() - start_time\u001b[39m\n\u001b[32m    120\u001b[39m \u001b[33m    with open(jsonl_path, \u001b[39m\u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m, encoding=\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m) as writer:\u001b[39m\n\u001b[32m    121\u001b[39m \u001b[33m        for record in records:\u001b[39m\n\u001b[32m    122\u001b[39m \u001b[33m            writer.write(json.dumps(record) + \u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m)\u001b[39m\n\u001b[32m    123\u001b[39m \u001b[33m    with open(summary_path, \u001b[39m\u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m, encoding=\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m) as writer:\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[33m        json.dump(\u001b[39m\n\u001b[32m    125\u001b[39m \u001b[33m            \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\n\u001b[32m    126\u001b[39m \u001b[38;5;250m                \u001b[39m\u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m args.model_name,\u001b[39m\n\u001b[32m    127\u001b[39m \u001b[33m                \u001b[39m\u001b[33m\"\u001b[39m\u001b[33mvariant\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m: args.variant_name,\u001b[39m\n\u001b[32m    128\u001b[39m \u001b[33m                \u001b[39m\u001b[33m\"\u001b[39m\u001b[33mgroup\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m: args.group_name,\u001b[39m\n\u001b[32m    129\u001b[39m \u001b[33m                \u001b[39m\u001b[33m\"\u001b[39m\u001b[33mrun_id\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m: args.run_id,\u001b[39m\n\u001b[32m    130\u001b[39m \u001b[33m                \u001b[39m\u001b[33m\"\u001b[39m\u001b[33mprompt_count\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m: len(prompts),\u001b[39m\n\u001b[32m    131\u001b[39m \u001b[33m                \u001b[39m\u001b[33m\"\u001b[39m\u001b[33mrecords\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m: len(records),\u001b[39m\n\u001b[32m    132\u001b[39m \u001b[33m                \u001b[39m\u001b[33m\"\u001b[39m\u001b[33mfailures\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m: failures,\u001b[39m\n\u001b[32m    133\u001b[39m \u001b[33m                \u001b[39m\u001b[33m\"\u001b[39m\u001b[33mattempt_char_limits\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m: attempt_limits,\u001b[39m\n\u001b[32m    134\u001b[39m \u001b[33m                \u001b[39m\u001b[33m\"\u001b[39m\u001b[33melapsed_sec\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m: elapsed,\u001b[39m\n\u001b[32m    135\u001b[39m \u001b[33m            \u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m,\u001b[39m\n\u001b[32m    136\u001b[39m \u001b[33m            writer,\u001b[39m\n\u001b[32m    137\u001b[39m \u001b[33m            indent=2,\u001b[39m\n\u001b[32m    138\u001b[39m \u001b[33m        )\u001b[39m\n\u001b[32m    139\u001b[39m \u001b[33m    log(f\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCompleted run \u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs.run_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00melapsed\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33ms (records=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(records)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, failures=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(failures)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m)\u001b[39m\n\u001b[32m    140\u001b[39m \n\u001b[32m    141\u001b[39m \n\u001b[32m    142\u001b[39m \u001b[33mif __name__ == \u001b[39m\u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m:\u001b[39m\n\u001b[32m    143\u001b[39m \u001b[33m    main()\u001b[39m\n\u001b[32m    144\u001b[39m \u001b[33m\"\"\"\u001b[39m\n\u001b[32m    145\u001b[39m )\n\u001b[32m    148\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msync_remote_runner_script\u001b[39m() -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    149\u001b[39m     payload = REMOTE_SCRIPT_BODY.strip() + \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n",
            "\u001b[31mNameError\u001b[39m: name 'args' is not defined"
          ]
        }
      ],
      "source": [
        "REMOTE_SCRIPT_TEMPLATE = Template(\n",
        "    \"\"\"\n",
        "#!/usr/bin/env python\n",
        "import argparse\n",
        "import json\n",
        "import time\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "\n",
        "def resolve_dtype(name: str) -> torch.dtype:\n",
        "    if not name:\n",
        "        return torch.float16\n",
        "    lowered = name.lower()\n",
        "    if lowered in {\"bfloat16\", \"bf16\"}:\n",
        "        return torch.bfloat16\n",
        "    return torch.float16\n",
        "\n",
        "\n",
        "def main() -> None:\n",
        "    parser = argparse.ArgumentParser(description=\"Physics prompt logprob capture\")\n",
        "    parser.add_argument(\"--model-name\", required=True)\n",
        "    parser.add_argument(\"--tokenizer-name\", required=True)\n",
        "    parser.add_argument(\"--prompt-file\", required=True)\n",
        "    parser.add_argument(\"--output-dir\", required=True)\n",
        "    parser.add_argument(\"--attempt-char-limits\", default=\"1000,800,600,400\")\n",
        "    parser.add_argument(\"--min-tokens\", type=int, default=$MIN_TOKENS)\n",
        "    parser.add_argument(\"--dtype\", default=\"float16\")\n",
        "    parser.add_argument(\"--device\", default=\"cuda:0\")\n",
        "    parser.add_argument(\"--run-id\", required=True)\n",
        "    parser.add_argument(\"--group-name\", required=True)\n",
        "    parser.add_argument(\"--variant-name\", required=True)\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    attempt_limits = [int(part.strip()) for part in args.attempt_char_limits.split(\",\") if part.strip()]\n",
        "    output_dir = Path(args.output_dir)\n",
        "    output_dir.mkdir(parents=True, exist_ok=True)\n",
        "    jsonl_path = output_dir / f\"logprobs_{args.run_id}.jsonl\"\n",
        "    summary_path = output_dir / f\"summary_{args.run_id}.json\"\n",
        "    log_path = output_dir / f\"run_{args.run_id}.log\"\n",
        "\n",
        "    dtype = resolve_dtype(args.dtype)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name)\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        args.model_name,\n",
        "        torch_dtype=dtype,\n",
        "        low_cpu_mem_usage=True,\n",
        "    ).to(args.device)\n",
        "    model.eval()\n",
        "\n",
        "    with open(args.prompt_file, \"r\", encoding=\"utf-8\") as handle:\n",
        "        prompts = [line.strip() for line in handle if line.strip()]\n",
        "\n",
        "    def log(message: str) -> None:\n",
        "        timestamp = datetime.utcnow().isoformat() + \"Z\"\n",
        "        payload = f\"[{{}}] {{}}\\n\".format(timestamp, message)\n",
        "        with open(log_path, \"a\", encoding=\"utf-8\") as log_handle:\n",
        "            log_handle.write(payload)\n",
        "        print(payload, end=\"\")\n",
        "\n",
        "    records = []\n",
        "    failures = []\n",
        "    start_time = time.time()\n",
        "\n",
        "    for idx, prompt in enumerate(prompts):\n",
        "        success = False\n",
        "        for limit in attempt_limits:\n",
        "            truncated = prompt if len(prompt) <= limit else prompt[:limit]\n",
        "            encoded = tokenizer(\n",
        "                truncated,\n",
        "                return_tensors=\"pt\",\n",
        "                padding=False,\n",
        "                add_special_tokens=False,\n",
        "            )\n",
        "            seq_len = int(encoded[\"input_ids\"].shape[-1])\n",
        "            if seq_len < max(2, args.min_tokens):\n",
        "                continue\n",
        "            encoded = {key: value.to(args.device) for key, value in encoded.items()}\n",
        "            try:\n",
        "                with torch.no_grad():\n",
        "                    outputs = model(**encoded)\n",
        "                logits = outputs.logits[:, :-1, :]\n",
        "                target_ids = encoded[\"input_ids\"][:, 1:]\n",
        "                log_probs = torch.log_softmax(logits, dim=-1)\n",
        "                gathered = log_probs.gather(-1, target_ids.unsqueeze(-1)).squeeze(-1)\n",
        "                token_log_probs = gathered[0].tolist()\n",
        "                cumulative_nll = (-torch.cumsum(gathered[0], dim=0)).tolist()\n",
        "                token_ids = target_ids[0].tolist()\n",
        "                tokens = tokenizer.convert_ids_to_tokens(token_ids)\n",
        "                record = {\n",
        "                    \"model\": args.model_name,\n",
        "                    \"variant\": args.variant_name,\n",
        "                    \"group\": args.group_name,\n",
        "                    \"prompt_idx\": idx,\n",
        "                    \"char_limit\": limit,\n",
        "                    \"chars_used\": len(truncated),\n",
        "                    \"token_count\": len(token_log_probs),\n",
        "                    \"token_log_probs\": token_log_probs,\n",
        "                    \"cumulative_nll\": cumulative_nll,\n",
        "                    \"tokens\": tokens,\n",
        "                }\n",
        "                records.append(record)\n",
        "                success = True\n",
        "                log(f\"prompt {idx}: success at {limit} chars (token_count={record['token_count']})\")\n",
        "                break\n",
        "            except torch.cuda.OutOfMemoryError:\n",
        "                torch.cuda.empty_cache()\n",
        "                log(f\"prompt {idx}: OOM at {limit} chars, retrying shorter\")\n",
        "                continue\n",
        "        if not success:\n",
        "            failures.append({\"prompt_idx\": idx, \"reason\": \"all_attempts_failed\"})\n",
        "            log(f\"prompt {idx}: FAILED after attempts {attempt_limits}\")\n",
        "\n",
        "    elapsed = time.time() - start_time\n",
        "    with open(jsonl_path, \"w\", encoding=\"utf-8\") as writer:\n",
        "        for record in records:\n",
        "            writer.write(json.dumps(record) + \"\\n\")\n",
        "    with open(summary_path, \"w\", encoding=\"utf-8\") as writer:\n",
        "        json.dump(\n",
        "            {\n",
        "                \"model\": args.model_name,\n",
        "                \"variant\": args.variant_name,\n",
        "                \"group\": args.group_name,\n",
        "                \"run_id\": args.run_id,\n",
        "                \"prompt_count\": len(prompts),\n",
        "                \"records\": len(records),\n",
        "                \"failures\": failures,\n",
        "                \"attempt_char_limits\": attempt_limits,\n",
        "                \"elapsed_sec\": elapsed,\n",
        "            },\n",
        "            writer,\n",
        "            indent=2,\n",
        "        )\n",
        "    log(f\"Completed run {args.run_id} in {elapsed:.1f}s (records={len(records)}, failures={len(failures)})\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "REMOTE_SCRIPT_BODY = textwrap.dedent(\n",
        "    REMOTE_SCRIPT_TEMPLATE.substitute(MIN_TOKENS=MIN_PROMPT_TOKENS)\n",
        ")\n",
        "\n",
        "\n",
        "def sync_remote_runner_script() -> None:\n",
        "    payload = REMOTE_SCRIPT_BODY.strip() + \"\\n\"\n",
        "    heredoc = f\"cat <<'PY' > {shlex.quote(REMOTE_SCRIPT_PATH)}\\n{payload}\\nPY\\nchmod +x {shlex.quote(REMOTE_SCRIPT_PATH)}\"\n",
        "    run_ssh(heredoc)\n",
        "    print(f\"Uploaded remote helper to {REMOTE_SCRIPT_PATH}\")\n",
        "\n",
        "\n",
        "sync_remote_runner_script()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ALL_REMOTE_MODELS = sorted({spec.base for spec in MODEL_SPECS} | {spec.sft for spec in MODEL_SPECS})\n",
        "\n",
        "\n",
        "def prefetch_models_on_remote() -> None:\n",
        "    for model_name in tqdm(ALL_REMOTE_MODELS, desc=\"Prefetch models\"):\n",
        "        script = textwrap.dedent(\n",
        "            f\"\"\"\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "model_name = \\\"{model_name}\\\"\n",
        "print(f\\\"[prefetch] tokenizer {model_name}\\\")\n",
        "AutoTokenizer.from_pretrained(model_name)\n",
        "print(f\\\"[prefetch] model {model_name}\\\")\n",
        "AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16, device_map=\\\"cpu\\\")\n",
        "print(f\\\"[prefetch] done {model_name}\\\")\n",
        "\"\"\"\n",
        "        ).strip()\n",
        "        cmd = (\n",
        "            f\"{REMOTE_PYTHON} - <<'PY'\\n{script}\\nPY\"\n",
        "        )\n",
        "        run_ssh(cmd)\n",
        "\n",
        "\n",
        "# Uncomment to predownload everything before running prompts\n",
        "# prefetch_models_on_remote()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "RUN_RECORDS: List[Dict[str, object]] = []\n",
        "\n",
        "\n",
        "def build_run_id(spec: ModelSpec, variant: str, group: str) -> str:\n",
        "    timestamp = datetime.utcnow().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    return f\"{timestamp}_{spec.name}_{variant}_{group}\"\n",
        "\n",
        "\n",
        "def remote_output_dir(spec: ModelSpec, variant: str, group: str) -> str:\n",
        "    return f\"{REMOTE_OUTPUT_ROOT}/{group}/{spec.name}/{variant}\"\n",
        "\n",
        "\n",
        "def local_output_dir(spec: ModelSpec, variant: str, group: str) -> Path:\n",
        "    path = LOCAL_OUTPUT_ROOT / group / spec.name / variant\n",
        "    path.mkdir(parents=True, exist_ok=True)\n",
        "    return path\n",
        "\n",
        "\n",
        "def run_remote_logprob_job(spec: ModelSpec, variant: str, group: str, *, dry_run: bool = True) -> None:\n",
        "    model_name = spec.base if variant == \"base\" else spec.sft\n",
        "    tokenizer_name = spec.tokenizer\n",
        "    remote_prompt = REMOTE_PROMPT_MAP[group][\"remote_path\"]\n",
        "    run_id = build_run_id(spec, variant, group)\n",
        "    remote_dir = remote_output_dir(spec, variant, group)\n",
        "    ensure_remote_dir(remote_dir)\n",
        "    attempt_arg = \",\".join(str(val) for val in ATTEMPT_CHAR_LIMITS)\n",
        "    arg_pairs = [\n",
        "        (\"--model-name\", model_name),\n",
        "        (\"--tokenizer-name\", tokenizer_name),\n",
        "        (\"--prompt-file\", remote_prompt),\n",
        "        (\"--output-dir\", remote_dir),\n",
        "        (\"--attempt-char-limits\", attempt_arg),\n",
        "        (\"--min-tokens\", str(MIN_PROMPT_TOKENS)),\n",
        "        (\"--dtype\", spec.dtype),\n",
        "        (\"--device\", spec.device),\n",
        "        (\"--run-id\", run_id),\n",
        "        (\"--group-name\", group),\n",
        "        (\"--variant-name\", variant),\n",
        "    ]\n",
        "    arg_str = \" \".join(f\"{flag} {shlex.quote(value)}\" for flag, value in arg_pairs)\n",
        "    remote_cmd = textwrap.dedent(\n",
        "        f\"\"\"\n",
        "set -euo pipefail\n",
        "{REMOTE_PYTHON} {shlex.quote(REMOTE_SCRIPT_PATH)} {arg_str}\n",
        "\"\"\"\n",
        "    ).strip()\n",
        "    print(f\"[run] {spec.name} ({variant}) group={group} -> {remote_dir}\")\n",
        "    if dry_run:\n",
        "        print(remote_cmd)\n",
        "        return\n",
        "    run_ssh(remote_cmd)\n",
        "    local_dir = local_output_dir(spec, variant, group)\n",
        "    pull_remote_tree(remote_dir, local_dir)\n",
        "    RUN_RECORDS.append(\n",
        "        {\n",
        "            \"model\": spec.name,\n",
        "            \"variant\": variant,\n",
        "            \"group\": group,\n",
        "            \"run_id\": run_id,\n",
        "            \"remote_dir\": remote_dir,\n",
        "            \"local_dir\": str(local_dir),\n",
        "        }\n",
        "    )\n",
        "\n",
        "\n",
        "def execute_full_sweep(*, dry_run: bool = True) -> None:\n",
        "    for spec in MODEL_SPECS:\n",
        "        for variant in (\"base\", \"sft\"):\n",
        "            for group in REMOTE_PROMPT_MAP.keys():\n",
        "                run_remote_logprob_job(spec, variant, group, dry_run=dry_run)\n",
        "\n",
        "\n",
        "# Preview commands without running remote work\n",
        "execute_full_sweep(dry_run=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "MANIFEST_PATH = LOCAL_OUTPUT_ROOT / \"manifest.csv\"\n",
        "\n",
        "\n",
        "def load_existing_manifest() -> pd.DataFrame:\n",
        "    if MANIFEST_PATH.exists():\n",
        "        return pd.read_csv(MANIFEST_PATH)\n",
        "    return pd.DataFrame(columns=[\"model\", \"variant\", \"group\", \"run_id\", \"local_dir\", \"remote_dir\", \"timestamp\"])\n",
        "\n",
        "\n",
        "def update_manifest(records: Optional[List[Dict[str, object]]] = None) -> pd.DataFrame:\n",
        "    df = load_existing_manifest()\n",
        "    entries = records or RUN_RECORDS\n",
        "    if not entries:\n",
        "        return df\n",
        "    new_rows = []\n",
        "    for entry in entries:\n",
        "        new_rows.append(\n",
        "            {\n",
        "                \"model\": entry[\"model\"],\n",
        "                \"variant\": entry[\"variant\"],\n",
        "                \"group\": entry[\"group\"],\n",
        "                \"run_id\": entry[\"run_id\"],\n",
        "                \"local_dir\": entry[\"local_dir\"],\n",
        "                \"remote_dir\": entry[\"remote_dir\"],\n",
        "                \"timestamp\": datetime.utcnow().isoformat() + \"Z\",\n",
        "            }\n",
        "        )\n",
        "    df = pd.concat([df, pd.DataFrame(new_rows)], ignore_index=True)\n",
        "    df.to_csv(MANIFEST_PATH, index=False)\n",
        "    return df\n",
        "\n",
        "\n",
        "def summarize_local_outputs() -> pd.DataFrame:\n",
        "    rows = []\n",
        "    for spec in MODEL_SPECS:\n",
        "        for variant in (\"base\", \"sft\"):\n",
        "            for group in PROMPT_CACHE.keys():\n",
        "                local_dir = LOCAL_OUTPUT_ROOT / group / spec.name / variant\n",
        "                jsonls = sorted(local_dir.glob(\"*.jsonl\")) if local_dir.exists() else []\n",
        "                summaries = sorted(local_dir.glob(\"summary_*.json\")) if local_dir.exists() else []\n",
        "                rows.append(\n",
        "                    {\n",
        "                        \"model\": spec.name,\n",
        "                        \"variant\": variant,\n",
        "                        \"group\": group,\n",
        "                        \"jsonl_files\": len(jsonls),\n",
        "                        \"summary_files\": len(summaries),\n",
        "                        \"local_dir\": str(local_dir),\n",
        "                    }\n",
        "                )\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "\n",
        "# Call after pulling remote outputs to refresh the manifest\n",
        "# manifest_df = update_manifest()\n",
        "# manifest_df\n",
        "\n",
        "# summarize_local_outputs()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Run + Manifest Checklist\n",
        "\n",
        "1. Run `prefetch_models_on_remote()` once to cache checkpoints.\n",
        "2. Set `execute_full_sweep(dry_run=False)` when ready to launch all model/group jobs. Monitor `run_*.log` files in each remote output directory.\n",
        "3. After each job completes, call `pull_remote_tree(...)` or rerun `execute_full_sweep` for remaining entries (the helper skips completed directories).\n",
        "4. Use `update_manifest()` to append the latest records and `summarize_local_outputs()` to verify that every group/model/variant has JSONL + summary artifacts under `notebooks/h200_outputs_perplexity/`.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "VASTAI-SSH-JUPYTER-PYTORCH-ENV",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
