# Production Configuration
# Full-scale training configuration for powerful GPU instances
# Use this for final training runs on expensive instances

# Model Configuration
model:
  name: "production_model"
  params: 100000000  # 100M parameters
  hidden_size: 1024
  num_layers: 12
  dropout: 0.1

# Data Configuration
data:
  path: "/workspace/data"
  batch_size: 64  # Larger batch size for production
  num_workers: 8  # More workers for faster data loading
  sequence_length: 2048
  max_samples: null  # Use full dataset

# Training Configuration
training:
  max_epochs: 100
  learning_rate: 1e-4
  weight_decay: 0.01
  grad_clip: 1.0
  warmup_steps: 1000
  checkpoint_interval: 2000  # Less frequent checkpoints in prod
  log_interval: 100
  save_best: true

# Validation Configuration
validation:
  interval: 1  # Validate every epoch
  batch_size: 128

# Device Configuration
device:
  use_cuda: true
  cuda_device: 0

# Experiment Tracking
logging:
  use_wandb: true  # Enable in production
  wandb_project: "vastai-production"
  use_tensorboard: true
  tensorboard_dir: "/workspace/logs/tensorboard"

# Checkpoint Configuration
checkpoint:
  local_dir: "/workspace/persistent/checkpoints"
  s3_bucket: "your-s3-bucket-name"  # Set to your S3 bucket
  s3_prefix: "checkpoints/prod"
  keep_last_n: 10  # Keep more checkpoints in prod

# Cost Tracking
cost:
  hourly_rate: 2.00  # Estimated hourly rate for production instance
  instance_type: "A100"  # Example

# Development Flags
dev_mode: false
dry_run: false

# Residual Comparison Experiment
residual_compare:
  base_model: "gpt2-xl"
  sft_model: "gpt2-xl"
  tokenizer: null
  device: "cuda"
  dtype: "bfloat16"
  top_k: 50
  prompt_file: "experiments/prompts/prod_prompts.txt"
  prompts:
    - "Summarize the economic outlook for the eurozone in 2024."
  # Keep fuzzing/perturbations opt-in in production as well.
  prompt_variants:
    - identity
  prompt_variant_options:
    dual_channel:
      base_prefix: "BASE CONTEXT:\\n"
      sft_prefix: "SFT CONTEXT:\\n"
      order: "sft_first"
  tracked_token_strings:
    - "ĠYes"
    - "ĠNo"
    - "Ġrefuse"
  interesting_token_map:
    eos: "<|endoftext|>"
  local_run:
    enabled: false
    min_vram_gb: 24
    require_gpu_name_substring: "4090"
    cuda_device_index: 0
    allowed_pairs:
      - base: "gpt2-xl"
        sft: "gpt2-xl"
        device: "cuda"
        dtype: "float16"
        model_kwargs:
          attn_implementation: "eager"
  embedding_variants:
    - name: "default"
      base:
        embedding_source: "base"
        unembedding_source: "base"
      sft:
        embedding_source: "sft"
        unembedding_source: "sft"
  multi_pass:
    enabled: false
    runs:
      - name: "BB"
        model: "base"
        embedding_source: "base"
      - name: "BS"
        model: "sft"
        embedding_source: "base"
      - name: "SB"
        model: "base"
        embedding_source: "sft"
      - name: "SS"
        model: "sft"
        embedding_source: "sft"
    pairwise_differences:
      - ["BB", "BS"]
      - ["BB", "SB"]
      - ["BB", "SS"]
      - ["BS", "SB"]
      - ["BS", "SS"]
      - ["SB", "SS"]
    - name: "shared_unembedding"
      base:
        embedding_source: "base"
        unembedding_source: "sft"
      sft:
        embedding_source: "sft"
        unembedding_source: "sft"
    - name: "shared_embeddings"
      base:
        embedding_source: "sft"
        unembedding_source: "base"
      sft:
        embedding_source: "sft"
        unembedding_source: "sft"
