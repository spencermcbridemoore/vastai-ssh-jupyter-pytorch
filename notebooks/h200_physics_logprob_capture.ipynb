{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# H200 Physics Prompt Logprob Capture\n",
        "\n",
        "This notebook orchestrates log-probability sweeps for the Physics A/B prompt suites on a Vast H200 instance. It mirrors the residual workflows but focuses solely on per-token logprobs so that perplexity for any prefix length can be reconstructed offline.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Usage Overview\n",
        "\n",
        "1. Detect or specify the target Vast H200 instance.\n",
        "2. Prefetch all required checkpoints before running any prompts.\n",
        "3. Filter the Physics A/B prompt files to drop items under 30 tokens.\n",
        "4. Stage the filtered prompts and remote helper script on the instance.\n",
        "5. For each model (base + SFT), attempt 1000/800/600/400 character truncations until a run succeeds, collecting JSONL logprob traces under `notebooks/h200_outputs_perplexity/`.\n",
        "6. Pull artifacts back locally and update the manifest to track which models/groups completed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Repo root: C:\\Users\\spenc\\Cursor Repos\\vastai-ssh-jupyter-pytorch\n",
            "Local output root: C:\\Users\\spenc\\Cursor Repos\\vastai-ssh-jupyter-pytorch\\notebooks\\h200_outputs_perplexity\n",
            "Vast instance: 28352791 -> root@208.64.254.75:16693\n"
          ]
        }
      ],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import json\n",
        "import os\n",
        "import shlex\n",
        "import subprocess\n",
        "import textwrap\n",
        "from dataclasses import dataclass\n",
        "from pathlib import Path, PurePosixPath\n",
        "from string import Template\n",
        "from typing import Dict, List, Optional, Sequence\n",
        "\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "REPO_ROOT = Path(\"..\").resolve()\n",
        "if not (REPO_ROOT / \"experiments\").exists():\n",
        "    REPO_ROOT = Path.cwd().resolve()\n",
        "\n",
        "NOTEBOOK_DIR = REPO_ROOT / \"notebooks\"\n",
        "LOCAL_OUTPUT_ROOT = NOTEBOOK_DIR / \"h200_outputs_perplexity\"\n",
        "LOCAL_OUTPUT_ROOT.mkdir(parents=True, exist_ok=True)\n",
        "LOCAL_PROMPT_CACHE_DIR = LOCAL_OUTPUT_ROOT / \"_prompt_cache\"\n",
        "LOCAL_PROMPT_CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "REMOTE_REPO = os.environ.get(\"H200_REMOTE_REPO\", \"/workspace/vastai-ssh-jupyter-pytorch\")\n",
        "REMOTE_REPO_PATH = PurePosixPath(REMOTE_REPO)\n",
        "REMOTE_OUTPUT_ROOT = str(REMOTE_REPO_PATH / \"notebooks\" / \"h200_outputs_perplexity\")\n",
        "REMOTE_PROMPT_DIR = f\"{REMOTE_OUTPUT_ROOT}/prompts\"\n",
        "REMOTE_SCRIPT_PATH = str(REMOTE_REPO_PATH / \"scripts\" / \"physics_logprob_remote.py\")\n",
        "REMOTE_PYTHON = os.environ.get(\"H200_REMOTE_PYTHON\", \"python3\")\n",
        "\n",
        "ATTEMPT_CHAR_LIMITS = [1000, 800, 600, 400]\n",
        "MIN_PROMPT_TOKENS = 30\n",
        "PROMPT_LIMIT = int(os.environ.get(\"H200_PHYSICS_PROMPT_LIMIT\", \"100\"))\n",
        "PROMPT_SOURCES = {\n",
        "    \"physics_A\": REPO_ROOT / \"experiments\" / \"prompts\" / \"physics_answers_A.txt\",\n",
        "    \"physics_B\": REPO_ROOT / \"experiments\" / \"prompts\" / \"physics_answers_B.txt\",\n",
        "}\n",
        "\n",
        "\n",
        "def detect_vast_instance(preferred_id: Optional[str]) -> Dict[str, Optional[str]]:\n",
        "    try:\n",
        "        result = subprocess.run(\n",
        "            [\"vastai\", \"show\", \"instances\"],\n",
        "            capture_output=True,\n",
        "            text=True,\n",
        "            check=True,\n",
        "        )\n",
        "    except (FileNotFoundError, subprocess.CalledProcessError):\n",
        "        return {\"instance_id\": preferred_id, \"ssh_host\": None, \"ssh_port\": None}\n",
        "\n",
        "    lines = [line.strip() for line in result.stdout.splitlines() if line.strip()]\n",
        "    rows = [line for line in lines if line and line[0].isdigit()]\n",
        "    target_row = None\n",
        "    for row in rows:\n",
        "        parts = row.split()\n",
        "        if not parts:\n",
        "            continue\n",
        "        row_id = parts[0]\n",
        "        if preferred_id and row_id == preferred_id:\n",
        "            target_row = parts\n",
        "            break\n",
        "        if target_row is None:\n",
        "            target_row = parts\n",
        "    if not target_row or len(target_row) < 11:\n",
        "        return {\n",
        "            \"instance_id\": preferred_id or (target_row[0] if target_row else None),\n",
        "            \"ssh_host\": None,\n",
        "            \"ssh_port\": None,\n",
        "        }\n",
        "\n",
        "    return {\n",
        "        \"instance_id\": target_row[0],\n",
        "        \"ssh_host\": target_row[9],\n",
        "        \"ssh_port\": target_row[10],\n",
        "    }\n",
        "\n",
        "\n",
        "def fetch_ssh_settings(instance_id: Optional[str]) -> Dict[str, Optional[str]]:\n",
        "    if not instance_id:\n",
        "        return {}\n",
        "    try:\n",
        "        result = subprocess.run(\n",
        "            [\"vastai\", \"ssh-url\", instance_id],\n",
        "            capture_output=True,\n",
        "            text=True,\n",
        "            check=True,\n",
        "        )\n",
        "    except (FileNotFoundError, subprocess.CalledProcessError):\n",
        "        return {}\n",
        "    from urllib.parse import urlparse\n",
        "\n",
        "    url = result.stdout.strip()\n",
        "    if not url:\n",
        "        return {}\n",
        "    parsed = urlparse(url)\n",
        "    if not parsed.hostname:\n",
        "        return {}\n",
        "    return {\n",
        "        \"ssh_user\": parsed.username or \"root\",\n",
        "        \"ssh_host\": parsed.hostname,\n",
        "        \"ssh_port\": str(parsed.port) if parsed.port else None,\n",
        "    }\n",
        "\n",
        "\n",
        "detected = detect_vast_instance(os.environ.get(\"H200_INSTANCE_ID\"))\n",
        "INSTANCE_ID = os.environ.get(\"H200_INSTANCE_ID\") or detected.get(\"instance_id\")\n",
        "ssh_url = fetch_ssh_settings(INSTANCE_ID)\n",
        "SSH_USER = os.environ.get(\"H200_SSH_USER\") or ssh_url.get(\"ssh_user\") or \"root\"\n",
        "SSH_HOST = os.environ.get(\"H200_SSH_HOST\") or ssh_url.get(\"ssh_host\") or detected.get(\"ssh_host\") or \"ssh.vast.ai\"\n",
        "SSH_PORT = int(os.environ.get(\"H200_SSH_PORT\") or ssh_url.get(\"ssh_port\") or detected.get(\"ssh_port\") or \"22\")\n",
        "_identity_env = os.environ.get(\"H200_SSH_IDENTITY\")\n",
        "default_identity = Path.home() / \".ssh\" / \"id_rsa\"\n",
        "if _identity_env:\n",
        "    SSH_IDENTITY: Optional[Path] = Path(_identity_env).expanduser()\n",
        "elif default_identity.exists():\n",
        "    SSH_IDENTITY = default_identity\n",
        "else:\n",
        "    SSH_IDENTITY = None\n",
        "\n",
        "print(f\"Repo root: {REPO_ROOT}\")\n",
        "print(f\"Local output root: {LOCAL_OUTPUT_ROOT}\")\n",
        "print(f\"Vast instance: {INSTANCE_ID} -> {SSH_USER}@{SSH_HOST}:{SSH_PORT}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tracking 10 model pairs.\n"
          ]
        }
      ],
      "source": [
        "@dataclass(frozen=True)\n",
        "class ModelSpec:\n",
        "    name: str\n",
        "    base: str\n",
        "    sft: str\n",
        "    tokenizer: str\n",
        "    dtype: str\n",
        "    device: str = \"cuda:0\"\n",
        "    notes: str = \"\"\n",
        "\n",
        "\n",
        "MODEL_SPECS: List[ModelSpec] = [\n",
        "    ModelSpec(\n",
        "        name=\"qwen-0_5b\",\n",
        "        base=\"Qwen/Qwen2.5-0.5B\",\n",
        "        sft=\"Qwen/Qwen2.5-0.5B-Instruct\",\n",
        "        tokenizer=\"Qwen/Qwen2.5-0.5B-Instruct\",\n",
        "        dtype=\"float16\",\n",
        "        notes=\"Tiny sanity check; should finish within minutes.\",\n",
        "    ),\n",
        "    ModelSpec(\n",
        "        name=\"qwen-1_5b\",\n",
        "        base=\"Qwen/Qwen2.5-1.5B\",\n",
        "        sft=\"Qwen/Qwen2.5-1.5B-Instruct\",\n",
        "        tokenizer=\"Qwen/Qwen2.5-1.5B-Instruct\",\n",
        "        dtype=\"float16\",\n",
        "        notes=\"Requires ~12-16 GB; still lightweight on H200.\",\n",
        "    ),\n",
        "    ModelSpec(\n",
        "        name=\"qwen-3b\",\n",
        "        base=\"Qwen/Qwen2.5-3B\",\n",
        "        sft=\"Qwen/Qwen2.5-3B-Instruct\",\n",
        "        tokenizer=\"Qwen/Qwen2.5-3B-Instruct\",\n",
        "        dtype=\"float16\",\n",
        "        notes=\"Comfortably fits on an H200 in fp16.\",\n",
        "    ),\n",
        "    ModelSpec(\n",
        "        name=\"qwen-7b\",\n",
        "        base=\"Qwen/Qwen2.5-7B\",\n",
        "        sft=\"Qwen/Qwen2.5-7B-Instruct\",\n",
        "        tokenizer=\"Qwen/Qwen2.5-7B-Instruct\",\n",
        "        dtype=\"bfloat16\",\n",
        "        notes=\"Use bf16 for stability on longer contexts.\",\n",
        "    ),\n",
        "    ModelSpec(\n",
        "        name=\"qwen-14b\",\n",
        "        base=\"Qwen/Qwen2.5-14B\",\n",
        "        sft=\"Qwen/Qwen2.5-14B-Instruct\",\n",
        "        tokenizer=\"Qwen/Qwen2.5-14B-Instruct\",\n",
        "        dtype=\"bfloat16\",\n",
        "        notes=\"Large but still single-GPU on H200.\",\n",
        "    ),\n",
        "    ModelSpec(\n",
        "        name=\"qwen-32b\",\n",
        "        base=\"Qwen/Qwen2.5-32B\",\n",
        "        sft=\"Qwen/Qwen2.5-32B-Instruct\",\n",
        "        tokenizer=\"Qwen/Qwen2.5-32B-Instruct\",\n",
        "        dtype=\"bfloat16\",\n",
        "        notes=\"Heavyweight pair; expect slower throughput.\",\n",
        "    ),\n",
        "    ModelSpec(\n",
        "        name=\"qwen-72b\",\n",
        "        base=\"Qwen/Qwen2.5-72B\",\n",
        "        sft=\"Qwen/Qwen2.5-72B-Instruct\",\n",
        "        tokenizer=\"Qwen/Qwen2.5-72B-Instruct\",\n",
        "        dtype=\"bfloat16\",\n",
        "        notes=\"Pushes memory; keep batch size at 1.\",\n",
        "    ),\n",
        "    ModelSpec(\n",
        "        name=\"llama-8b\",\n",
        "        base=\"meta-llama/Llama-3.1-8B\",\n",
        "        sft=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
        "        tokenizer=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
        "        dtype=\"bfloat16\",\n",
        "        notes=\"Requires HF auth; set HUGGINGFACE_TOKEN remotely.\",\n",
        "    ),\n",
        "    ModelSpec(\n",
        "        name=\"llama-70b\",\n",
        "        base=\"meta-llama/Llama-3.1-70B\",\n",
        "        sft=\"meta-llama/Llama-3.1-70B-Instruct\",\n",
        "        tokenizer=\"meta-llama/Llama-3.1-70B-Instruct\",\n",
        "        dtype=\"bfloat16\",\n",
        "        notes=\"Plan for long download time (~200 GB).\",\n",
        "    ),\n",
        "    ModelSpec(\n",
        "        name=\"mistral-8b\",\n",
        "        base=\"nvidia/Mistral-NeMo-Minitron-8B-Base\",\n",
        "        sft=\"nvidia/Mistral-NeMo-Minitron-8B-Instruct\",\n",
        "        tokenizer=\"nvidia/Mistral-NeMo-Minitron-8B-Instruct\",\n",
        "        dtype=\"bfloat16\",\n",
        "        notes=\"NVIDIA NeMo release; requires NGC credential if private.\",\n",
        "    ),\n",
        "]\n",
        "\n",
        "print(f\"Tracking {len(MODEL_SPECS)} model pairs.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Group physics_A: kept 100 prompts (>= 30 tokens)\n",
            "Group physics_B: kept 100 prompts (>= 30 tokens)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'physics_A': {'local_path': WindowsPath('C:/Users/spenc/Cursor Repos/vastai-ssh-jupyter-pytorch/notebooks/h200_outputs_perplexity/_prompt_cache/physics_A.txt'),\n",
              "  'count': 100},\n",
              " 'physics_B': {'local_path': WindowsPath('C:/Users/spenc/Cursor Repos/vastai-ssh-jupyter-pytorch/notebooks/h200_outputs_perplexity/_prompt_cache/physics_B.txt'),\n",
              "  'count': 100}}"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "FALLBACK_TOKENIZER_NAME = os.environ.get(\"H200_FALLBACK_TOKENIZER\", \"Qwen/Qwen2.5-0.5B-Instruct\")\n",
        "FALLBACK_TOKENIZER = AutoTokenizer.from_pretrained(FALLBACK_TOKENIZER_NAME)\n",
        "if FALLBACK_TOKENIZER.pad_token is None:\n",
        "    FALLBACK_TOKENIZER.pad_token = FALLBACK_TOKENIZER.eos_token\n",
        "\n",
        "PROMPT_CACHE: Dict[str, Dict[str, object]] = {}\n",
        "\n",
        "\n",
        "def truncate_to_char_limit(text: str, limit: int) -> str:\n",
        "    if len(text) <= limit:\n",
        "        return text\n",
        "    return text[:limit]\n",
        "\n",
        "\n",
        "def load_and_filter_prompts(path: Path, limit: int) -> List[str]:\n",
        "    filtered: List[str] = []\n",
        "    with path.open(\"r\", encoding=\"utf-8\") as handle:\n",
        "        for line in handle:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "            tokenized = FALLBACK_TOKENIZER(line, add_special_tokens=False)\n",
        "            if len(tokenized[\"input_ids\"]) < MIN_PROMPT_TOKENS:\n",
        "                continue\n",
        "            filtered.append(line)\n",
        "            if len(filtered) >= limit:\n",
        "                break\n",
        "    return filtered\n",
        "\n",
        "\n",
        "def refresh_prompt_cache() -> None:\n",
        "    PROMPT_CACHE.clear()\n",
        "    for group, src_path in PROMPT_SOURCES.items():\n",
        "        prompts = load_and_filter_prompts(src_path, PROMPT_LIMIT)\n",
        "        cache_path = LOCAL_PROMPT_CACHE_DIR / f\"{group}.txt\"\n",
        "        with cache_path.open(\"w\", encoding=\"utf-8\") as handle:\n",
        "            for prompt in prompts:\n",
        "                handle.write(prompt + \"\\n\")\n",
        "        PROMPT_CACHE[group] = {\n",
        "            \"local_path\": cache_path,\n",
        "            \"count\": len(prompts),\n",
        "        }\n",
        "        print(f\"Group {group}: kept {len(prompts)} prompts (>= {MIN_PROMPT_TOKENS} tokens)\")\n",
        "\n",
        "\n",
        "refresh_prompt_cache()\n",
        "PROMPT_CACHE\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Remote directories ready.\n"
          ]
        }
      ],
      "source": [
        "def _ssh_base() -> List[str]:\n",
        "    parts = [\"ssh\", \"-p\", str(SSH_PORT)]\n",
        "    if SSH_IDENTITY:\n",
        "        parts += [\"-i\", str(SSH_IDENTITY)]\n",
        "    parts += [f\"{SSH_USER}@{SSH_HOST}\"]\n",
        "    return parts\n",
        "\n",
        "\n",
        "def run_ssh(command: str, *, check: bool = True, capture_output: bool = False) -> subprocess.CompletedProcess:\n",
        "    full_cmd = _ssh_base() + [command]\n",
        "    return subprocess.run(\n",
        "        full_cmd,\n",
        "        check=check,\n",
        "        capture_output=capture_output,\n",
        "        text=True,\n",
        "    )\n",
        "\n",
        "\n",
        "def run_local(command: Sequence[str], *, check: bool = True) -> subprocess.CompletedProcess:\n",
        "    return subprocess.run(command, check=check)\n",
        "\n",
        "\n",
        "def ensure_remote_dir(path: str) -> None:\n",
        "    run_ssh(f\"mkdir -p {shlex.quote(path)}\")\n",
        "\n",
        "\n",
        "def push_file(local_path: Path, remote_path: str) -> None:\n",
        "    cmd = [\"scp\", \"-P\", str(SSH_PORT)]\n",
        "    if SSH_IDENTITY:\n",
        "        cmd += [\"-i\", str(SSH_IDENTITY)]\n",
        "    cmd += [str(local_path), f\"{SSH_USER}@{SSH_HOST}:{remote_path}\"]\n",
        "    run_local(cmd)\n",
        "\n",
        "\n",
        "def pull_remote_tree(remote_path: str, local_path: Path) -> None:\n",
        "    local_path.mkdir(parents=True, exist_ok=True)\n",
        "    cmd = [\"scp\", \"-r\", \"-P\", str(SSH_PORT)]\n",
        "    if SSH_IDENTITY:\n",
        "        cmd += [\"-i\", str(SSH_IDENTITY)]\n",
        "    cmd += [f\"{SSH_USER}@{SSH_HOST}:{remote_path}\", str(local_path)]\n",
        "    run_local(cmd)\n",
        "\n",
        "\n",
        "ensure_remote_dir(REMOTE_OUTPUT_ROOT)\n",
        "ensure_remote_dir(REMOTE_PROMPT_DIR)\n",
        "print(\"Remote directories ready.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bbffe272f5a240e494949f121431aeda",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Staging prompt files:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "physics_A: uploading physics_A.txt → /workspace/vastai-ssh-jupyter-pytorch/notebooks/h200_outputs_perplexity/prompts/physics_A.txt\n",
            "physics_A: upload complete (100 prompts)\n",
            "physics_B: uploading physics_B.txt → /workspace/vastai-ssh-jupyter-pytorch/notebooks/h200_outputs_perplexity/prompts/physics_B.txt\n",
            "physics_B: upload complete (100 prompts)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'physics_A': {'remote_path': '/workspace/vastai-ssh-jupyter-pytorch/notebooks/h200_outputs_perplexity/prompts/physics_A.txt',\n",
              "  'local_path': 'C:\\\\Users\\\\spenc\\\\Cursor Repos\\\\vastai-ssh-jupyter-pytorch\\\\notebooks\\\\h200_outputs_perplexity\\\\_prompt_cache\\\\physics_A.txt',\n",
              "  'count': 100},\n",
              " 'physics_B': {'remote_path': '/workspace/vastai-ssh-jupyter-pytorch/notebooks/h200_outputs_perplexity/prompts/physics_B.txt',\n",
              "  'local_path': 'C:\\\\Users\\\\spenc\\\\Cursor Repos\\\\vastai-ssh-jupyter-pytorch\\\\notebooks\\\\h200_outputs_perplexity\\\\_prompt_cache\\\\physics_B.txt',\n",
              "  'count': 100}}"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from tqdm.auto import tqdm\n",
        "\n",
        "def stage_prompts_remote(show_progress: bool = True) -> Dict[str, Dict[str, str]]:\n",
        "    ensure_remote_dir(REMOTE_PROMPT_DIR)\n",
        "    summary: Dict[str, Dict[str, str]] = {}\n",
        "\n",
        "    iterator = PROMPT_CACHE.items()\n",
        "    if show_progress:\n",
        "        iterator = tqdm(\n",
        "            iterator,\n",
        "            desc=\"Staging prompt files\",\n",
        "            total=len(PROMPT_CACHE),\n",
        "            leave=True,\n",
        "        )\n",
        "\n",
        "    for group, meta in iterator:\n",
        "        remote_path = f\"{REMOTE_PROMPT_DIR}/{group}.txt\"\n",
        "        message = f\"{group}: uploading {meta['local_path'].name} → {remote_path}\"\n",
        "        if show_progress:\n",
        "            tqdm.write(message)\n",
        "        else:\n",
        "            print(message)\n",
        "\n",
        "        push_file(meta[\"local_path\"], remote_path)\n",
        "\n",
        "        done_msg = f\"{group}: upload complete ({meta['count']} prompts)\"\n",
        "        if show_progress:\n",
        "            tqdm.write(done_msg)\n",
        "        else:\n",
        "            print(done_msg)\n",
        "\n",
        "        summary[group] = {\n",
        "            \"remote_path\": remote_path,\n",
        "            \"local_path\": str(meta[\"local_path\"]),\n",
        "            \"count\": meta[\"count\"],\n",
        "        }\n",
        "\n",
        "    return summary\n",
        "\n",
        "\n",
        "REMOTE_PROMPT_MAP = stage_prompts_remote()\n",
        "REMOTE_PROMPT_MAP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Uploaded remote helper to /workspace/vastai-ssh-jupyter-pytorch/scripts/physics_logprob_remote.py\n"
          ]
        }
      ],
      "source": [
        "REMOTE_SCRIPT_TEMPLATE = Template(\n",
        "    \"\"\"\n",
        "#!/usr/bin/env python\n",
        "import argparse\n",
        "import json\n",
        "import time\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "\n",
        "def resolve_dtype(name: str) -> torch.dtype:\n",
        "    if not name:\n",
        "        return torch.float16\n",
        "    lowered = name.lower()\n",
        "    if lowered in {\"bfloat16\", \"bf16\"}:\n",
        "        return torch.bfloat16\n",
        "    return torch.float16\n",
        "\n",
        "\n",
        "def main() -> None:\n",
        "    parser = argparse.ArgumentParser(description=\"Physics prompt logprob capture\")\n",
        "    parser.add_argument(\"--model-name\", required=True)\n",
        "    parser.add_argument(\"--tokenizer-name\", required=True)\n",
        "    parser.add_argument(\"--prompt-file\", required=True)\n",
        "    parser.add_argument(\"--output-dir\", required=True)\n",
        "    parser.add_argument(\"--attempt-char-limits\", default=\"1000,800,600,400\")\n",
        "    parser.add_argument(\"--min-tokens\", type=int, default=$MIN_TOKENS)\n",
        "    parser.add_argument(\"--dtype\", default=\"float16\")\n",
        "    parser.add_argument(\"--device\", default=\"cuda:0\")\n",
        "    parser.add_argument(\"--run-id\", required=True)\n",
        "    parser.add_argument(\"--group-name\", required=True)\n",
        "    parser.add_argument(\"--variant-name\", required=True)\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    attempt_limits = [int(part.strip()) for part in args.attempt_char_limits.split(\",\") if part.strip()]\n",
        "    output_dir = Path(args.output_dir)\n",
        "    output_dir.mkdir(parents=True, exist_ok=True)\n",
        "    jsonl_path = output_dir / f\"logprobs_{args.run_id}.jsonl\"\n",
        "    summary_path = output_dir / f\"summary_{args.run_id}.json\"\n",
        "    log_path = output_dir / f\"run_{args.run_id}.log\"\n",
        "\n",
        "    dtype = resolve_dtype(args.dtype)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name)\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        args.model_name,\n",
        "        torch_dtype=dtype,\n",
        "        low_cpu_mem_usage=True,\n",
        "    ).to(args.device)\n",
        "    model.eval()\n",
        "\n",
        "    with open(args.prompt_file, \"r\", encoding=\"utf-8\") as handle:\n",
        "        prompts = [line.strip() for line in handle if line.strip()]\n",
        "\n",
        "    def log(message: str) -> None:\n",
        "        timestamp = datetime.utcnow().isoformat() + \"Z\"\n",
        "        payload = \"[{}] {}\\n\".format(timestamp, message)\n",
        "        with open(log_path, \"a\", encoding=\"utf-8\") as log_handle:\n",
        "            log_handle.write(payload)\n",
        "        print(payload, end=\"\")\n",
        "\n",
        "    records = []\n",
        "    failures = []\n",
        "    start_time = time.time()\n",
        "\n",
        "    for idx, prompt in enumerate(prompts):\n",
        "        success = False\n",
        "        for limit in attempt_limits:\n",
        "            truncated = prompt if len(prompt) <= limit else prompt[:limit]\n",
        "            encoded = tokenizer(\n",
        "                truncated,\n",
        "                return_tensors=\"pt\",\n",
        "                padding=False,\n",
        "                add_special_tokens=False,\n",
        "            )\n",
        "            seq_len = int(encoded[\"input_ids\"].shape[-1])\n",
        "            if seq_len < max(2, args.min_tokens):\n",
        "                continue\n",
        "            encoded = {key: value.to(args.device) for key, value in encoded.items()}\n",
        "            try:\n",
        "                with torch.no_grad():\n",
        "                    outputs = model(**encoded)\n",
        "                logits = outputs.logits[:, :-1, :]\n",
        "                target_ids = encoded[\"input_ids\"][:, 1:]\n",
        "                log_probs = torch.log_softmax(logits, dim=-1)\n",
        "                gathered = log_probs.gather(-1, target_ids.unsqueeze(-1)).squeeze(-1)\n",
        "                token_log_probs = gathered[0].tolist()\n",
        "                cumulative_nll = (-torch.cumsum(gathered[0], dim=0)).tolist()\n",
        "                token_ids = target_ids[0].tolist()\n",
        "                tokens = tokenizer.convert_ids_to_tokens(token_ids)\n",
        "                record = {\n",
        "                    \"model\": args.model_name,\n",
        "                    \"variant\": args.variant_name,\n",
        "                    \"group\": args.group_name,\n",
        "                    \"prompt_idx\": idx,\n",
        "                    \"char_limit\": limit,\n",
        "                    \"chars_used\": len(truncated),\n",
        "                    \"token_count\": len(token_log_probs),\n",
        "                    \"token_log_probs\": token_log_probs,\n",
        "                    \"cumulative_nll\": cumulative_nll,\n",
        "                    \"tokens\": tokens,\n",
        "                }\n",
        "                records.append(record)\n",
        "                success = True\n",
        "                log(f\"prompt {idx}: success at {limit} chars (token_count={record['token_count']})\")\n",
        "                break\n",
        "            except torch.cuda.OutOfMemoryError:\n",
        "                torch.cuda.empty_cache()\n",
        "                log(f\"prompt {idx}: OOM at {limit} chars, retrying shorter\")\n",
        "                continue\n",
        "        if not success:\n",
        "            failures.append({\"prompt_idx\": idx, \"reason\": \"all_attempts_failed\"})\n",
        "            log(f\"prompt {idx}: FAILED after attempts {attempt_limits}\")\n",
        "\n",
        "    elapsed = time.time() - start_time\n",
        "    with open(jsonl_path, \"w\", encoding=\"utf-8\") as writer:\n",
        "        for record in records:\n",
        "            writer.write(json.dumps(record) + \"\\n\")\n",
        "    with open(summary_path, \"w\", encoding=\"utf-8\") as writer:\n",
        "        json.dump(\n",
        "            {\n",
        "                \"model\": args.model_name,\n",
        "                \"variant\": args.variant_name,\n",
        "                \"group\": args.group_name,\n",
        "                \"run_id\": args.run_id,\n",
        "                \"prompt_count\": len(prompts),\n",
        "                \"records\": len(records),\n",
        "                \"failures\": failures,\n",
        "                \"attempt_char_limits\": attempt_limits,\n",
        "                \"elapsed_sec\": elapsed,\n",
        "            },\n",
        "            writer,\n",
        "            indent=2,\n",
        "        )\n",
        "    log(f\"Completed run {args.run_id} in {elapsed:.1f}s (records={len(records)}, failures={len(failures)})\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "REMOTE_SCRIPT_BODY = textwrap.dedent(\n",
        "    REMOTE_SCRIPT_TEMPLATE.substitute(MIN_TOKENS=MIN_PROMPT_TOKENS)\n",
        ")\n",
        "\n",
        "\n",
        "def sync_remote_runner_script() -> None:\n",
        "    payload = REMOTE_SCRIPT_BODY.strip() + \"\\n\"\n",
        "    heredoc = f\"cat <<'PY' > {shlex.quote(REMOTE_SCRIPT_PATH)}\\n{payload}\\nPY\\nchmod +x {shlex.quote(REMOTE_SCRIPT_PATH)}\"\n",
        "    run_ssh(heredoc)\n",
        "    print(f\"Uploaded remote helper to {REMOTE_SCRIPT_PATH}\")\n",
        "\n",
        "\n",
        "sync_remote_runner_script()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "ALL_REMOTE_MODELS = sorted({spec.base for spec in MODEL_SPECS} | {spec.sft for spec in MODEL_SPECS})\n",
        "\n",
        "\n",
        "def prefetch_models_on_remote() -> None:\n",
        "    for model_name in tqdm(ALL_REMOTE_MODELS, desc=\"Prefetch models\"):\n",
        "        script = textwrap.dedent(\n",
        "            f\"\"\"\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "model_name = \\\"{model_name}\\\"\n",
        "print(f\\\"[prefetch] tokenizer {model_name}\\\")\n",
        "AutoTokenizer.from_pretrained(model_name)\n",
        "print(f\\\"[prefetch] model {model_name}\\\")\n",
        "AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16, device_map=\\\"cpu\\\")\n",
        "print(f\\\"[prefetch] done {model_name}\\\")\n",
        "\"\"\"\n",
        "        ).strip()\n",
        "        cmd = (\n",
        "            f\"{REMOTE_PYTHON} - <<'PY'\\n{script}\\nPY\"\n",
        "        )\n",
        "        run_ssh(cmd)\n",
        "\n",
        "\n",
        "# Uncomment to predownload everything before running prompts\n",
        "# prefetch_models_on_remote()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "RUN_RECORDS: List[Dict[str, object]] = []\n",
        "\n",
        "\n",
        "def build_run_id(spec: ModelSpec, variant: str, group: str) -> str:\n",
        "    timestamp = datetime.utcnow().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    return f\"{timestamp}_{spec.name}_{variant}_{group}\"\n",
        "\n",
        "\n",
        "def remote_output_dir(spec: ModelSpec, variant: str, group: str) -> str:\n",
        "    return f\"{REMOTE_OUTPUT_ROOT}/{group}/{spec.name}/{variant}\"\n",
        "\n",
        "\n",
        "def local_output_dir(spec: ModelSpec, variant: str, group: str) -> Path:\n",
        "    path = LOCAL_OUTPUT_ROOT / group / spec.name / variant\n",
        "    path.mkdir(parents=True, exist_ok=True)\n",
        "    return path\n",
        "\n",
        "\n",
        "def run_remote_logprob_job(spec: ModelSpec, variant: str, group: str, *, dry_run: bool = True) -> None:\n",
        "    model_name = spec.base if variant == \"base\" else spec.sft\n",
        "    tokenizer_name = spec.tokenizer\n",
        "    remote_prompt = REMOTE_PROMPT_MAP[group][\"remote_path\"]\n",
        "    run_id = build_run_id(spec, variant, group)\n",
        "    remote_dir = remote_output_dir(spec, variant, group)\n",
        "    ensure_remote_dir(remote_dir)\n",
        "    attempt_arg = \",\".join(str(val) for val in ATTEMPT_CHAR_LIMITS)\n",
        "    arg_pairs = [\n",
        "        (\"--model-name\", model_name),\n",
        "        (\"--tokenizer-name\", tokenizer_name),\n",
        "        (\"--prompt-file\", remote_prompt),\n",
        "        (\"--output-dir\", remote_dir),\n",
        "        (\"--attempt-char-limits\", attempt_arg),\n",
        "        (\"--min-tokens\", str(MIN_PROMPT_TOKENS)),\n",
        "        (\"--dtype\", spec.dtype),\n",
        "        (\"--device\", spec.device),\n",
        "        (\"--run-id\", run_id),\n",
        "        (\"--group-name\", group),\n",
        "        (\"--variant-name\", variant),\n",
        "    ]\n",
        "    arg_str = \" \".join(f\"{flag} {shlex.quote(value)}\" for flag, value in arg_pairs)\n",
        "    remote_cmd = textwrap.dedent(\n",
        "        f\"\"\"\n",
        "set -euo pipefail\n",
        "{REMOTE_PYTHON} {shlex.quote(REMOTE_SCRIPT_PATH)} {arg_str}\n",
        "\"\"\"\n",
        "    ).strip()\n",
        "    print(f\"[run] {spec.name} ({variant}) group={group} -> {remote_dir}\")\n",
        "    if dry_run:\n",
        "        print(remote_cmd)\n",
        "        return\n",
        "    run_ssh(remote_cmd)\n",
        "    local_dir = local_output_dir(spec, variant, group)\n",
        "    pull_remote_tree(remote_dir, local_dir)\n",
        "    RUN_RECORDS.append(\n",
        "        {\n",
        "            \"model\": spec.name,\n",
        "            \"variant\": variant,\n",
        "            \"group\": group,\n",
        "            \"run_id\": run_id,\n",
        "            \"remote_dir\": remote_dir,\n",
        "            \"local_dir\": str(local_dir),\n",
        "        }\n",
        "    )\n",
        "\n",
        "\n",
        "def execute_full_sweep(*, dry_run: bool = True) -> None:\n",
        "    for spec in MODEL_SPECS:\n",
        "        for variant in (\"base\", \"sft\"):\n",
        "            for group in REMOTE_PROMPT_MAP.keys():\n",
        "                run_remote_logprob_job(spec, variant, group, dry_run=dry_run)\n",
        "\n",
        "\n",
        "# Preview commands without running remote work\n",
        "#execute_full_sweep(dry_run=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "MANIFEST_PATH = LOCAL_OUTPUT_ROOT / \"manifest.csv\"\n",
        "\n",
        "\n",
        "def load_existing_manifest() -> pd.DataFrame:\n",
        "    if MANIFEST_PATH.exists():\n",
        "        return pd.read_csv(MANIFEST_PATH)\n",
        "    return pd.DataFrame(columns=[\"model\", \"variant\", \"group\", \"run_id\", \"local_dir\", \"remote_dir\", \"timestamp\"])\n",
        "\n",
        "\n",
        "def update_manifest(records: Optional[List[Dict[str, object]]] = None) -> pd.DataFrame:\n",
        "    df = load_existing_manifest()\n",
        "    entries = records or RUN_RECORDS\n",
        "    if not entries:\n",
        "        return df\n",
        "    new_rows = []\n",
        "    for entry in entries:\n",
        "        new_rows.append(\n",
        "            {\n",
        "                \"model\": entry[\"model\"],\n",
        "                \"variant\": entry[\"variant\"],\n",
        "                \"group\": entry[\"group\"],\n",
        "                \"run_id\": entry[\"run_id\"],\n",
        "                \"local_dir\": entry[\"local_dir\"],\n",
        "                \"remote_dir\": entry[\"remote_dir\"],\n",
        "                \"timestamp\": datetime.utcnow().isoformat() + \"Z\",\n",
        "            }\n",
        "        )\n",
        "    df = pd.concat([df, pd.DataFrame(new_rows)], ignore_index=True)\n",
        "    df.to_csv(MANIFEST_PATH, index=False)\n",
        "    return df\n",
        "\n",
        "\n",
        "def summarize_local_outputs() -> pd.DataFrame:\n",
        "    rows = []\n",
        "    for spec in MODEL_SPECS:\n",
        "        for variant in (\"base\", \"sft\"):\n",
        "            for group in PROMPT_CACHE.keys():\n",
        "                local_dir = LOCAL_OUTPUT_ROOT / group / spec.name / variant\n",
        "                jsonls = sorted(local_dir.glob(\"*.jsonl\")) if local_dir.exists() else []\n",
        "                summaries = sorted(local_dir.glob(\"summary_*.json\")) if local_dir.exists() else []\n",
        "                rows.append(\n",
        "                    {\n",
        "                        \"model\": spec.name,\n",
        "                        \"variant\": variant,\n",
        "                        \"group\": group,\n",
        "                        \"jsonl_files\": len(jsonls),\n",
        "                        \"summary_files\": len(summaries),\n",
        "                        \"local_dir\": str(local_dir),\n",
        "                    }\n",
        "                )\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "\n",
        "# Call after pulling remote outputs to refresh the manifest\n",
        "# manifest_df = update_manifest()\n",
        "# manifest_df\n",
        "\n",
        "# summarize_local_outputs()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Run + Manifest Checklist\n",
        "\n",
        "1. Run `prefetch_models_on_remote()` once to cache checkpoints.\n",
        "2. Set `execute_full_sweep(dry_run=False)` when ready to launch all model/group jobs. Monitor `run_*.log` files in each remote output directory.\n",
        "3. After each job completes, call `pull_remote_tree(...)` or rerun `execute_full_sweep` for remaining entries (the helper skips completed directories).\n",
        "4. Use `update_manifest()` to append the latest records and `summarize_local_outputs()` to verify that every group/model/variant has JSONL + summary artifacts under `notebooks/h200_outputs_perplexity/`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\spenc\\AppData\\Local\\Temp\\ipykernel_19388\\3181165547.py:7: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  timestamp = datetime.utcnow().strftime(\"%Y%m%d_%H%M%S\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[run] qwen-0_5b (base) group=physics_A -> /workspace/vastai-ssh-jupyter-pytorch/notebooks/h200_outputs_perplexity/physics_A/qwen-0_5b/base\n"
          ]
        },
        {
          "ename": "CalledProcessError",
          "evalue": "Command '['ssh', '-p', '16693', '-i', 'C:\\\\Users\\\\spenc\\\\.ssh\\\\id_rsa', 'root@208.64.254.75', 'set -euo pipefail\\npython3 /workspace/vastai-ssh-jupyter-pytorch/scripts/physics_logprob_remote.py --model-name Qwen/Qwen2.5-0.5B --tokenizer-name Qwen/Qwen2.5-0.5B-Instruct --prompt-file /workspace/vastai-ssh-jupyter-pytorch/notebooks/h200_outputs_perplexity/prompts/physics_A.txt --output-dir /workspace/vastai-ssh-jupyter-pytorch/notebooks/h200_outputs_perplexity/physics_A/qwen-0_5b/base --attempt-char-limits 1000,800,600,400 --min-tokens 30 --dtype float16 --device cuda:0 --run-id 20251130_013913_qwen-0_5b_base_physics_A --group-name physics_A --variant-name base']' returned non-zero exit status 1.",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mCalledProcessError\u001b[39m                        Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mexecute_full_sweep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdry_run\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 72\u001b[39m, in \u001b[36mexecute_full_sweep\u001b[39m\u001b[34m(dry_run)\u001b[39m\n\u001b[32m     70\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m variant \u001b[38;5;129;01min\u001b[39;00m (\u001b[33m\"\u001b[39m\u001b[33mbase\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33msft\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     71\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m REMOTE_PROMPT_MAP.keys():\n\u001b[32m---> \u001b[39m\u001b[32m72\u001b[39m         \u001b[43mrun_remote_logprob_job\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdry_run\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdry_run\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 53\u001b[39m, in \u001b[36mrun_remote_logprob_job\u001b[39m\u001b[34m(spec, variant, group, dry_run)\u001b[39m\n\u001b[32m     51\u001b[39m     \u001b[38;5;28mprint\u001b[39m(remote_cmd)\n\u001b[32m     52\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m \u001b[43mrun_ssh\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremote_cmd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     54\u001b[39m local_dir = local_output_dir(spec, variant, group)\n\u001b[32m     55\u001b[39m pull_remote_tree(remote_dir, local_dir)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36mrun_ssh\u001b[39m\u001b[34m(command, check, capture_output)\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun_ssh\u001b[39m(command: \u001b[38;5;28mstr\u001b[39m, *, check: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mTrue\u001b[39;00m, capture_output: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m) -> subprocess.CompletedProcess:\n\u001b[32m     10\u001b[39m     full_cmd = _ssh_base() + [command]\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msubprocess\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfull_cmd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcheck\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcheck\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcapture_output\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcapture_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\USERS\\SPENC\\.CONDA\\ENVS\\VASTAI-SSH-JUPYTER-PYTORCH-ENV\\Lib\\subprocess.py:571\u001b[39m, in \u001b[36mrun\u001b[39m\u001b[34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[39m\n\u001b[32m    569\u001b[39m     retcode = process.poll()\n\u001b[32m    570\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m check \u001b[38;5;129;01mand\u001b[39;00m retcode:\n\u001b[32m--> \u001b[39m\u001b[32m571\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m CalledProcessError(retcode, process.args,\n\u001b[32m    572\u001b[39m                                  output=stdout, stderr=stderr)\n\u001b[32m    573\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m CompletedProcess(process.args, retcode, stdout, stderr)\n",
            "\u001b[31mCalledProcessError\u001b[39m: Command '['ssh', '-p', '16693', '-i', 'C:\\\\Users\\\\spenc\\\\.ssh\\\\id_rsa', 'root@208.64.254.75', 'set -euo pipefail\\npython3 /workspace/vastai-ssh-jupyter-pytorch/scripts/physics_logprob_remote.py --model-name Qwen/Qwen2.5-0.5B --tokenizer-name Qwen/Qwen2.5-0.5B-Instruct --prompt-file /workspace/vastai-ssh-jupyter-pytorch/notebooks/h200_outputs_perplexity/prompts/physics_A.txt --output-dir /workspace/vastai-ssh-jupyter-pytorch/notebooks/h200_outputs_perplexity/physics_A/qwen-0_5b/base --attempt-char-limits 1000,800,600,400 --min-tokens 30 --dtype float16 --device cuda:0 --run-id 20251130_013913_qwen-0_5b_base_physics_A --group-name physics_A --variant-name base']' returned non-zero exit status 1."
          ]
        }
      ],
      "source": [
        "execute_full_sweep(dry_run=False)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "VASTAI-SSH-JUPYTER-PYTORCH-ENV",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
