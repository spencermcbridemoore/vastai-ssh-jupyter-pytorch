# Development Configuration
# Small-scale testing configuration for cheap instances ($0.20-0.50/hr)
# Use this for rapid iteration and debugging

# Model Configuration
model:
  name: "example_model"
  params: 1000000  # 1M parameters for dev
  hidden_size: 512
  num_layers: 4
  dropout: 0.1

# Data Configuration
data:
  path: "/workspace/data"
  batch_size: 16  # Small batch size for dev
  num_workers: 2  # Fewer workers for dev
  sequence_length: 512
  max_samples: 10000  # Limit dataset size for dev

# Training Configuration
training:
  max_epochs: 5  # Few epochs for quick iteration
  learning_rate: 1e-4
  weight_decay: 0.01
  grad_clip: 1.0
  warmup_steps: 100
  checkpoint_interval: 500  # Checkpoint more frequently in dev
  log_interval: 50
  save_best: true

# Validation Configuration
validation:
  interval: 1  # Validate every epoch
  batch_size: 32

# Device Configuration
device:
  use_cuda: true
  cuda_device: 0

# Experiment Tracking
logging:
  use_wandb: false  # Disable in dev to save API calls
  wandb_project: "vastai-dev"
  use_tensorboard: true
  tensorboard_dir: "/workspace/logs/tensorboard"

# Checkpoint Configuration
checkpoint:
  local_dir: "/workspace/persistent/checkpoints"
  s3_bucket: null  # Set to your S3 bucket if using remote storage
  s3_prefix: "checkpoints/dev"
  keep_last_n: 3  # Keep fewer checkpoints in dev

# Cost Tracking
cost:
  hourly_rate: 0.20  # Estimated hourly rate for dev instance
  instance_type: "RTX 3090"  # Example

# Development Flags
dev_mode: true
dry_run: false  # Set to true to validate code without using GPU

# Residual Comparison Experiment
residual_compare:
  base_model: "gpt2"
  sft_model: "gpt2"
  tokenizer: null
  device: null
  dtype: "auto"
  top_k: 25
  prompts:
    - "Explain the moon landing in one paragraph."
    - "Write a short Python function that adds two numbers."
    - "Provide a balanced summary of the pros and cons of open sourcing frontier models."
    - "Describe how you would teach Hamiltonian mechanics to a first-year physics student."
  # Keep fuzzing/perturbations opt-in by default.
  prompt_variants:
    - identity
  prompt_variant_options:
    dual_channel:
      base_prefix: "BASE PROMPT:\\n"
      sft_prefix: "SFT PROMPT:\\n"
      order: "base_first"
  tracked_token_strings:
    - "ĠYes"
    - "ĠNo"
  interesting_token_map:
    eos: "<|endoftext|>"
  local_run:
    enabled: true
    min_vram_gb: 23.5
    require_gpu_name_substring: "4090"
    cuda_device_index: 0
    allowed_pairs:
      - base: "gpt2"
        sft: "gpt2"
        device: "cuda"
        dtype: "float16"
        model_kwargs:
          attn_implementation: "eager"
        notes: "Baseline sanity pair for local smoke tests"
      - base: "Qwen/Qwen2.5-0.5B"
        sft: "Qwen/Qwen2.5-0.5B-Instruct"
        device: "cuda"
        dtype: "float16"
        tokenizer: "Qwen/Qwen2.5-0.5B-Instruct"
        model_kwargs:
          attn_implementation: "eager"
        notes: "Fits comfortably within 16 GB; safe on 20 GB budget"
      - base: "Qwen/Qwen2.5-1.5B"
        sft: "Qwen/Qwen2.5-1.5B-Instruct"
        device: "cuda"
        dtype: "float16"
        tokenizer: "Qwen/Qwen2.5-1.5B-Instruct"
        model_kwargs:
          attn_implementation: "eager"
        notes: "≈16 GB usage; still under 20 GB cap"
      - base: "Qwen/Qwen2.5-Math-1.5B"
        sft: "Qwen/Qwen2.5-Math-1.5B-Instruct"
        device: "cuda"
        dtype: "bfloat16"
        tokenizer: "Qwen/Qwen2.5-Math-1.5B-Instruct"
        model_kwargs:
          attn_implementation: "eager"
        notes: "Math domain variant with similar footprint"
      - base: "nvidia/Mistral-NeMo-Minitron-8B-Base"
        sft: "nvidia/Mistral-NeMo-Minitron-8B-Instruct"
        device: "cuda"
        dtype: "bfloat16"
        tokenizer: "nvidia/Mistral-NeMo-Minitron-8B-Instruct"
        model_kwargs:
          attn_implementation: "eager"
        notes: "Keep prompts ≤50 tokens on a single 24 GB RTX 4090."
  embedding_variants:
    - name: "default"
      base:
        embedding_source: "base"
        unembedding_source: "base"
      sft:
        embedding_source: "sft"
        unembedding_source: "sft"
    - name: "sft_unembed"
      base:
        embedding_source: "base"
        unembedding_source: "sft"
      sft:
        embedding_source: "sft"
        unembedding_source: "sft"
    - name: "sft_embedding"
      base:
        embedding_source: "sft"
        unembedding_source: "base"
      sft:
        embedding_source: "sft"
        unembedding_source: "sft"
  model_sweep:
    - name: "gpt2_baseline"
      base_model: "gpt2"
      sft_model: "gpt2"
      dtype: "float16"
      estimated_vram_gb: 4
      notes: "Sanity check pair; confirm pipeline + logging work."
    - name: "qwen-0.5b"
      base_model: "Qwen/Qwen2.5-0.5B"
      sft_model: "Qwen/Qwen2.5-0.5B-Instruct"
      tokenizer: "Qwen/Qwen2.5-0.5B-Instruct"
      dtype: "float16"
      estimated_vram_gb: 10
      notes: "Lightweight; should finish in minutes."
    - name: "qwen-1.5b"
      base_model: "Qwen/Qwen2.5-1.5B"
      sft_model: "Qwen/Qwen2.5-1.5B-Instruct"
      tokenizer: "Qwen/Qwen2.5-1.5B-Instruct"
      dtype: "float16"
      estimated_vram_gb: 18
      notes: "Comfortably fits in 40 GB with full residual capture."
    - name: "qwen-3b"
      base_model: "Qwen/Qwen2.5-3B"
      sft_model: "Qwen/Qwen2.5-3B-Instruct"
      tokenizer: "Qwen/Qwen2.5-3B-Instruct"
      dtype: "bfloat16"
      estimated_vram_gb: 30
      notes: "Heaviest Qwen pair before needing >40 GB."
    - name: "qwen-7b"
      base_model: "Qwen/Qwen2.5-7B"
      sft_model: "Qwen/Qwen2.5-7B-Instruct"
      tokenizer: "Qwen/Qwen2.5-7B-Instruct"
      dtype: "bfloat16"
      estimated_vram_gb: 39
      notes: "Upper VRAM limit for a single 40 GB A100."
    - name: "qwen-math-1.5b"
      base_model: "Qwen/Qwen2.5-Math-1.5B"
      sft_model: "Qwen/Qwen2.5-Math-1.5B-Instruct"
      tokenizer: "Qwen/Qwen2.5-Math-1.5B-Instruct"
      dtype: "bfloat16"
      estimated_vram_gb: 20
      notes: "Math-focused variant; similar footprint to generic 1.5B."
    - name: "qwen-math-7b"
      base_model: "Qwen/Qwen2.5-Math-7B"
      sft_model: "Qwen/Qwen2.5-Math-7B-Instruct"
      tokenizer: "Qwen/Qwen2.5-Math-7B-Instruct"
      dtype: "bfloat16"
      estimated_vram_gb: 39
      notes: "Ensure math tokenizer is available in the cache."
    - name: "llama-3.1-8b"
      base_model: "meta-llama/Llama-3.1-8B"
      sft_model: "meta-llama/Llama-3.1-8B-Instruct"
      tokenizer: "meta-llama/Llama-3.1-8B-Instruct"
      dtype: "bfloat16"
      estimated_vram_gb: 34
      notes: "Requires HF auth + license acceptance before download."
  multi_pass:
    enabled: true
    runs:
      - name: "BB"
        model: "base"
        embedding_source: "base"
      - name: "BS"
        model: "sft"
        embedding_source: "base"
      - name: "SB"
        model: "base"
        embedding_source: "sft"
      - name: "SS"
        model: "sft"
        embedding_source: "sft"
    pairwise_differences:
      - ["BB", "BS"]
      - ["BB", "SB"]
      - ["BB", "SS"]
      - ["BS", "SB"]
      - ["BS", "SS"]
      - ["SB", "SS"]

