{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# H200 Long-Form Residual Runs\n",
        "\n",
        "Use this notebook to orchestrate long-sequence residual comparisons on the Vast H200 instance with the following guarantees:\n",
        "\n",
        "- Pre-download every base/SFT pair before exercising the GPU.\n",
        "- Force the four production prompts to remain long-form (≈200 tokens) with automatic 100-token and 50-token fallbacks if a model cannot handle the full text.\n",
        "- Run only the default base vs SFT embedding configuration (no swaps, no fuzzing/perturbations).\n",
        "- Stream progress with `tqdm` and immediately download each JSON/log back to this machine after every run.\n",
        "\n",
        "> Fill in the connection settings in the next cell before running anything else.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Repo root: C:\\Users\\spenc\\Cursor Repos\\vastai-ssh-jupyter-pytorch\n",
            "Local outputs: C:\\Users\\spenc\\Cursor Repos\\vastai-ssh-jupyter-pytorch\\notebooks\\h200_long_outputs\n",
            "Vast H200 instance 28316581: ssh root@208.64.254.178 -p 30975\n"
          ]
        }
      ],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import json\n",
        "import os\n",
        "import re\n",
        "import shlex\n",
        "import subprocess\n",
        "import textwrap\n",
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "from typing import Dict, Iterable, List, Optional\n",
        "from urllib.parse import urlparse\n",
        "\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "REPO_ROOT = Path(\"..\").resolve()\n",
        "if not (REPO_ROOT / \"experiments\").exists():\n",
        "    REPO_ROOT = Path.cwd().resolve()\n",
        "\n",
        "\n",
        "def detect_vast_instance(preferred_id: Optional[str]) -> Dict[str, Optional[str]]:\n",
        "    try:\n",
        "        result = subprocess.run(\n",
        "            [\"vastai\", \"show\", \"instances\"],\n",
        "            capture_output=True,\n",
        "            text=True,\n",
        "            check=True,\n",
        "        )\n",
        "    except (FileNotFoundError, subprocess.CalledProcessError):\n",
        "        return {\"instance_id\": preferred_id, \"ssh_host\": None, \"ssh_port\": None}\n",
        "\n",
        "    lines = [line.strip() for line in result.stdout.splitlines() if line.strip()]\n",
        "    rows = [line for line in lines if line and line[0].isdigit()]\n",
        "    target_row = None\n",
        "    for row in rows:\n",
        "        parts = row.split()\n",
        "        if not parts:\n",
        "            continue\n",
        "        row_id = parts[0]\n",
        "        if preferred_id and row_id == preferred_id:\n",
        "            target_row = parts\n",
        "            break\n",
        "        if target_row is None:\n",
        "            target_row = parts\n",
        "    if not target_row or len(target_row) < 11:\n",
        "        return {\"instance_id\": preferred_id or (target_row[0] if target_row else None), \"ssh_host\": None, \"ssh_port\": None}\n",
        "\n",
        "    return {\n",
        "        \"instance_id\": target_row[0],\n",
        "        \"ssh_host\": target_row[9],\n",
        "        \"ssh_port\": target_row[10],\n",
        "    }\n",
        "\n",
        "\n",
        "def fetch_ssh_settings(instance_id: Optional[str]) -> Dict[str, Optional[str]]:\n",
        "    if not instance_id:\n",
        "        return {}\n",
        "    try:\n",
        "        result = subprocess.run(\n",
        "            [\"vastai\", \"ssh-url\", instance_id],\n",
        "            capture_output=True,\n",
        "            text=True,\n",
        "            check=True,\n",
        "        )\n",
        "    except (FileNotFoundError, subprocess.CalledProcessError):\n",
        "        return {}\n",
        "    url = result.stdout.strip()\n",
        "    if not url:\n",
        "        return {}\n",
        "    parsed = urlparse(url)\n",
        "    if not parsed.hostname:\n",
        "        return {}\n",
        "    return {\n",
        "        \"ssh_user\": parsed.username or \"root\",\n",
        "        \"ssh_host\": parsed.hostname,\n",
        "        \"ssh_port\": str(parsed.port) if parsed.port else None,\n",
        "    }\n",
        "\n",
        "\n",
        "detected = detect_vast_instance(os.environ.get(\"H200_INSTANCE_ID\"))\n",
        "INSTANCE_ID = os.environ.get(\"H200_INSTANCE_ID\") or detected.get(\"instance_id\") or \"28315978\"\n",
        "ssh_url = fetch_ssh_settings(INSTANCE_ID)\n",
        "SSH_USER = os.environ.get(\"H200_SSH_USER\") or ssh_url.get(\"ssh_user\") or \"root\"\n",
        "SSH_HOST = os.environ.get(\"H200_SSH_HOST\") or ssh_url.get(\"ssh_host\") or detected.get(\"ssh_host\") or \"ssh7.vast.ai\"\n",
        "SSH_PORT = int(os.environ.get(\"H200_SSH_PORT\") or ssh_url.get(\"ssh_port\") or detected.get(\"ssh_port\") or \"35978\")\n",
        "_identity_env = os.environ.get(\"H200_SSH_IDENTITY\")\n",
        "default_identity = Path.home() / \".ssh\" / \"id_rsa\"\n",
        "if _identity_env:\n",
        "    SSH_IDENTITY: Optional[Path] = Path(_identity_env).expanduser()\n",
        "elif default_identity.exists():\n",
        "    SSH_IDENTITY = default_identity\n",
        "else:\n",
        "    SSH_IDENTITY = None\n",
        "REMOTE_REPO = os.environ.get(\"H200_REMOTE_REPO\", \"/workspace/vastai-ssh-jupyter-pytorch\")\n",
        "LOCAL_OUTPUT_ROOT = Path(os.environ.get(\"H200_LOCAL_OUTPUT\", \"h200_long_outputs\")).resolve()\n",
        "LOCAL_OUTPUT_ROOT.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "PROMPT_SOURCE = REPO_ROOT / \"experiments\" / \"prompts\" / \"prod_prompts.txt\"\n",
        "PROMPT_VARIANTS_DIR = REPO_ROOT / \"notebooks\" / \"generated_prompts\"\n",
        "PROMPT_VARIANTS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "REMOTE_PROMPT_DIR = f\"{REMOTE_REPO}/experiments/prompts\"\n",
        "\n",
        "FALLBACK_TOKENIZER = os.environ.get(\"H200_FALLBACK_TOKENIZER\", \"Qwen/Qwen2.5-0.5B-Instruct\")\n",
        "PROMPT_VARIANT_ORDER = (\"full\", \"tok100\", \"tok50\")\n",
        "\n",
        "print(f\"Repo root: {REPO_ROOT}\")\n",
        "print(f\"Local outputs: {LOCAL_OUTPUT_ROOT}\")\n",
        "print(f\"Vast H200 instance {INSTANCE_ID}: ssh {SSH_USER}@{SSH_HOST} -p {SSH_PORT}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[ModelSpec(name='qwen-0_5b', base='Qwen/Qwen2.5-0.5B', sft='Qwen/Qwen2.5-0.5B-Instruct', tokenizer='Qwen/Qwen2.5-0.5B-Instruct', dtype='float16', device='cuda:0', notes='Tiny sanity check; fast to run.'),\n",
              " ModelSpec(name='qwen-1_5b', base='Qwen/Qwen2.5-1.5B', sft='Qwen/Qwen2.5-1.5B-Instruct', tokenizer='Qwen/Qwen2.5-1.5B-Instruct', dtype='float16', device='cuda:0', notes='Fits comfortably on a single H200 GPU.'),\n",
              " ModelSpec(name='qwen-3b', base='Qwen/Qwen2.5-3B', sft='Qwen/Qwen2.5-3B-Instruct', tokenizer='Qwen/Qwen2.5-3B-Instruct', dtype='bfloat16', device='cuda:0', notes='Baseline mid-size model.'),\n",
              " ModelSpec(name='qwen-7b', base='Qwen/Qwen2.5-7B', sft='Qwen/Qwen2.5-7B-Instruct', tokenizer='Qwen/Qwen2.5-7B-Instruct', dtype='bfloat16', device='cuda:0', notes='Primary comparison target.'),\n",
              " ModelSpec(name='qwen-14b', base='Qwen/Qwen2.5-14B', sft='Qwen/Qwen2.5-14B-Instruct', tokenizer='Qwen/Qwen2.5-14B-Instruct', dtype='bfloat16', device='cuda:0', notes='Largest Qwen2.5 variant that fits on H200.'),\n",
              " ModelSpec(name='mistral-minitron-8b', base='nvidia/Mistral-NeMo-Minitron-8B-Base', sft='nvidia/Mistral-NeMo-Minitron-8B-Instruct', tokenizer='nvidia/Mistral-NeMo-Minitron-8B-Instruct', dtype='bfloat16', device='cuda:0', notes='NVIDIA NeMo Minitron pair.')]"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "@dataclass(frozen=True)\n",
        "class ModelSpec:\n",
        "    name: str\n",
        "    base: str\n",
        "    sft: str\n",
        "    tokenizer: str\n",
        "    dtype: str = \"bfloat16\"\n",
        "    device: str = \"cuda:0\"\n",
        "    notes: str = \"\"\n",
        "\n",
        "\n",
        "MODEL_SPECS: List[ModelSpec] = [\n",
        "    ModelSpec(\n",
        "        name=\"qwen-0_5b\",\n",
        "        base=\"Qwen/Qwen2.5-0.5B\",\n",
        "        sft=\"Qwen/Qwen2.5-0.5B-Instruct\",\n",
        "        tokenizer=\"Qwen/Qwen2.5-0.5B-Instruct\",\n",
        "        dtype=\"float16\",\n",
        "        notes=\"Tiny sanity check; fast to run.\",\n",
        "    ),\n",
        "    ModelSpec(\n",
        "        name=\"qwen-1_5b\",\n",
        "        base=\"Qwen/Qwen2.5-1.5B\",\n",
        "        sft=\"Qwen/Qwen2.5-1.5B-Instruct\",\n",
        "        tokenizer=\"Qwen/Qwen2.5-1.5B-Instruct\",\n",
        "        dtype=\"float16\",\n",
        "        notes=\"Fits comfortably on a single H200 GPU.\",\n",
        "    ),\n",
        "    ModelSpec(\n",
        "        name=\"qwen-3b\",\n",
        "        base=\"Qwen/Qwen2.5-3B\",\n",
        "        sft=\"Qwen/Qwen2.5-3B-Instruct\",\n",
        "        tokenizer=\"Qwen/Qwen2.5-3B-Instruct\",\n",
        "        notes=\"Baseline mid-size model.\",\n",
        "    ),\n",
        "    ModelSpec(\n",
        "        name=\"qwen-7b\",\n",
        "        base=\"Qwen/Qwen2.5-7B\",\n",
        "        sft=\"Qwen/Qwen2.5-7B-Instruct\",\n",
        "        tokenizer=\"Qwen/Qwen2.5-7B-Instruct\",\n",
        "        notes=\"Primary comparison target.\",\n",
        "    ),\n",
        "    ModelSpec(\n",
        "        name=\"qwen-14b\",\n",
        "        base=\"Qwen/Qwen2.5-14B\",\n",
        "        sft=\"Qwen/Qwen2.5-14B-Instruct\",\n",
        "        tokenizer=\"Qwen/Qwen2.5-14B-Instruct\",\n",
        "        notes=\"Largest Qwen2.5 variant that fits on H200.\",\n",
        "    ),\n",
        "    ModelSpec(\n",
        "        name=\"mistral-minitron-8b\",\n",
        "        base=\"nvidia/Mistral-NeMo-Minitron-8B-Base\",\n",
        "        sft=\"nvidia/Mistral-NeMo-Minitron-8B-Instruct\",\n",
        "        tokenizer=\"nvidia/Mistral-NeMo-Minitron-8B-Instruct\",\n",
        "        notes=\"NVIDIA NeMo Minitron pair.\",\n",
        "    ),\n",
        "]\n",
        "\n",
        "MODEL_SPECS\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "JSON_PATH_RE = re.compile(r\"Wrote comparison JSON to:\\s*(.+)\")\n",
        "\n",
        "\n",
        "def _ssh_base() -> List[str]:\n",
        "    cmd = [\"ssh\", \"-p\", str(SSH_PORT)]\n",
        "    identity = None\n",
        "    if SSH_IDENTITY:\n",
        "        identity = Path(SSH_IDENTITY).expanduser()\n",
        "    if identity and identity.exists():\n",
        "        cmd += [\"-i\", str(identity)]\n",
        "    cmd.append(f\"{SSH_USER}@{SSH_HOST}\")\n",
        "    return cmd\n",
        "\n",
        "\n",
        "def _scp_base() -> List[str]:\n",
        "    cmd = [\"scp\", \"-P\", str(SSH_PORT)]\n",
        "    identity = None\n",
        "    if SSH_IDENTITY:\n",
        "        identity = Path(SSH_IDENTITY).expanduser()\n",
        "    if identity and identity.exists():\n",
        "        cmd += [\"-i\", str(identity)]\n",
        "    return cmd\n",
        "\n",
        "\n",
        "def run_remote(command: str, *, desc: Optional[str] = None, check: bool = True) -> subprocess.CompletedProcess:\n",
        "    if desc:\n",
        "        tqdm.write(desc)\n",
        "    full_cmd = _ssh_base() + [command]\n",
        "    result = subprocess.run(full_cmd, capture_output=True, text=True)\n",
        "    if check and result.returncode != 0:\n",
        "        raise RuntimeError(f\"Remote command failed ({result.returncode})\\nSTDOUT:\\n{result.stdout}\\nSTDERR:\\n{result.stderr}\")\n",
        "    return result\n",
        "\n",
        "\n",
        "def scp_to_remote(local_path: Path, remote_path: str, *, desc: Optional[str] = None) -> None:\n",
        "    if desc:\n",
        "        tqdm.write(f\"Upload → {remote_path}: {local_path}\")\n",
        "    local_path = local_path.expanduser().resolve()\n",
        "    cmd = _scp_base() + [str(local_path), f\"{SSH_USER}@{SSH_HOST}:{remote_path}\"]\n",
        "    subprocess.run(cmd, check=True)\n",
        "\n",
        "\n",
        "def scp_from_remote(remote_path: str, local_path: Path, *, desc: Optional[str] = None) -> None:\n",
        "    if desc:\n",
        "        tqdm.write(f\"Download ← {remote_path} -> {local_path}\")\n",
        "    local_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    cmd = _scp_base() + [f\"{SSH_USER}@{SSH_HOST}:{remote_path}\", str(local_path)]\n",
        "    subprocess.run(cmd, check=True)\n",
        "\n",
        "\n",
        "def write_log(log_path: Path, stdout: str, stderr: str) -> None:\n",
        "    log_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    log_path.write_text(\n",
        "        stdout + (\"\\n--- stderr ---\\n\" + stderr if stderr else \"\"),\n",
        "        encoding=\"utf-8\",\n",
        "    )\n",
        "\n",
        "\n",
        "def format_env(env: Dict[str, str]) -> str:\n",
        "    return \" \".join(f\"{key}={shlex.quote(str(value))}\" for key, value in env.items())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wrote full prompt file → C:\\Users\\spenc\\Cursor Repos\\vastai-ssh-jupyter-pytorch\\notebooks\\generated_prompts\\prod_prompts_full.txt\n",
            "Wrote tok100 prompt file → C:\\Users\\spenc\\Cursor Repos\\vastai-ssh-jupyter-pytorch\\notebooks\\generated_prompts\\prod_prompts_tok100.txt\n",
            "Wrote tok50 prompt file → C:\\Users\\spenc\\Cursor Repos\\vastai-ssh-jupyter-pytorch\\notebooks\\generated_prompts\\prod_prompts_tok50.txt\n",
            "Upload → /workspace/vastai-ssh-jupyter-pytorch/experiments/prompts/prod_prompts_full.txt: C:\\Users\\spenc\\Cursor Repos\\vastai-ssh-jupyter-pytorch\\notebooks\\generated_prompts\\prod_prompts_full.txt\n"
          ]
        },
        {
          "ename": "CalledProcessError",
          "evalue": "Command '['scp', '-P', '36580', '-i', 'C:\\\\Users\\\\spenc\\\\.ssh\\\\id_rsa', 'C:\\\\Users\\\\spenc\\\\Cursor Repos\\\\vastai-ssh-jupyter-pytorch\\\\notebooks\\\\generated_prompts\\\\prod_prompts_full.txt', 'root@ssh8.vast.ai:/workspace/vastai-ssh-jupyter-pytorch/experiments/prompts/prod_prompts_full.txt']' returned non-zero exit status 255.",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mCalledProcessError\u001b[39m                        Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 44\u001b[39m\n\u001b[32m     42\u001b[39m     local_path = LOCAL_PROMPT_FILES[label]\n\u001b[32m     43\u001b[39m     remote_path = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mREMOTE_PROMPT_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/prod_prompts_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabel\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.txt\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m     \u001b[43mscp_to_remote\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocal_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mremote_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mSync \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlabel\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m prompts\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     45\u001b[39m     REMOTE_PROMPT_FILES[label] = remote_path\n\u001b[32m     47\u001b[39m REMOTE_PROMPT_FILES\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 40\u001b[39m, in \u001b[36mscp_to_remote\u001b[39m\u001b[34m(local_path, remote_path, desc)\u001b[39m\n\u001b[32m     38\u001b[39m local_path = local_path.expanduser().resolve()\n\u001b[32m     39\u001b[39m cmd = _scp_base() + [\u001b[38;5;28mstr\u001b[39m(local_path), \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mSSH_USER\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m@\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mSSH_HOST\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mremote_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m \u001b[43msubprocess\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\spenc\\.conda\\envs\\vastai-ssh-jupyter-pytorch-env\\Lib\\subprocess.py:571\u001b[39m, in \u001b[36mrun\u001b[39m\u001b[34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[39m\n\u001b[32m    569\u001b[39m     retcode = process.poll()\n\u001b[32m    570\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m check \u001b[38;5;129;01mand\u001b[39;00m retcode:\n\u001b[32m--> \u001b[39m\u001b[32m571\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m CalledProcessError(retcode, process.args,\n\u001b[32m    572\u001b[39m                                  output=stdout, stderr=stderr)\n\u001b[32m    573\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m CompletedProcess(process.args, retcode, stdout, stderr)\n",
            "\u001b[31mCalledProcessError\u001b[39m: Command '['scp', '-P', '36580', '-i', 'C:\\\\Users\\\\spenc\\\\.ssh\\\\id_rsa', 'C:\\\\Users\\\\spenc\\\\Cursor Repos\\\\vastai-ssh-jupyter-pytorch\\\\notebooks\\\\generated_prompts\\\\prod_prompts_full.txt', 'root@ssh8.vast.ai:/workspace/vastai-ssh-jupyter-pytorch/experiments/prompts/prod_prompts_full.txt']' returned non-zero exit status 255."
          ]
        }
      ],
      "source": [
        "with PROMPT_SOURCE.open(\"r\", encoding=\"utf-8\") as handle:\n",
        "    RAW_PROMPTS = [line.strip() for line in handle if line.strip()]\n",
        "\n",
        "if len(RAW_PROMPTS) != 4:\n",
        "    raise ValueError(f\"Expected 4 prompts, found {len(RAW_PROMPTS)} in {PROMPT_SOURCE}\")\n",
        "\n",
        "base_tokenizer = AutoTokenizer.from_pretrained(FALLBACK_TOKENIZER)\n",
        "\n",
        "\n",
        "def truncate_prompt(text: str, max_tokens: Optional[int]) -> str:\n",
        "    if max_tokens is None:\n",
        "        return text\n",
        "    encoded = base_tokenizer(text, add_special_tokens=False)\n",
        "    ids = encoded[\"input_ids\"]\n",
        "    if len(ids) <= max_tokens:\n",
        "        return text\n",
        "    truncated = base_tokenizer.decode(\n",
        "        ids[:max_tokens],\n",
        "        skip_special_tokens=False,\n",
        "        clean_up_tokenization_spaces=False,\n",
        "    )\n",
        "    return truncated\n",
        "\n",
        "\n",
        "PROMPT_VARIANTS: Dict[str, List[str]] = {\n",
        "    \"full\": RAW_PROMPTS,\n",
        "    \"tok100\": [truncate_prompt(prompt, 100) for prompt in RAW_PROMPTS],\n",
        "    \"tok50\": [truncate_prompt(prompt, 50) for prompt in RAW_PROMPTS],\n",
        "}\n",
        "\n",
        "LOCAL_PROMPT_FILES: Dict[str, Path] = {}\n",
        "for label, prompts in PROMPT_VARIANTS.items():\n",
        "    local_path = PROMPT_VARIANTS_DIR / f\"prod_prompts_{label}.txt\"\n",
        "    with local_path.open(\"w\", encoding=\"utf-8\") as handle:\n",
        "        for prompt in prompts:\n",
        "            handle.write(prompt + \"\\n\")\n",
        "    LOCAL_PROMPT_FILES[label] = local_path\n",
        "    tqdm.write(f\"Wrote {label} prompt file → {local_path}\")\n",
        "\n",
        "REMOTE_PROMPT_FILES: Dict[str, str] = {}\n",
        "for label in PROMPT_VARIANT_ORDER:\n",
        "    local_path = LOCAL_PROMPT_FILES[label]\n",
        "    remote_path = f\"{REMOTE_PROMPT_DIR}/prod_prompts_{label}.txt\"\n",
        "    scp_to_remote(local_path, remote_path, desc=f\"Sync {label} prompts\")\n",
        "    REMOTE_PROMPT_FILES[label] = remote_path\n",
        "\n",
        "REMOTE_PROMPT_FILES\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "PROD_CONFIG_REMOTE = f\"{REMOTE_REPO}/configs/prod_config.yaml\"\n",
        "PROD_CONFIG_BACKUP = f\"{REMOTE_REPO}/configs/prod_config.notebook.bak\"\n",
        "\n",
        "CONFIG_PATCH_SCRIPT = textwrap.dedent(\n",
        "    \"\"\"\n",
        "    import yaml\n",
        "    from pathlib import Path\n",
        "\n",
        "    config_path = Path(\"{config}\")\n",
        "    backup_path = Path(\"{backup}\")\n",
        "    if not backup_path.exists():\n",
        "        backup_path.write_text(config_path.read_text(encoding='utf-8'), encoding='utf-8')\n",
        "\n",
        "    data = yaml.safe_load(config_path.read_text(encoding='utf-8'))\n",
        "    rc = data.get('residual_compare', {})\n",
        "    rc['prompt_variants'] = ['identity']\n",
        "    rc['prompt_variant_options'] = {}\n",
        "    rc['embedding_variants'] = [\n",
        "        {{\n",
        "            'name': 'default',\n",
        "            'base': {{'embedding_source': 'base', 'unembedding_source': 'base'}},\n",
        "            'sft': {{'embedding_source': 'sft', 'unembedding_source': 'sft'}},\n",
        "        }}\n",
        "    ]\n",
        "    data['residual_compare'] = rc\n",
        "    config_path.write_text(yaml.safe_dump(data, sort_keys=False), encoding='utf-8')\n",
        "    \"\"\".format(config=PROD_CONFIG_REMOTE, backup=PROD_CONFIG_BACKUP)\n",
        ")\n",
        "\n",
        "CONFIG_RESTORE_SCRIPT = textwrap.dedent(\n",
        "    \"\"\"\n",
        "    from pathlib import Path\n",
        "\n",
        "    config_path = Path(\"{config}\")\n",
        "    backup_path = Path(\"{backup}\")\n",
        "    if backup_path.exists():\n",
        "        config_path.write_text(backup_path.read_text(encoding='utf-8'), encoding='utf-8')\n",
        "    \"\"\".format(config=PROD_CONFIG_REMOTE, backup=PROD_CONFIG_BACKUP)\n",
        ")\n",
        "\n",
        "\n",
        "def apply_config_override() -> None:\n",
        "    run_remote(\n",
        "        f\"cd {REMOTE_REPO} && python - <<'PY'\\n{CONFIG_PATCH_SCRIPT}\\nPY\",\n",
        "        desc=\"Apply no-fuzz config override\",\n",
        "    )\n",
        "\n",
        "\n",
        "def restore_config_override() -> None:\n",
        "    run_remote(\n",
        "        f\"cd {REMOTE_REPO} && python - <<'PY'\\n{CONFIG_RESTORE_SCRIPT}\\nPY\",\n",
        "        desc=\"Restore original prod_config.yaml\",\n",
        "        check=False,\n",
        "    )\n",
        "\n",
        "\n",
        "apply_config_override()\n",
        "tqdm.write(\"prod_config.yaml patched (prompt_variants=identity, embedding_variants=default).\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def prefetch_models(specs: Iterable[ModelSpec]) -> None:\n",
        "    repos = set()\n",
        "    for spec in specs:\n",
        "        repos.update([spec.base, spec.sft, spec.tokenizer])\n",
        "    for repo in tqdm(sorted(repos), desc=\"Prefetching HF weights\", unit=\"repo\"):\n",
        "        script = textwrap.dedent(\n",
        "            f\"\"\"\n",
        "            from huggingface_hub import snapshot_download\n",
        "            snapshot_download('{repo}', repo_type='model', resume_download=True)\n",
        "            \"\"\"\n",
        "        )\n",
        "        run_remote(\n",
        "            f\"cd {REMOTE_REPO} && python - <<'PY'\\n{script}\\nPY\",\n",
        "            desc=f\"Cache {repo}\",\n",
        "            check=False,\n",
        "        )\n",
        "\n",
        "\n",
        "prefetch_models(MODEL_SPECS)\n",
        "tqdm.write(\"All requested models/tokenizers have been cached on the H200.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "BASE_ENV = {\n",
        "    \"DEV_MODE\": \"False\",\n",
        "    \"PYTHONUNBUFFERED\": \"1\",\n",
        "}\n",
        "\n",
        "\n",
        "def build_env(spec: ModelSpec, prompt_label: str) -> Dict[str, str]:\n",
        "    env = dict(BASE_ENV)\n",
        "    env.update(\n",
        "        {\n",
        "            \"RESIDUAL_BASE_MODEL\": spec.base,\n",
        "            \"RESIDUAL_SFT_MODEL\": spec.sft,\n",
        "            \"RESIDUAL_TOKENIZER\": spec.tokenizer,\n",
        "            \"RESIDUAL_DTYPE\": spec.dtype,\n",
        "            \"RESIDUAL_DEVICE\": spec.device,\n",
        "            \"RESIDUAL_PROMPT_FILE\": REMOTE_PROMPT_FILES[prompt_label],\n",
        "        }\n",
        "    )\n",
        "    return env\n",
        "\n",
        "\n",
        "def execute_residual_attempt(spec: ModelSpec, prompt_label: str) -> Dict[str, object]:\n",
        "    env_line = format_env(build_env(spec, prompt_label))\n",
        "    command = f\"cd {REMOTE_REPO} && {env_line} python experiments/base_vs_sft_residual.py\"\n",
        "    result = run_remote(command, desc=f\"{spec.name} [{prompt_label}]\", check=False)\n",
        "    match = JSON_PATH_RE.search(result.stdout)\n",
        "    json_path = match.group(1).strip() if match else None\n",
        "    return {\n",
        "        \"completed_process\": result,\n",
        "        \"json_path\": json_path,\n",
        "        \"prompt_label\": prompt_label,\n",
        "    }\n",
        "\n",
        "\n",
        "def download_artifacts(spec: ModelSpec, prompt_label: str, json_remote: str, result: subprocess.CompletedProcess) -> Dict[str, str]:\n",
        "    model_dir = LOCAL_OUTPUT_ROOT / spec.name\n",
        "    model_dir.mkdir(parents=True, exist_ok=True)\n",
        "    local_json = model_dir / f\"{Path(json_remote).name}\" if json_remote else model_dir / f\"{spec.name}_missing.json\"\n",
        "    scp_from_remote(json_remote, local_json, desc=f\"JSON → {spec.name}\")\n",
        "    log_path = model_dir / f\"{local_json.stem}_{prompt_label}.log\"\n",
        "    write_log(log_path, result.stdout, result.stderr)\n",
        "    return {\"json_local\": str(local_json), \"log_local\": str(log_path)}\n",
        "\n",
        "\n",
        "def run_with_fallbacks(spec: ModelSpec, prompts: Iterable[str] = PROMPT_VARIANT_ORDER) -> Dict[str, str]:\n",
        "    attempts = list(prompts)\n",
        "    for label in attempts:\n",
        "        outcome = execute_residual_attempt(spec, label)\n",
        "        result = outcome[\"completed_process\"]\n",
        "        if result.returncode == 0 and outcome[\"json_path\"]:\n",
        "            artifacts = download_artifacts(spec, label, outcome[\"json_path\"], result)\n",
        "            tqdm.write(f\"✅ {spec.name} succeeded with {label} prompts\")\n",
        "            return {\n",
        "                \"model\": spec.name,\n",
        "                \"prompt_variant\": label,\n",
        "                \"remote_json\": outcome[\"json_path\"],\n",
        "                **artifacts,\n",
        "            }\n",
        "        tqdm.write(\n",
        "            f\"⚠️ {spec.name} failed with {label} prompts (rc={result.returncode}). Trying next fallback...\"\n",
        "        )\n",
        "    raise RuntimeError(f\"All prompt variants failed for {spec.name}. Check remote logs for details.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "run_records: List[Dict[str, str]] = []\n",
        "\n",
        "for spec in tqdm(MODEL_SPECS, desc=\"Residual sweep\", unit=\"model\"):\n",
        "    try:\n",
        "        outcome = run_with_fallbacks(spec)\n",
        "        outcome[\"status\"] = \"ok\"\n",
        "    except Exception as exc:  # pylint: disable=broad-except\n",
        "        tqdm.write(f\"❌ {spec.name} failed: {exc}\")\n",
        "        outcome = {\"model\": spec.name, \"status\": \"failed\", \"error\": str(exc)}\n",
        "    run_records.append(outcome)\n",
        "\n",
        "summary_df = pd.DataFrame(run_records)\n",
        "summary_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run this cell after all experiments finish to restore prod_config.yaml\n",
        "restore_config_override()\n",
        "tqdm.write(\"prod_config.yaml restored from backup.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.insert(0, str(REPO_ROOT / \"src\"))\n",
        "\n",
        "from src.analysis.residual_results import iter_results, summarize_file  # type: ignore\n",
        "\n",
        "json_paths = sorted(LOCAL_OUTPUT_ROOT.rglob(\"residual_compare_*.json\"))\n",
        "if not json_paths:\n",
        "    raise RuntimeError(f\"No residual_compare JSON files found in {LOCAL_OUTPUT_ROOT}\")\n",
        "\n",
        "summaries = []\n",
        "for path in json_paths:\n",
        "    summary = summarize_file(path)\n",
        "    summaries.append(\n",
        "        {\n",
        "            \"model\": path.parent.name,\n",
        "            \"json_path\": str(path),\n",
        "            \"num_results\": summary.num_results,\n",
        "            \"avg_tokens\": summary.avg_tokens,\n",
        "        }\n",
        "    )\n",
        "\n",
        "summary_table = pd.DataFrame(summaries)\n",
        "summary_table\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "vastai-ssh-jupyter-pytorch-env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
