{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Residual Analysis Notebook\n",
        "\n",
        "Use this notebook to explore the latest residual compare JSON outputs.\n",
        "\n",
        "Environment prep:\n",
        "1. Activate your Python environment for this repo.\n",
        "2. Run `pip install -r requirements.txt` inside the same interpreter (adds Jupyter + streaming deps).\n",
        "3. Launch Jupyter from the project root so all relative paths resolve.\n",
        "\n",
        "All file IO shown below assumes `encoding='utf-8'` for Windows compatibility.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('c:\\\\Users\\\\spenc\\\\.conda\\\\envs\\\\vastai-ssh-jupyter-pytorch-env\\\\python312.zip',\n",
              " WindowsPath('c:/Users/spenc/Cursor Repos/vastai-ssh-jupyter-pytorch/notebooks'))"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "   import sys, pathlib\n",
        "   next(p for p in sys.path if 'vastai-ssh-jupyter-pytorch' in p), pathlib.Path.cwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[WindowsPath('C:/Users/spenc/Cursor Repos/vastai-ssh-jupyter-pytorch/h200_outputs_multi')]"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import functools\n",
        "import itertools\n",
        "import json\n",
        "import os\n",
        "import sys\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "def _locate_project_root(start: Path) -> Path:\n",
        "    \"\"\"Walk up from the starting directory until we find the repo root.\"\"\"\n",
        "\n",
        "    for candidate in [start, *start.parents]:\n",
        "        if (candidate / \"src\").exists():\n",
        "            return candidate\n",
        "    raise RuntimeError(\"Could not locate project root relative to notebook.\")\n",
        "\n",
        "\n",
        "PROJECT_ROOT = _locate_project_root(Path.cwd().resolve())\n",
        "SRC_DIR = PROJECT_ROOT / \"src\"\n",
        "if str(PROJECT_ROOT) not in sys.path:\n",
        "    sys.path.append(str(PROJECT_ROOT))\n",
        "\n",
        "from src.analysis.residual_results import (\n",
        "    all_latest_jsons,\n",
        "    chunked_metric_frames,\n",
        "    iter_metric_rows,\n",
        "    latest_json_for,\n",
        "    list_models,\n",
        "    plot_correlation_heatmap,\n",
        "    plot_metric_distribution,\n",
        "    plot_metric_scatter,\n",
        "    plot_metric_trend,\n",
        ")\n",
        "\n",
        "# Focus analysis on multi-pass outputs for richer comparisons.\n",
        "RESULT_BASE_DIRS = [PROJECT_ROOT / \"h200_outputs_multi\"]\n",
        "FINDER_KWARGS = {\n",
        "    \"base_dirs\": RESULT_BASE_DIRS,\n",
        "    \"glob_pattern\": \"multi_pass_*.json\",\n",
        "}\n",
        "\n",
        "sns.set_theme(style=\"whitegrid\", context=\"notebook\")\n",
        "\n",
        "RESULT_BASE_DIRS\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Discover Latest Residual JSON per Model\n",
        "\n",
        "Use the helper utilities to list every model with available outputs and grab the newest JSON artifact for each one. This notebook is scoped to `h200_outputs_multi` to leverage the richer multi-pass structure.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No multi-pass JSON files found under:\n",
            " - C:\\Users\\spenc\\Cursor Repos\\vastai-ssh-jupyter-pytorch\\h200_outputs_multi\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>model</th>\n",
              "      <th>path</th>\n",
              "      <th>updated_at</th>\n",
              "      <th>size_mb</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [model, path, updated_at, size_mb]\n",
              "Index: []"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "latest_map = all_latest_jsons(**FINDER_KWARGS)\n",
        "\n",
        "if not latest_map:\n",
        "    print(\"No multi-pass JSON files found under:\")\n",
        "    for base in RESULT_BASE_DIRS:\n",
        "        print(f\" - {base}\")\n",
        "    latest_df = pd.DataFrame(columns=[\"model\", \"path\", \"updated_at\", \"size_mb\"])\n",
        "else:\n",
        "    latest_df = (\n",
        "        pd.DataFrame(\n",
        "            [\n",
        "                {\n",
        "                    \"model\": model,\n",
        "                    \"path\": str(path),\n",
        "                    \"updated_at\": datetime.fromtimestamp(path.stat().st_mtime),\n",
        "                    \"size_mb\": round(path.stat().st_size / (1024 ** 2), 2),\n",
        "                }\n",
        "                for model, path in latest_map.items()\n",
        "            ]\n",
        "        )\n",
        "        .sort_values(\"model\")\n",
        "        .reset_index(drop=True)\n",
        "    )\n",
        "    print(f\"Discovered {len(latest_map)} multi-pass models\")\n",
        "latest_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No multi-pass residual outputs found yet.\n"
          ]
        }
      ],
      "source": [
        "models = list_models(**FINDER_KWARGS)\n",
        "if not models:\n",
        "    print(\"No multi-pass residual outputs found yet.\")\n",
        "else:\n",
        "    example_model = models[0]\n",
        "    latest_path = latest_json_for(example_model, **FINDER_KWARGS)\n",
        "    print(f\"Latest JSON for {example_model} -> {latest_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Stream Records Lazily\n",
        "\n",
        "Operate on one record at a time with the streaming loader utilities. The snippets below preview a few rows without loading an entire file into memory.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No residual JSON files detected. Populate h200_outputs first.\n"
          ]
        }
      ],
      "source": [
        "if not latest_map:\n",
        "    print(\"No residual JSON files detected. Populate h200_outputs first.\")\n",
        "else:\n",
        "    sample_model, sample_path = next(iter(latest_map.items()))\n",
        "    print(f\"Previewing rows from {sample_model}: {sample_path}\")\n",
        "    row_iter = iter_metric_rows(\n",
        "        sample_path,\n",
        "        metadata_fields=(\"model\", \"task\", \"dataset\"),\n",
        "        aggregations_to_run=(\"residual_strength\",),\n",
        "    )\n",
        "    sample_rows = list(itertools.islice(row_iter, 3))\n",
        "    pd.DataFrame(sample_rows)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Chunked aggregation across models\n",
        "\n",
        "Use `chunked_metric_frames` to build manageable pandas DataFrames (e.g., 256 rows at a time) spanning all latest outputs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No JSON files to stream.\n"
          ]
        }
      ],
      "source": [
        "latest_paths = list(latest_map.values())\n",
        "metric_chunk = None\n",
        "\n",
        "if not latest_paths:\n",
        "    print(\"No JSON files to stream.\")\n",
        "else:\n",
        "    metric_chunk = next(\n",
        "        chunked_metric_frames(\n",
        "            *latest_paths,\n",
        "            chunk_size=256,\n",
        "            metadata_fields=(\"model\", \"task\", \"dataset\"),\n",
        "            aggregations_to_run=(\"residual_strength\",),\n",
        "        ),\n",
        "        None,\n",
        "    )\n",
        "    if metric_chunk is None or metric_chunk.empty:\n",
        "        print(\"Chunk generator produced no rows.\")\n",
        "    else:\n",
        "        metric_chunk.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Organize, Reduce, and Correlate Metrics\n",
        "\n",
        "Once you have a DataFrame chunk, standard pandas tooling (groupby, describe, corr) is available. The helpers below run a few common operations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Run the chunked loader cell above first.\n"
          ]
        }
      ],
      "source": [
        "if metric_chunk is None or metric_chunk.empty:\n",
        "    print(\"Run the chunked loader cell above first.\")\n",
        "else:\n",
        "    delta_col = \"agg.residual_strength.mean_norm_delta\"\n",
        "    grouped = (\n",
        "        metric_chunk.groupby(\"meta.model\", dropna=False)[delta_col]\n",
        "        .describe()\n",
        "        .rename_axis(\"meta.model\")\n",
        "    )\n",
        "    grouped\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Run the chunked loader cell above first.\n"
          ]
        }
      ],
      "source": [
        "if metric_chunk is None or metric_chunk.empty:\n",
        "    print(\"Run the chunked loader cell above first.\")\n",
        "else:\n",
        "    corr_cols = [\n",
        "        \"agg.residual_strength.mean_norm_base\",\n",
        "        \"agg.residual_strength.mean_norm_sft\",\n",
        "        \"agg.residual_strength.mean_norm_delta\",\n",
        "    ]\n",
        "    correlation = correlate_metric_columns(metric_chunk, columns=corr_cols)\n",
        "    correlation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Visualize Metrics\n",
        "\n",
        "The plotting helpers wrap matplotlib/seaborn primitives, so they work in any vanilla Jupyter kernel. Each function accepts either a DataFrame or an iterable of rows.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Run the chunked loader cell to generate metric_chunk first.\n"
          ]
        }
      ],
      "source": [
        "if metric_chunk is None or metric_chunk.empty:\n",
        "    print(\"Run the chunked loader cell to generate metric_chunk first.\")\n",
        "else:\n",
        "    ax = plot_metric_distribution(\n",
        "        metric_chunk,\n",
        "        column=\"agg.residual_strength.mean_norm_delta\",\n",
        "        bins=40,\n",
        "        kde=True,\n",
        "    )\n",
        "    ax.figure.tight_layout()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Run the chunked loader cell to generate metric_chunk first.\n"
          ]
        }
      ],
      "source": [
        "if metric_chunk is None or metric_chunk.empty:\n",
        "    print(\"Run the chunked loader cell to generate metric_chunk first.\")\n",
        "else:\n",
        "    ax = plot_metric_scatter(\n",
        "        metric_chunk,\n",
        "        x=\"agg.residual_strength.mean_norm_base\",\n",
        "        y=\"agg.residual_strength.mean_norm_sft\",\n",
        "        hue=\"meta.model\",\n",
        "        style=\"sft_embedding\",\n",
        "    )\n",
        "    ax.figure.tight_layout()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Run the chunked loader cell to generate metric_chunk first.\n"
          ]
        }
      ],
      "source": [
        "if metric_chunk is None or metric_chunk.empty:\n",
        "    print(\"Run the chunked loader cell to generate metric_chunk first.\")\n",
        "else:\n",
        "    trend_chunk = metric_chunk.reset_index(drop=True).assign(row_id=lambda df: df.index)\n",
        "    ax = plot_metric_trend(\n",
        "        trend_chunk,\n",
        "        x=\"row_id\",\n",
        "        y=\"agg.residual_strength.mean_norm_delta\",\n",
        "        hue=\"meta.model\",\n",
        "        estimator=None,\n",
        "    )\n",
        "    ax.set_xlabel(\"Row index (proxy for prompt order)\")\n",
        "    ax.figure.tight_layout()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Run the chunked loader cell to generate metric_chunk first.\n"
          ]
        }
      ],
      "source": [
        "if metric_chunk is None or metric_chunk.empty:\n",
        "    print(\"Run the chunked loader cell to generate metric_chunk first.\")\n",
        "else:\n",
        "    corr_cols = [\n",
        "        \"agg.residual_strength.mean_norm_base\",\n",
        "        \"agg.residual_strength.mean_norm_sft\",\n",
        "        \"agg.residual_strength.mean_norm_delta\",\n",
        "    ]\n",
        "    ax = plot_correlation_heatmap(metric_chunk, columns=corr_cols)\n",
        "    ax.figure.tight_layout()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "vastai-ssh-jupyter-pytorch-env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
