{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Residual Analysis Notebook\n",
        "\n",
        "Use this notebook to explore the latest residual compare JSON outputs.\n",
        "\n",
        "Environment prep:\n",
        "1. Activate your Python environment for this repo.\n",
        "2. Run `pip install -r requirements.txt` inside the same interpreter (adds Jupyter + streaming deps).\n",
        "3. Launch Jupyter from the project root so all relative paths resolve.\n",
        "\n",
        "All file IO shown below assumes `encoding='utf-8'` for Windows compatibility.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ]
        }
      ],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('C:\\\\Users\\\\spenc\\\\Cursor Repos\\\\vastai-ssh-jupyter-pytorch',\n",
              " WindowsPath('c:/Users/spenc/Cursor Repos/vastai-ssh-jupyter-pytorch/notebooks'))"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "   import sys, pathlib\n",
        "   next(p for p in sys.path if 'vastai-ssh-jupyter-pytorch' in p), pathlib.Path.cwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'ijson'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 30\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(PROJECT_ROOT) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m sys.path:\n\u001b[32m     28\u001b[39m     sys.path.append(\u001b[38;5;28mstr\u001b[39m(PROJECT_ROOT))\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01manalysis\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mresidual_results\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     31\u001b[39m     all_latest_jsons,\n\u001b[32m     32\u001b[39m     chunked_metric_frames,\n\u001b[32m     33\u001b[39m     iter_metric_rows,\n\u001b[32m     34\u001b[39m     latest_json_for,\n\u001b[32m     35\u001b[39m     list_models,\n\u001b[32m     36\u001b[39m     plot_correlation_heatmap,\n\u001b[32m     37\u001b[39m     plot_metric_distribution,\n\u001b[32m     38\u001b[39m     plot_metric_scatter,\n\u001b[32m     39\u001b[39m     plot_metric_trend,\n\u001b[32m     40\u001b[39m )\n\u001b[32m     42\u001b[39m \u001b[38;5;66;03m# Focus analysis on multi-pass outputs for richer comparisons.\u001b[39;00m\n\u001b[32m     43\u001b[39m RESULT_BASE_DIRS = [PROJECT_ROOT / \u001b[33m\"\u001b[39m\u001b[33mh200_outputs_multi\u001b[39m\u001b[33m\"\u001b[39m]\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\Cursor Repos\\vastai-ssh-jupyter-pytorch\\src\\analysis\\residual_results\\__init__.py:32\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlatest_runs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     12\u001b[39m     ModelRunFile,\n\u001b[32m     13\u001b[39m     all_latest_jsons,\n\u001b[32m   (...)\u001b[39m\u001b[32m     16\u001b[39m     list_models,\n\u001b[32m     17\u001b[39m )\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mloader\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     19\u001b[39m     MultiPassResidualRecord,\n\u001b[32m     20\u001b[39m     ResidualLayerStats,\n\u001b[32m   (...)\u001b[39m\u001b[32m     30\u001b[39m     load_results,\n\u001b[32m     31\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mloader_streaming\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     33\u001b[39m     chunked_metric_frames,\n\u001b[32m     34\u001b[39m     correlate_metric_columns,\n\u001b[32m     35\u001b[39m     iter_metric_rows,\n\u001b[32m     36\u001b[39m     stream_multi_pass_records,\n\u001b[32m     37\u001b[39m     stream_pairwise_results,\n\u001b[32m     38\u001b[39m     stream_raw_records,\n\u001b[32m     39\u001b[39m )\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mvisualize\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     41\u001b[39m     plot_correlation_heatmap,\n\u001b[32m     42\u001b[39m     plot_metric_distribution,\n\u001b[32m     43\u001b[39m     plot_metric_scatter,\n\u001b[32m     44\u001b[39m     plot_metric_trend,\n\u001b[32m     45\u001b[39m )\n\u001b[32m     47\u001b[39m __all__ = [\n\u001b[32m     48\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mModelRunFile\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     49\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mMultiPassResidualRecord\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     74\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mplot_metric_trend\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     75\u001b[39m ]\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\Cursor Repos\\vastai-ssh-jupyter-pytorch\\src\\analysis\\residual_results\\loader_streaming.py:14\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Iterable, Iterator, Mapping, MutableMapping, Sequence\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mijson\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m aggregations\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'ijson'"
          ]
        }
      ],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import functools\n",
        "import itertools\n",
        "import json\n",
        "import os\n",
        "import sys\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "def _locate_project_root(start: Path) -> Path:\n",
        "    \"\"\"Walk up from the starting directory until we find the repo root.\"\"\"\n",
        "\n",
        "    for candidate in [start, *start.parents]:\n",
        "        if (candidate / \"src\").exists():\n",
        "            return candidate\n",
        "    raise RuntimeError(\"Could not locate project root relative to notebook.\")\n",
        "\n",
        "\n",
        "PROJECT_ROOT = _locate_project_root(Path.cwd().resolve())\n",
        "SRC_DIR = PROJECT_ROOT / \"src\"\n",
        "if str(PROJECT_ROOT) not in sys.path:\n",
        "    sys.path.append(str(PROJECT_ROOT))\n",
        "\n",
        "from src.analysis.residual_results import (\n",
        "    all_latest_jsons,\n",
        "    chunked_metric_frames,\n",
        "    iter_metric_rows,\n",
        "    latest_json_for,\n",
        "    list_models,\n",
        "    plot_correlation_heatmap,\n",
        "    plot_metric_distribution,\n",
        "    plot_metric_scatter,\n",
        "    plot_metric_trend,\n",
        ")\n",
        "\n",
        "# Focus analysis on multi-pass outputs for richer comparisons.\n",
        "RESULT_BASE_DIRS = [PROJECT_ROOT / \"h200_outputs_multi\"]\n",
        "FINDER_KWARGS = {\"base_dirs\": RESULT_BASE_DIRS}\n",
        "\n",
        "sns.set_theme(style=\"whitegrid\", context=\"notebook\")\n",
        "\n",
        "RESULT_BASE_DIRS\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Discover Latest Residual JSON per Model\n",
        "\n",
        "Use the helper utilities to list every model with available outputs and grab the newest JSON artifact for each one. This notebook is scoped to `h200_outputs_multi` to leverage the richer multi-pass structure.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "latest_map = all_latest_jsons(**FINDER_KWARGS)\n",
        "\n",
        "if not latest_map:\n",
        "    print(\"No multi-pass JSON files found under:\")\n",
        "    for base in RESULT_BASE_DIRS:\n",
        "        print(f\" - {base}\")\n",
        "    latest_df = pd.DataFrame(columns=[\"model\", \"path\", \"updated_at\", \"size_mb\"])\n",
        "else:\n",
        "    latest_df = (\n",
        "        pd.DataFrame(\n",
        "            [\n",
        "                {\n",
        "                    \"model\": model,\n",
        "                    \"path\": str(path),\n",
        "                    \"updated_at\": datetime.fromtimestamp(path.stat().st_mtime),\n",
        "                    \"size_mb\": round(path.stat().st_size / (1024 ** 2), 2),\n",
        "                }\n",
        "                for model, path in latest_map.items()\n",
        "            ]\n",
        "        )\n",
        "        .sort_values(\"model\")\n",
        "        .reset_index(drop=True)\n",
        "    )\n",
        "    print(f\"Discovered {len(latest_map)} multi-pass models\")\n",
        "latest_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "models = list_models(**FINDER_KWARGS)\n",
        "if not models:\n",
        "    print(\"No multi-pass residual outputs found yet.\")\n",
        "else:\n",
        "    example_model = models[0]\n",
        "    latest_path = latest_json_for(example_model, **FINDER_KWARGS)\n",
        "    print(f\"Latest JSON for {example_model} -> {latest_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Stream Records Lazily\n",
        "\n",
        "Operate on one record at a time with the streaming loader utilities. The snippets below preview a few rows without loading an entire file into memory.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if not latest_map:\n",
        "    print(\"No residual JSON files detected. Populate h200_outputs first.\")\n",
        "else:\n",
        "    sample_model, sample_path = next(iter(latest_map.items()))\n",
        "    print(f\"Previewing rows from {sample_model}: {sample_path}\")\n",
        "    row_iter = iter_metric_rows(\n",
        "        sample_path,\n",
        "        metadata_fields=(\"model\", \"task\", \"dataset\"),\n",
        "        aggregations_to_run=(\"residual_strength\",),\n",
        "    )\n",
        "    sample_rows = list(itertools.islice(row_iter, 3))\n",
        "    pd.DataFrame(sample_rows)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Chunked aggregation across models\n",
        "\n",
        "Use `chunked_metric_frames` to build manageable pandas DataFrames (e.g., 256 rows at a time) spanning all latest outputs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "latest_paths = list(latest_map.values())\n",
        "metric_chunk = None\n",
        "\n",
        "if not latest_paths:\n",
        "    print(\"No JSON files to stream.\")\n",
        "else:\n",
        "    metric_chunk = next(\n",
        "        chunked_metric_frames(\n",
        "            *latest_paths,\n",
        "            chunk_size=256,\n",
        "            metadata_fields=(\"model\", \"task\", \"dataset\"),\n",
        "            aggregations_to_run=(\"residual_strength\",),\n",
        "        ),\n",
        "        None,\n",
        "    )\n",
        "    if metric_chunk is None or metric_chunk.empty:\n",
        "        print(\"Chunk generator produced no rows.\")\n",
        "    else:\n",
        "        metric_chunk.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Organize, Reduce, and Correlate Metrics\n",
        "\n",
        "Once you have a DataFrame chunk, standard pandas tooling (groupby, describe, corr) is available. The helpers below run a few common operations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if metric_chunk is None or metric_chunk.empty:\n",
        "    print(\"Run the chunked loader cell above first.\")\n",
        "else:\n",
        "    delta_col = \"agg.residual_strength.mean_norm_delta\"\n",
        "    grouped = (\n",
        "        metric_chunk.groupby(\"meta.model\", dropna=False)[delta_col]\n",
        "        .describe()\n",
        "        .rename_axis(\"meta.model\")\n",
        "    )\n",
        "    grouped\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if metric_chunk is None or metric_chunk.empty:\n",
        "    print(\"Run the chunked loader cell above first.\")\n",
        "else:\n",
        "    corr_cols = [\n",
        "        \"agg.residual_strength.mean_norm_base\",\n",
        "        \"agg.residual_strength.mean_norm_sft\",\n",
        "        \"agg.residual_strength.mean_norm_delta\",\n",
        "    ]\n",
        "    correlation = correlate_metric_columns(metric_chunk, columns=corr_cols)\n",
        "    correlation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Visualize Metrics\n",
        "\n",
        "The plotting helpers wrap matplotlib/seaborn primitives, so they work in any vanilla Jupyter kernel. Each function accepts either a DataFrame or an iterable of rows.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if metric_chunk is None or metric_chunk.empty:\n",
        "    print(\"Run the chunked loader cell to generate metric_chunk first.\")\n",
        "else:\n",
        "    ax = plot_metric_distribution(\n",
        "        metric_chunk,\n",
        "        column=\"agg.residual_strength.mean_norm_delta\",\n",
        "        bins=40,\n",
        "        kde=True,\n",
        "    )\n",
        "    ax.figure.tight_layout()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if metric_chunk is None or metric_chunk.empty:\n",
        "    print(\"Run the chunked loader cell to generate metric_chunk first.\")\n",
        "else:\n",
        "    ax = plot_metric_scatter(\n",
        "        metric_chunk,\n",
        "        x=\"agg.residual_strength.mean_norm_base\",\n",
        "        y=\"agg.residual_strength.mean_norm_sft\",\n",
        "        hue=\"meta.model\",\n",
        "        style=\"sft_embedding\",\n",
        "    )\n",
        "    ax.figure.tight_layout()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if metric_chunk is None or metric_chunk.empty:\n",
        "    print(\"Run the chunked loader cell to generate metric_chunk first.\")\n",
        "else:\n",
        "    trend_chunk = metric_chunk.reset_index(drop=True).assign(row_id=lambda df: df.index)\n",
        "    ax = plot_metric_trend(\n",
        "        trend_chunk,\n",
        "        x=\"row_id\",\n",
        "        y=\"agg.residual_strength.mean_norm_delta\",\n",
        "        hue=\"meta.model\",\n",
        "        estimator=None,\n",
        "    )\n",
        "    ax.set_xlabel(\"Row index (proxy for prompt order)\")\n",
        "    ax.figure.tight_layout()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if metric_chunk is None or metric_chunk.empty:\n",
        "    print(\"Run the chunked loader cell to generate metric_chunk first.\")\n",
        "else:\n",
        "    corr_cols = [\n",
        "        \"agg.residual_strength.mean_norm_base\",\n",
        "        \"agg.residual_strength.mean_norm_sft\",\n",
        "        \"agg.residual_strength.mean_norm_delta\",\n",
        "    ]\n",
        "    ax = plot_correlation_heatmap(metric_chunk, columns=corr_cols)\n",
        "    ax.figure.tight_layout()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "vastai",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
