{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Residual compare structure demo\n",
        "\n",
        "This notebook shows light-weight inspection patterns for the large\n",
        "`notebooks/h200_long_outputs/physics_A/qwen-0_5b/residual_compare_20251129_053550_Qwen_Qwen2_5_0_5B_Instruct.json`\n",
        "file. The goal is to understand its structure using Python helpers while\n",
        "avoiding massive prints or full data dumps.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'src'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpprint\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pprint\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Any, Mapping, Sequence\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01manalysis\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mresidual_results\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m loader\n\u001b[32m      9\u001b[39m DATA_PATH = Path(\n\u001b[32m     10\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mnotebooks/h200_long_outputs/physics_A/qwen-0_5b/\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     11\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mresidual_compare_20251129_053550_Qwen_Qwen2_5_0_5B_Instruct.json\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     12\u001b[39m ).resolve()\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m DATA_PATH.exists():\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'src'"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import sys\n",
        "import json\n",
        "import itertools as it\n",
        "from pprint import pprint\n",
        "from typing import Any, Mapping, Sequence\n",
        "\n",
        "\n",
        "def resolve_project_root() -> Path:\n",
        "    \"\"\"Locate the repository root (directory containing the src package).\"\"\"\n",
        "    candidates = [Path.cwd().resolve()]\n",
        "    if \"__file__\" in globals():\n",
        "        candidates.append(Path(__file__).resolve().parent)\n",
        "    candidates.extend(candidate.parent for candidate in list(candidates))\n",
        "    for candidate in candidates:\n",
        "        if (candidate / \"src\").exists():\n",
        "            return candidate\n",
        "    raise RuntimeError(\"Unable to locate project root containing 'src'.\")\n",
        "\n",
        "\n",
        "PROJECT_ROOT = resolve_project_root()\n",
        "if str(PROJECT_ROOT) not in sys.path:\n",
        "    sys.path.insert(0, str(PROJECT_ROOT))\n",
        "\n",
        "from src.analysis.residual_results import loader\n",
        "\n",
        "DATA_PATH = Path(\n",
        "    \"notebooks/h200_long_outputs/physics_A/qwen-0_5b/\"\n",
        "    \"residual_compare_20251129_053550_Qwen_Qwen2_5_0_5B_Instruct.json\"\n",
        ").resolve()\n",
        "\n",
        "if not DATA_PATH.exists():\n",
        "    raise FileNotFoundError(DATA_PATH)\n",
        "\n",
        "size_mb = DATA_PATH.stat().st_size / (1024 ** 2)\n",
        "print(f\"Data path: {DATA_PATH}\")\n",
        "print(f\"File size: {size_mb:.2f} MiB\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Raw JSON peek\n",
        "\n",
        "The next helpers read a tiny slice of the JSON file and summarize what is\n",
        "available (keys, token counts, metadata keys, etc.). They explicitly cap\n",
        "how many tokens or layers are displayed so the notebook stays readable.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'Path' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_small_sample\u001b[39m(path: \u001b[43mPath\u001b[49m, max_items: \u001b[38;5;28mint\u001b[39m = \u001b[32m2\u001b[39m):\n\u001b[32m      2\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return up to `max_items` entries plus the total record count.\"\"\"\u001b[39;00m\n\u001b[32m      3\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m path.open(\u001b[33m\"\u001b[39m\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m, encoding=\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fh:\n",
            "\u001b[31mNameError\u001b[39m: name 'Path' is not defined"
          ]
        }
      ],
      "source": [
        "def load_small_sample(path: Path, max_items: int = 2):\n",
        "    \"\"\"Return up to `max_items` entries plus the total record count.\"\"\"\n",
        "    with path.open(\"r\", encoding=\"utf-8\") as fh:\n",
        "        payload = json.load(fh)\n",
        "    if not isinstance(payload, list):\n",
        "        raise TypeError(\"Expected a list of records at the top level.\")\n",
        "    return payload[:max_items], len(payload)\n",
        "\n",
        "\n",
        "def describe_record(record: Mapping[str, Any], token_limit: int = 8) -> Mapping[str, Any]:\n",
        "    \"\"\"Extract a compact summary from a single raw record.\"\"\"\n",
        "    tokens = record.get(\"tokens\", [])\n",
        "    metadata = record.get(\"metadata\", {})\n",
        "    layers = record.get(\"layers\", [])\n",
        "    base_swap = record.get(\"base_swap\", {})\n",
        "    sft_swap = record.get(\"sft_swap\", {})\n",
        "    return {\n",
        "        \"available_keys\": sorted(record.keys()),\n",
        "        \"token_count\": len(tokens),\n",
        "        \"token_preview\": tokens[:token_limit],\n",
        "        \"metadata_keys\": sorted(metadata.keys()),\n",
        "        \"layer_count\": len(layers),\n",
        "        \"layer_indices\": [layer.get(\"layer_index\") for layer in layers[:3]],\n",
        "        \"base_swap_sources\": {\n",
        "            \"embedding\": base_swap.get(\"embedding_source\"),\n",
        "            \"unembedding\": base_swap.get(\"unembedding_source\"),\n",
        "        },\n",
        "        \"sft_swap_sources\": {\n",
        "            \"embedding\": sft_swap.get(\"embedding_source\"),\n",
        "            \"unembedding\": sft_swap.get(\"unembedding_source\"),\n",
        "        },\n",
        "    }\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "raw_sample, total_records = load_small_sample(DATA_PATH, max_items=1)\n",
        "print(f\"Top-level entries: {total_records}\")\n",
        "print(\"\\nFirst record summary:\")\n",
        "pprint(describe_record(raw_sample[0]), width=100, compact=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "record = raw_sample[0]\n",
        "metadata_preview = list(it.islice(record.get(\"metadata\", {}).items(), 5))\n",
        "print(\"Metadata sample (first 5 entries):\")\n",
        "pprint(metadata_preview, width=100, compact=True)\n",
        "\n",
        "first_layer = record.get(\"layers\", [])[0]\n",
        "layer_positions = first_layer.get(\"positions\", [])\n",
        "print(\"\\nLayer 0 summary:\")\n",
        "pprint(\n",
        "    {\n",
        "        \"layer_index\": first_layer.get(\"layer_index\"),\n",
        "        \"num_positions\": len(layer_positions),\n",
        "        \"token_names\": [pos.get(\"token\") for pos in layer_positions[:5]],\n",
        "    },\n",
        "    width=100,\n",
        "    compact=True,\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Structured dataclass view\n",
        "\n",
        "The `src.analysis.residual_results.loader` module exposes iterators and\n",
        "summaries that convert each JSON entry into typed dataclasses. This keeps\n",
        "the parsing lazy and provides convenience accessors for prompts, tokens,\n",
        "and layers.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "file_summary = loader.summarize_file(DATA_PATH)\n",
        "print(\"ResidualResult summary:\")\n",
        "pprint(\n",
        "    {\n",
        "        \"path\": str(file_summary.path),\n",
        "        \"num_results\": file_summary.num_results,\n",
        "        \"total_tokens\": file_summary.total_tokens,\n",
        "        \"avg_tokens\": round(file_summary.avg_tokens, 2),\n",
        "    },\n",
        "    width=100,\n",
        "    compact=True,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "result_iter = loader.iter_results(DATA_PATH)\n",
        "first_result = next(result_iter)\n",
        "\n",
        "structured_summary = {\n",
        "    \"prompt_chars\": len(first_result.prompt),\n",
        "    \"prompt_preview\": first_result.prompt[:120].replace(\"\\n\", \" \") + (\n",
        "        \"â€¦\" if len(first_result.prompt) > 120 else \"\"\n",
        "    ),\n",
        "    \"token_count\": first_result.num_tokens(),\n",
        "    \"token_preview\": list(first_result.tokens[:8]),\n",
        "    \"num_layers\": first_result.num_layers(),\n",
        "}\n",
        "\n",
        "print(\"First ResidualResult summary:\")\n",
        "pprint(structured_summary, width=100, compact=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Reusable query helpers\n",
        "\n",
        "Wrapping repeated inspection patterns into tiny functions keeps the\n",
        "notebook tidy. The helpers below take dataclass instances and emit\n",
        "summaries limited by caller-provided caps.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def summarize_tokens(result: loader.ResidualResult, max_tokens: int = 10) -> Mapping[str, Any]:\n",
        "    return {\n",
        "        \"total\": result.num_tokens(),\n",
        "        \"preview\": list(result.tokens[:max_tokens]),\n",
        "    }\n",
        "\n",
        "\n",
        "def list_layers(result: loader.ResidualResult, max_layers: int = 3, max_positions: int = 5):\n",
        "    for layer in it.islice(result.layers, max_layers):\n",
        "        yield {\n",
        "            \"layer_index\": layer.layer_index,\n",
        "            \"positions\": [pos.token for pos in layer.positions[:max_positions]],\n",
        "        }\n",
        "\n",
        "\n",
        "def layer_norm_deltas(result: loader.ResidualResult, max_layers: int = 3, max_positions: int = 5):\n",
        "    for layer in it.islice(result.layers, max_layers):\n",
        "        stats = []\n",
        "        for pos in layer.positions[:max_positions]:\n",
        "            stats.append(\n",
        "                {\n",
        "                    \"token\": pos.token,\n",
        "                    \"norm_diff\": pos.norm_diff,\n",
        "                    \"kl_div\": pos.kl_div,\n",
        "                }\n",
        "            )\n",
        "        yield layer.layer_index, stats\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Token preview via summarize_tokens():\")\n",
        "pprint(summarize_tokens(first_result, max_tokens=6), width=100, compact=True)\n",
        "\n",
        "print(\"\\nLayer token overview (first 2 layers):\")\n",
        "for info in list_layers(first_result, max_layers=2, max_positions=6):\n",
        "    pprint(info, width=100, compact=True)\n",
        "\n",
        "print(\"\\nLayer norm / KL samples:\")\n",
        "for layer_idx, stats in layer_norm_deltas(first_result, max_layers=1, max_positions=5):\n",
        "    print(f\"Layer {layer_idx}\")\n",
        "    pprint(stats, width=100, compact=True)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "VASTAI-SSH-JUPYTER-PYTORCH-ENV",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
