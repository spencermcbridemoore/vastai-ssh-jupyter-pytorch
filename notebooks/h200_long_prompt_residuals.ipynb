{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# H200 Long-Form Residual Runs\n",
        "\n",
        "Use this notebook to orchestrate long-sequence residual comparisons on the Vast H200 instance with the following guarantees:\n",
        "\n",
        "- Pre-download every base/SFT pair before exercising the GPU.\n",
        "- Force the four production prompts to remain long-form (≈200 tokens) with automatic 100-token and 50-token fallbacks if a model cannot handle the full text.\n",
        "- Run only the default base vs SFT embedding configuration (no swaps, no fuzzing/perturbations).\n",
        "- Stream progress with `tqdm` and immediately download each JSON/log back to this machine after every run.\n",
        "\n",
        "> Fill in the connection settings in the next cell before running anything else.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Repo root: C:\\Users\\spenc\\Cursor Repos\\vastai-ssh-jupyter-pytorch\n",
            "Local outputs: C:\\Users\\spenc\\Cursor Repos\\vastai-ssh-jupyter-pytorch\\notebooks\\h200_long_outputs\n",
            "Vast H200 instance 28316581: ssh root@208.64.254.178 -p 30975\n"
          ]
        }
      ],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import json\n",
        "import os\n",
        "import re\n",
        "import shlex\n",
        "import subprocess\n",
        "import tarfile\n",
        "import tempfile\n",
        "import textwrap\n",
        "from dataclasses import dataclass\n",
        "from pathlib import Path, PurePosixPath\n",
        "from typing import Dict, Iterable, List, Optional\n",
        "from urllib.parse import urlparse\n",
        "\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "REPO_ROOT = Path(\"..\").resolve()\n",
        "if not (REPO_ROOT / \"experiments\").exists():\n",
        "    REPO_ROOT = Path.cwd().resolve()\n",
        "\n",
        "\n",
        "def detect_vast_instance(preferred_id: Optional[str]) -> Dict[str, Optional[str]]:\n",
        "    try:\n",
        "        result = subprocess.run(\n",
        "            [\"vastai\", \"show\", \"instances\"],\n",
        "            capture_output=True,\n",
        "            text=True,\n",
        "            check=True,\n",
        "        )\n",
        "    except (FileNotFoundError, subprocess.CalledProcessError):\n",
        "        return {\"instance_id\": preferred_id, \"ssh_host\": None, \"ssh_port\": None}\n",
        "\n",
        "    lines = [line.strip() for line in result.stdout.splitlines() if line.strip()]\n",
        "    rows = [line for line in lines if line and line[0].isdigit()]\n",
        "    target_row = None\n",
        "    for row in rows:\n",
        "        parts = row.split()\n",
        "        if not parts:\n",
        "            continue\n",
        "        row_id = parts[0]\n",
        "        if preferred_id and row_id == preferred_id:\n",
        "            target_row = parts\n",
        "            break\n",
        "        if target_row is None:\n",
        "            target_row = parts\n",
        "    if not target_row or len(target_row) < 11:\n",
        "        return {\"instance_id\": preferred_id or (target_row[0] if target_row else None), \"ssh_host\": None, \"ssh_port\": None}\n",
        "\n",
        "    return {\n",
        "        \"instance_id\": target_row[0],\n",
        "        \"ssh_host\": target_row[9],\n",
        "        \"ssh_port\": target_row[10],\n",
        "    }\n",
        "\n",
        "\n",
        "def fetch_ssh_settings(instance_id: Optional[str]) -> Dict[str, Optional[str]]:\n",
        "    if not instance_id:\n",
        "        return {}\n",
        "    try:\n",
        "        result = subprocess.run(\n",
        "            [\"vastai\", \"ssh-url\", instance_id],\n",
        "            capture_output=True,\n",
        "            text=True,\n",
        "            check=True,\n",
        "        )\n",
        "    except (FileNotFoundError, subprocess.CalledProcessError):\n",
        "        return {}\n",
        "    url = result.stdout.strip()\n",
        "    if not url:\n",
        "        return {}\n",
        "    parsed = urlparse(url)\n",
        "    if not parsed.hostname:\n",
        "        return {}\n",
        "    return {\n",
        "        \"ssh_user\": parsed.username or \"root\",\n",
        "        \"ssh_host\": parsed.hostname,\n",
        "        \"ssh_port\": str(parsed.port) if parsed.port else None,\n",
        "    }\n",
        "\n",
        "\n",
        "detected = detect_vast_instance(os.environ.get(\"H200_INSTANCE_ID\"))\n",
        "INSTANCE_ID = os.environ.get(\"H200_INSTANCE_ID\") or detected.get(\"instance_id\") or \"28315978\"\n",
        "ssh_url = fetch_ssh_settings(INSTANCE_ID)\n",
        "SSH_USER = os.environ.get(\"H200_SSH_USER\") or ssh_url.get(\"ssh_user\") or \"root\"\n",
        "SSH_HOST = os.environ.get(\"H200_SSH_HOST\") or ssh_url.get(\"ssh_host\") or detected.get(\"ssh_host\") or \"ssh7.vast.ai\"\n",
        "SSH_PORT = int(os.environ.get(\"H200_SSH_PORT\") or ssh_url.get(\"ssh_port\") or detected.get(\"ssh_port\") or \"35978\")\n",
        "_identity_env = os.environ.get(\"H200_SSH_IDENTITY\")\n",
        "default_identity = Path.home() / \".ssh\" / \"id_rsa\"\n",
        "if _identity_env:\n",
        "    SSH_IDENTITY: Optional[Path] = Path(_identity_env).expanduser()\n",
        "elif default_identity.exists():\n",
        "    SSH_IDENTITY = default_identity\n",
        "else:\n",
        "    SSH_IDENTITY = None\n",
        "REMOTE_REPO = os.environ.get(\"H200_REMOTE_REPO\", \"/workspace/vastai-ssh-jupyter-pytorch\")\n",
        "LOCAL_OUTPUT_ROOT = Path(os.environ.get(\"H200_LOCAL_OUTPUT\", \"h200_long_outputs\")).resolve()\n",
        "LOCAL_OUTPUT_ROOT.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "PROMPT_SOURCE = REPO_ROOT / \"experiments\" / \"prompts\" / \"prod_prompts.txt\"\n",
        "PROMPT_VARIANTS_DIR = REPO_ROOT / \"notebooks\" / \"generated_prompts\"\n",
        "PROMPT_VARIANTS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "REMOTE_PROMPT_DIR = f\"{REMOTE_REPO}/experiments/prompts\"\n",
        "REMOTE_PYTHON = os.environ.get(\"H200_REMOTE_PYTHON\", \"python3\")\n",
        "PROD_CONFIG_REMOTE = f\"{REMOTE_REPO}/configs/prod_config.yaml\"\n",
        "PROD_CONFIG_BACKUP = f\"{REMOTE_REPO}/configs/prod_config.notebook.bak\"\n",
        "REMOTE_REPO_PATH = PurePosixPath(REMOTE_REPO)\n",
        "REMOTE_REPO_PARENT = str(REMOTE_REPO_PATH.parent)\n",
        "REMOTE_REPO_NAME = REMOTE_REPO_PATH.name\n",
        "REMOTE_REPO_URL = os.environ.get(\n",
        "    \"H200_REMOTE_REPO_URL\",\n",
        "    \"https://github.com/spencermcbridemoore/vastai-ssh-jupyter-pytorch.git\",\n",
        ")\n",
        "LOCAL_CONFIG_PATH = REPO_ROOT / \"configs\" / \"prod_config.yaml\"\n",
        "\n",
        "FALLBACK_TOKENIZER = os.environ.get(\"H200_FALLBACK_TOKENIZER\", \"Qwen/Qwen2.5-0.5B-Instruct\")\n",
        "PROMPT_VARIANT_ORDER = (\"full\", \"tok100\", \"tok50\")\n",
        "\n",
        "print(f\"Repo root: {REPO_ROOT}\")\n",
        "print(f\"Local outputs: {LOCAL_OUTPUT_ROOT}\")\n",
        "print(f\"Vast H200 instance {INSTANCE_ID}: ssh {SSH_USER}@{SSH_HOST} -p {SSH_PORT}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[ModelSpec(name='qwen-0_5b', base='Qwen/Qwen2.5-0.5B', sft='Qwen/Qwen2.5-0.5B-Instruct', tokenizer='Qwen/Qwen2.5-0.5B-Instruct', dtype='float16', device='cuda:0', notes='Tiny sanity check; fast to run.'),\n",
              " ModelSpec(name='qwen-1_5b', base='Qwen/Qwen2.5-1.5B', sft='Qwen/Qwen2.5-1.5B-Instruct', tokenizer='Qwen/Qwen2.5-1.5B-Instruct', dtype='float16', device='cuda:0', notes='Fits comfortably on a single H200 GPU.'),\n",
              " ModelSpec(name='qwen-3b', base='Qwen/Qwen2.5-3B', sft='Qwen/Qwen2.5-3B-Instruct', tokenizer='Qwen/Qwen2.5-3B-Instruct', dtype='bfloat16', device='cuda:0', notes='Baseline mid-size model.'),\n",
              " ModelSpec(name='qwen-7b', base='Qwen/Qwen2.5-7B', sft='Qwen/Qwen2.5-7B-Instruct', tokenizer='Qwen/Qwen2.5-7B-Instruct', dtype='bfloat16', device='cuda:0', notes='Primary comparison target.'),\n",
              " ModelSpec(name='qwen-14b', base='Qwen/Qwen2.5-14B', sft='Qwen/Qwen2.5-14B-Instruct', tokenizer='Qwen/Qwen2.5-14B-Instruct', dtype='bfloat16', device='cuda:0', notes='Largest Qwen2.5 variant that fits on H200.'),\n",
              " ModelSpec(name='mistral-minitron-8b', base='nvidia/Mistral-NeMo-Minitron-8B-Base', sft='nvidia/Mistral-NeMo-Minitron-8B-Instruct', tokenizer='nvidia/Mistral-NeMo-Minitron-8B-Instruct', dtype='bfloat16', device='cuda:0', notes='NVIDIA NeMo Minitron pair.')]"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "@dataclass(frozen=True)\n",
        "class ModelSpec:\n",
        "    name: str\n",
        "    base: str\n",
        "    sft: str\n",
        "    tokenizer: str\n",
        "    dtype: str = \"bfloat16\"\n",
        "    device: str = \"cuda:0\"\n",
        "    notes: str = \"\"\n",
        "\n",
        "\n",
        "MODEL_SPECS: List[ModelSpec] = [\n",
        "    ModelSpec(\n",
        "        name=\"qwen-0_5b\",\n",
        "        base=\"Qwen/Qwen2.5-0.5B\",\n",
        "        sft=\"Qwen/Qwen2.5-0.5B-Instruct\",\n",
        "        tokenizer=\"Qwen/Qwen2.5-0.5B-Instruct\",\n",
        "        dtype=\"float16\",\n",
        "        notes=\"Tiny sanity check; fast to run.\",\n",
        "    ),\n",
        "    ModelSpec(\n",
        "        name=\"qwen-1_5b\",\n",
        "        base=\"Qwen/Qwen2.5-1.5B\",\n",
        "        sft=\"Qwen/Qwen2.5-1.5B-Instruct\",\n",
        "        tokenizer=\"Qwen/Qwen2.5-1.5B-Instruct\",\n",
        "        dtype=\"float16\",\n",
        "        notes=\"Fits comfortably on a single H200 GPU.\",\n",
        "    ),\n",
        "    ModelSpec(\n",
        "        name=\"qwen-3b\",\n",
        "        base=\"Qwen/Qwen2.5-3B\",\n",
        "        sft=\"Qwen/Qwen2.5-3B-Instruct\",\n",
        "        tokenizer=\"Qwen/Qwen2.5-3B-Instruct\",\n",
        "        notes=\"Baseline mid-size model.\",\n",
        "    ),\n",
        "    ModelSpec(\n",
        "        name=\"qwen-7b\",\n",
        "        base=\"Qwen/Qwen2.5-7B\",\n",
        "        sft=\"Qwen/Qwen2.5-7B-Instruct\",\n",
        "        tokenizer=\"Qwen/Qwen2.5-7B-Instruct\",\n",
        "        notes=\"Primary comparison target.\",\n",
        "    ),\n",
        "    ModelSpec(\n",
        "        name=\"qwen-14b\",\n",
        "        base=\"Qwen/Qwen2.5-14B\",\n",
        "        sft=\"Qwen/Qwen2.5-14B-Instruct\",\n",
        "        tokenizer=\"Qwen/Qwen2.5-14B-Instruct\",\n",
        "        notes=\"Largest Qwen2.5 variant that fits on H200.\",\n",
        "    ),\n",
        "    ModelSpec(\n",
        "        name=\"mistral-minitron-8b\",\n",
        "        base=\"nvidia/Mistral-NeMo-Minitron-8B-Base\",\n",
        "        sft=\"nvidia/Mistral-NeMo-Minitron-8B-Instruct\",\n",
        "        tokenizer=\"nvidia/Mistral-NeMo-Minitron-8B-Instruct\",\n",
        "        notes=\"NVIDIA NeMo Minitron pair.\",\n",
        "    ),\n",
        "]\n",
        "\n",
        "MODEL_SPECS\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "JSON_PATH_RE = re.compile(r\"Wrote comparison JSON to:\\s*(.+)\")\n",
        "\n",
        "\n",
        "def _ssh_base() -> List[str]:\n",
        "    cmd = [\"ssh\", \"-p\", str(SSH_PORT)]\n",
        "    identity = None\n",
        "    if SSH_IDENTITY:\n",
        "        identity = Path(SSH_IDENTITY).expanduser()\n",
        "    if identity and identity.exists():\n",
        "        cmd += [\"-i\", str(identity)]\n",
        "    cmd.append(f\"{SSH_USER}@{SSH_HOST}\")\n",
        "    return cmd\n",
        "\n",
        "\n",
        "def _scp_base() -> List[str]:\n",
        "    cmd = [\"scp\", \"-P\", str(SSH_PORT)]\n",
        "    identity = None\n",
        "    if SSH_IDENTITY:\n",
        "        identity = Path(SSH_IDENTITY).expanduser()\n",
        "    if identity and identity.exists():\n",
        "        cmd += [\"-i\", str(identity)]\n",
        "    return cmd\n",
        "\n",
        "\n",
        "def run_remote(command: str, *, desc: Optional[str] = None, check: bool = True) -> subprocess.CompletedProcess:\n",
        "    if desc:\n",
        "        tqdm.write(desc)\n",
        "    full_cmd = _ssh_base() + [command]\n",
        "    result = subprocess.run(full_cmd, capture_output=True, text=True)\n",
        "    if check and result.returncode != 0:\n",
        "        raise RuntimeError(\n",
        "            f\"Remote command failed ({result.returncode})\\nSTDOUT:\\n{result.stdout}\\nSTDERR:\\n{result.stderr}\"\n",
        "        )\n",
        "    return result\n",
        "\n",
        "\n",
        "def scp_to_remote(local_path: Path, remote_path: str, *, desc: Optional[str] = None) -> None:\n",
        "    if desc:\n",
        "        tqdm.write(f\"Upload → {remote_path}: {local_path}\")\n",
        "    local_path = local_path.expanduser().resolve()\n",
        "    cmd = _scp_base() + [str(local_path), f\"{SSH_USER}@{SSH_HOST}:{remote_path}\"]\n",
        "    subprocess.run(cmd, check=True)\n",
        "\n",
        "\n",
        "def scp_from_remote(remote_path: str, local_path: Path, *, desc: Optional[str] = None) -> None:\n",
        "    if desc:\n",
        "        tqdm.write(f\"Download ← {remote_path} -> {local_path}\")\n",
        "    local_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    cmd = _scp_base() + [f\"{SSH_USER}@{SSH_HOST}:{remote_path}\", str(local_path)]\n",
        "    subprocess.run(cmd, check=True)\n",
        "\n",
        "\n",
        "def write_log(log_path: Path, stdout: str, stderr: str) -> None:\n",
        "    log_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    log_path.write_text(\n",
        "        stdout + (\"\\n--- stderr ---\\n\" + stderr if stderr else \"\"),\n",
        "        encoding=\"utf-8\",\n",
        "    )\n",
        "\n",
        "\n",
        "def format_env(env: Dict[str, str]) -> str:\n",
        "    return \" \".join(f\"{key}={shlex.quote(str(value))}\" for key, value in env.items())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def inspect_remote_repo(*, log: bool = True) -> Dict[str, object]:\n",
        "    script = textwrap.dedent(\n",
        "        f\"\"\"\n",
        "        from pathlib import Path\n",
        "        import json\n",
        "\n",
        "        workspace = Path('/workspace')\n",
        "        repo = Path(r\"{REMOTE_REPO}\")\n",
        "        configs_dir = repo / 'configs'\n",
        "        config_path = configs_dir / 'prod_config.yaml'\n",
        "\n",
        "        def list_dir(path):\n",
        "            if not path.exists():\n",
        "                return None\n",
        "            return sorted(f\"{{child.name}}/\" if child.is_dir() else child.name for child in path.iterdir())\n",
        "\n",
        "        info = {{\n",
        "            \"workspace_entries\": list_dir(workspace),\n",
        "            \"repo_exists\": repo.exists(),\n",
        "            \"repo_entries\": list_dir(repo),\n",
        "            \"configs_entries\": list_dir(configs_dir),\n",
        "            \"config_exists\": config_path.exists(),\n",
        "            \"repo_path\": str(repo),\n",
        "        }}\n",
        "        print(json.dumps(info))\n",
        "        \"\"\"\n",
        "    )\n",
        "    desc = \"Inspect remote filesystem\" if log else None\n",
        "    result = run_remote(\n",
        "        f\"{REMOTE_PYTHON} - <<'PY'\\n{script}\\nPY\",\n",
        "        desc=desc,\n",
        "        check=False,\n",
        "    )\n",
        "    info: Dict[str, object] = {}\n",
        "    stdout = (result.stdout or \"\").strip()\n",
        "    if stdout:\n",
        "        try:\n",
        "            info = json.loads(stdout)\n",
        "        except json.JSONDecodeError:\n",
        "            info = {}\n",
        "            if log:\n",
        "                tqdm.write(stdout)\n",
        "    if log and info:\n",
        "        tqdm.write(json.dumps(info, indent=2))\n",
        "    if result.stderr:\n",
        "        tqdm.write(\"--- stderr ---\\n\" + result.stderr)\n",
        "    return info\n",
        "\n",
        "\n",
        "def clone_repo_via_git() -> bool:\n",
        "    clone_script = textwrap.dedent(\n",
        "        f\"\"\"\n",
        "        set -euo pipefail\n",
        "        mkdir -p {REMOTE_REPO_PARENT}\n",
        "        cd {REMOTE_REPO_PARENT}\n",
        "        rm -rf {REMOTE_REPO_NAME}\n",
        "        git clone {REMOTE_REPO_URL} {REMOTE_REPO_NAME}\n",
        "        \"\"\"\n",
        "    )\n",
        "    result = run_remote(\n",
        "        f\"bash -lc \\\"{clone_script}\\\"\",\n",
        "        desc=\"Clone repo via git\",\n",
        "        check=False,\n",
        "    )\n",
        "    if result.returncode != 0:\n",
        "        tqdm.write(\"Git clone failed; stderr →\")\n",
        "        tqdm.write(result.stderr or \"(no stderr)\")\n",
        "    return result.returncode == 0\n",
        "\n",
        "\n",
        "def upload_repo_snapshot() -> None:\n",
        "    fd, tmp_path = tempfile.mkstemp(suffix=\".tar.gz\")\n",
        "    os.close(fd)\n",
        "    tmp_file = Path(tmp_path)\n",
        "    try:\n",
        "        with tarfile.open(tmp_file, \"w:gz\") as archive:\n",
        "            archive.add(str(REPO_ROOT), arcname=REMOTE_REPO_NAME)\n",
        "        remote_tar = f\"/tmp/{REMOTE_REPO_NAME}.tar.gz\"\n",
        "        scp_to_remote(tmp_file, remote_tar, desc=\"Upload repo snapshot\")\n",
        "        extract_script = textwrap.dedent(\n",
        "            f\"\"\"\n",
        "            set -euo pipefail\n",
        "            rm -rf {REMOTE_REPO}\n",
        "            mkdir -p {REMOTE_REPO_PARENT}\n",
        "            tar -xzf {remote_tar} -C {REMOTE_REPO_PARENT}\n",
        "            rm -f {remote_tar}\n",
        "            \"\"\"\n",
        "        )\n",
        "        run_remote(\n",
        "            f\"bash -lc \\\"{extract_script}\\\"\",\n",
        "            desc=\"Extract repo snapshot\",\n",
        "        )\n",
        "    finally:\n",
        "        tmp_file.unlink(missing_ok=True)\n",
        "\n",
        "\n",
        "def sync_prod_config_file() -> None:\n",
        "    if not LOCAL_CONFIG_PATH.exists():\n",
        "        raise FileNotFoundError(f\"Missing local config: {LOCAL_CONFIG_PATH}\")\n",
        "    run_remote(\n",
        "        f\"bash -lc \\\"mkdir -p {REMOTE_REPO}/configs\\\"\",\n",
        "        desc=\"Ensure remote configs dir\",\n",
        "    )\n",
        "    scp_to_remote(LOCAL_CONFIG_PATH, PROD_CONFIG_REMOTE, desc=\"Sync prod_config.yaml\")\n",
        "\n",
        "\n",
        "def validate_remote_config() -> bool:\n",
        "    script = textwrap.dedent(\n",
        "        f\"\"\"\n",
        "        import sys\n",
        "        import yaml\n",
        "        from pathlib import Path\n",
        "\n",
        "        config_path = Path(r\"{PROD_CONFIG_REMOTE}\")\n",
        "        if not config_path.exists():\n",
        "            print(\"prod_config.yaml missing\")\n",
        "            sys.exit(1)\n",
        "        try:\n",
        "            yaml.safe_load(config_path.read_text(encoding='utf-8'))\n",
        "        except Exception as exc:  # pylint: disable=broad-except\n",
        "            print(\"YAML error:\", exc)\n",
        "            sys.exit(1)\n",
        "        \"\"\"\n",
        "    )\n",
        "    result = run_remote(\n",
        "        f\"{REMOTE_PYTHON} - <<'PY'\\n{script}\\nPY\",\n",
        "        desc=\"Validate remote prod_config.yaml\",\n",
        "        check=False,\n",
        "    )\n",
        "    if result.returncode != 0 and result.stdout:\n",
        "        tqdm.write(result.stdout)\n",
        "    return result.returncode == 0\n",
        "\n",
        "\n",
        "def ensure_remote_repo() -> Dict[str, object]:\n",
        "    status = inspect_remote_repo(log=False)\n",
        "    if not status.get(\"repo_exists\"):\n",
        "        if not clone_repo_via_git():\n",
        "            tqdm.write(\"Falling back to local snapshot upload.\")\n",
        "            upload_repo_snapshot()\n",
        "        status = inspect_remote_repo(log=False)\n",
        "    if not status.get(\"config_exists\"):\n",
        "        tqdm.write(\"Remote prod_config.yaml missing; syncing local copy.\")\n",
        "        sync_prod_config_file()\n",
        "        status = inspect_remote_repo(log=False)\n",
        "    elif not validate_remote_config():\n",
        "        tqdm.write(\"Remote prod_config.yaml invalid; syncing local copy.\")\n",
        "        sync_prod_config_file()\n",
        "        validate_remote_config()\n",
        "    run_remote(\n",
        "        f\"bash -lc \\\"mkdir -p {REMOTE_PROMPT_DIR}\\\"\",\n",
        "        desc=\"Ensure remote prompt dir\",\n",
        "    )\n",
        "    return status\n",
        "\n",
        "\n",
        "def ensure_remote_ready() -> Dict[str, object]:\n",
        "    status = ensure_remote_repo()\n",
        "    tqdm.write(\"Remote repo status:\")\n",
        "    tqdm.write(json.dumps(status, indent=2))\n",
        "    return status\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ensure remote repo + prompt directory exists\n",
            "Inspect remote filesystem\n",
            "{\n",
            "  \"workspace_entries\": [\n",
            "    \".hf_home/\",\n",
            "    \".venv-backups/\",\n",
            "    \"vastai-ssh-jupyter-pytorch/\"\n",
            "  ],\n",
            "  \"repo_exists\": true,\n",
            "  \"repo_entries\": [\n",
            "    \".git/\",\n",
            "    \".gitignore\",\n",
            "    \"NEXT_STEPS.md\",\n",
            "    \"QUICKSTART.md\",\n",
            "    \"README.md\",\n",
            "    \"configs/\",\n",
            "    \"docs/\",\n",
            "    \"env.template\",\n",
            "    \"experiments/\",\n",
            "    \"h200_outputs_multi/\",\n",
            "    \"notebooks/\",\n",
            "    \"requirements.txt\",\n",
            "    \"scripts/\",\n",
            "    \"setup/\",\n",
            "    \"src/\",\n",
            "    \"tests/\"\n",
            "  ],\n",
            "  \"configs_entries\": [\n",
            "    \"dev_config.yaml\",\n",
            "    \"prod_config.notebook.bak\",\n",
            "    \"prod_config.yaml\"\n",
            "  ],\n",
            "  \"config_exists\": true,\n",
            "  \"repo_path\": \"/workspace/vastai-ssh-jupyter-pytorch\"\n",
            "}\n",
            "--- stderr ---\n",
            "Welcome to vast.ai. If authentication fails, try again after a few seconds, and double check your ssh key.\n",
            "Have fun!\n",
            "\n",
            "Verified remote repo checkout and prompt directory (with inspection).\n"
          ]
        }
      ],
      "source": [
        "def ensure_remote_repo() -> None:\n",
        "    script = textwrap.dedent(\n",
        "        f\"\"\"\n",
        "        import shutil\n",
        "        import subprocess\n",
        "        from pathlib import Path\n",
        "\n",
        "        repo_path = Path(r\"{REMOTE_REPO}\")\n",
        "        workspace = repo_path.parent\n",
        "        config_path = Path(r\"{PROD_CONFIG_REMOTE}\")\n",
        "        repo_url = \"https://github.com/spencermcbridemoore/vastai-ssh-jupyter-pytorch.git\"\n",
        "\n",
        "        def clone_repo() -> None:\n",
        "            workspace.mkdir(parents=True, exist_ok=True)\n",
        "            subprocess.run([\"git\", \"clone\", repo_url, str(repo_path)], check=True)\n",
        "\n",
        "        if not repo_path.exists():\n",
        "            clone_repo()\n",
        "        elif not config_path.exists():\n",
        "            print(\"prod_config.yaml missing; recloning repo\")\n",
        "            if repo_path.exists():\n",
        "                shutil.rmtree(repo_path)\n",
        "            clone_repo()\n",
        "\n",
        "        (repo_path / \"experiments\" / \"prompts\").mkdir(parents=True, exist_ok=True)\n",
        "        \"\"\"\n",
        "    )\n",
        "    run_remote(\n",
        "        f\"{REMOTE_PYTHON} - <<'PY'\\n{script}\\nPY\",\n",
        "        desc=\"Ensure remote repo + prompt directory exists\",\n",
        "    )\n",
        "\n",
        "\n",
        "def ensure_remote_ready() -> None:\n",
        "    ensure_remote_repo()\n",
        "    inspect_remote_repo()\n",
        "\n",
        "\n",
        "ensure_remote_ready()\n",
        "tqdm.write(\"Verified remote repo checkout and prompt directory (with inspection).\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wrote full prompt file → C:\\Users\\spenc\\Cursor Repos\\vastai-ssh-jupyter-pytorch\\notebooks\\generated_prompts\\prod_prompts_full.txt\n",
            "Wrote tok100 prompt file → C:\\Users\\spenc\\Cursor Repos\\vastai-ssh-jupyter-pytorch\\notebooks\\generated_prompts\\prod_prompts_tok100.txt\n",
            "Wrote tok50 prompt file → C:\\Users\\spenc\\Cursor Repos\\vastai-ssh-jupyter-pytorch\\notebooks\\generated_prompts\\prod_prompts_tok50.txt\n",
            "Upload → /workspace/vastai-ssh-jupyter-pytorch/experiments/prompts/prod_prompts_full.txt: C:\\Users\\spenc\\Cursor Repos\\vastai-ssh-jupyter-pytorch\\notebooks\\generated_prompts\\prod_prompts_full.txt\n",
            "Upload → /workspace/vastai-ssh-jupyter-pytorch/experiments/prompts/prod_prompts_tok100.txt: C:\\Users\\spenc\\Cursor Repos\\vastai-ssh-jupyter-pytorch\\notebooks\\generated_prompts\\prod_prompts_tok100.txt\n",
            "Upload → /workspace/vastai-ssh-jupyter-pytorch/experiments/prompts/prod_prompts_tok50.txt: C:\\Users\\spenc\\Cursor Repos\\vastai-ssh-jupyter-pytorch\\notebooks\\generated_prompts\\prod_prompts_tok50.txt\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'full': '/workspace/vastai-ssh-jupyter-pytorch/experiments/prompts/prod_prompts_full.txt',\n",
              " 'tok100': '/workspace/vastai-ssh-jupyter-pytorch/experiments/prompts/prod_prompts_tok100.txt',\n",
              " 'tok50': '/workspace/vastai-ssh-jupyter-pytorch/experiments/prompts/prod_prompts_tok50.txt'}"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "with PROMPT_SOURCE.open(\"r\", encoding=\"utf-8\") as handle:\n",
        "    RAW_PROMPTS = [line.strip() for line in handle if line.strip()]\n",
        "\n",
        "if len(RAW_PROMPTS) != 4:\n",
        "    raise ValueError(f\"Expected 4 prompts, found {len(RAW_PROMPTS)} in {PROMPT_SOURCE}\")\n",
        "\n",
        "base_tokenizer = AutoTokenizer.from_pretrained(FALLBACK_TOKENIZER)\n",
        "\n",
        "\n",
        "def truncate_prompt(text: str, max_tokens: Optional[int]) -> str:\n",
        "    if max_tokens is None:\n",
        "        return text\n",
        "    encoded = base_tokenizer(text, add_special_tokens=False)\n",
        "    ids = encoded[\"input_ids\"]\n",
        "    if len(ids) <= max_tokens:\n",
        "        return text\n",
        "    truncated = base_tokenizer.decode(\n",
        "        ids[:max_tokens],\n",
        "        skip_special_tokens=False,\n",
        "        clean_up_tokenization_spaces=False,\n",
        "    )\n",
        "    return truncated\n",
        "\n",
        "\n",
        "PROMPT_VARIANTS: Dict[str, List[str]] = {\n",
        "    \"full\": RAW_PROMPTS,\n",
        "    \"tok100\": [truncate_prompt(prompt, 100) for prompt in RAW_PROMPTS],\n",
        "    \"tok50\": [truncate_prompt(prompt, 50) for prompt in RAW_PROMPTS],\n",
        "}\n",
        "\n",
        "LOCAL_PROMPT_FILES: Dict[str, Path] = {}\n",
        "for label, prompts in PROMPT_VARIANTS.items():\n",
        "    local_path = PROMPT_VARIANTS_DIR / f\"prod_prompts_{label}.txt\"\n",
        "    with local_path.open(\"w\", encoding=\"utf-8\") as handle:\n",
        "        for prompt in prompts:\n",
        "            handle.write(prompt + \"\\n\")\n",
        "    LOCAL_PROMPT_FILES[label] = local_path\n",
        "    tqdm.write(f\"Wrote {label} prompt file → {local_path}\")\n",
        "\n",
        "REMOTE_PROMPT_FILES: Dict[str, str] = {}\n",
        "for label in PROMPT_VARIANT_ORDER:\n",
        "    local_path = LOCAL_PROMPT_FILES[label]\n",
        "    remote_path = f\"{REMOTE_PROMPT_DIR}/prod_prompts_{label}.txt\"\n",
        "    scp_to_remote(local_path, remote_path, desc=f\"Sync {label} prompts\")\n",
        "    REMOTE_PROMPT_FILES[label] = remote_path\n",
        "\n",
        "REMOTE_PROMPT_FILES\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ensure remote repo + prompt directory exists\n",
            "Apply no-fuzz config override\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "Remote command failed (1)\nSTDOUT:\n\nSTDERR:\nWelcome to vast.ai. If authentication fails, try again after a few seconds, and double check your ssh key.\nHave fun!\nTraceback (most recent call last):\n  File \"<stdin>\", line 10, in <module>\n  File \"/usr/local/lib/python3.12/dist-packages/yaml/__init__.py\", line 125, in safe_load\n    return load(stream, SafeLoader)\n           ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/yaml/__init__.py\", line 81, in load\n    return loader.get_single_data()\n           ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/yaml/constructor.py\", line 49, in get_single_data\n    node = self.get_single_node()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/yaml/composer.py\", line 36, in get_single_node\n    document = self.compose_document()\n               ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/yaml/composer.py\", line 55, in compose_document\n    node = self.compose_node(None, None)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/yaml/composer.py\", line 84, in compose_node\n    node = self.compose_mapping_node(anchor)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/yaml/composer.py\", line 133, in compose_mapping_node\n    item_value = self.compose_node(node, item_key)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/yaml/composer.py\", line 84, in compose_node\n    node = self.compose_mapping_node(anchor)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/yaml/composer.py\", line 133, in compose_mapping_node\n    item_value = self.compose_node(node, item_key)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/yaml/composer.py\", line 84, in compose_node\n    node = self.compose_mapping_node(anchor)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/yaml/composer.py\", line 127, in compose_mapping_node\n    while not self.check_event(MappingEndEvent):\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/yaml/parser.py\", line 98, in check_event\n    self.current_event = self.state()\n                         ^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/yaml/parser.py\", line 438, in parse_block_mapping_key\n    raise ParserError(\"while parsing a block mapping\", self.marks[-1],\nyaml.parser.ParserError: while parsing a block mapping\n  in \"<unicode string>\", line 111, column 5:\n        enabled: false\n        ^\nexpected <block end>, but found '-'\n  in \"<unicode string>\", line 132, column 5:\n        - name: \"shared_unembedding\"\n        ^\n",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 56\u001b[39m\n\u001b[32m     48\u001b[39m     ensure_remote_repo()\n\u001b[32m     49\u001b[39m     run_remote(\n\u001b[32m     50\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mcd \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mREMOTE_REPO\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m && \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mREMOTE_PYTHON\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m - <<\u001b[39m\u001b[33m'\u001b[39m\u001b[33mPY\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mCONFIG_RESTORE_SCRIPT\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mPY\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     51\u001b[39m         desc=\u001b[33m\"\u001b[39m\u001b[33mRestore original prod_config.yaml\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     52\u001b[39m         check=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     53\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m \u001b[43mapply_config_override\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     57\u001b[39m tqdm.write(\u001b[33m\"\u001b[39m\u001b[33mprod_config.yaml patched (prompt_variants=identity, embedding_variants=default).\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 41\u001b[39m, in \u001b[36mapply_config_override\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mapply_config_override\u001b[39m() -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     40\u001b[39m     ensure_remote_repo()\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m     \u001b[43mrun_remote\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcd \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mREMOTE_REPO\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m && \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mREMOTE_PYTHON\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m - <<\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mPY\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mCONFIG_PATCH_SCRIPT\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43mPY\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mApply no-fuzz config override\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 31\u001b[39m, in \u001b[36mrun_remote\u001b[39m\u001b[34m(command, desc, check)\u001b[39m\n\u001b[32m     29\u001b[39m result = subprocess.run(full_cmd, capture_output=\u001b[38;5;28;01mTrue\u001b[39;00m, text=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m check \u001b[38;5;129;01mand\u001b[39;00m result.returncode != \u001b[32m0\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m     32\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRemote command failed (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult.returncode\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mSTDOUT:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mresult.stdout\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mSTDERR:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mresult.stderr\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     33\u001b[39m     )\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
            "\u001b[31mRuntimeError\u001b[39m: Remote command failed (1)\nSTDOUT:\n\nSTDERR:\nWelcome to vast.ai. If authentication fails, try again after a few seconds, and double check your ssh key.\nHave fun!\nTraceback (most recent call last):\n  File \"<stdin>\", line 10, in <module>\n  File \"/usr/local/lib/python3.12/dist-packages/yaml/__init__.py\", line 125, in safe_load\n    return load(stream, SafeLoader)\n           ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/yaml/__init__.py\", line 81, in load\n    return loader.get_single_data()\n           ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/yaml/constructor.py\", line 49, in get_single_data\n    node = self.get_single_node()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/yaml/composer.py\", line 36, in get_single_node\n    document = self.compose_document()\n               ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/yaml/composer.py\", line 55, in compose_document\n    node = self.compose_node(None, None)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/yaml/composer.py\", line 84, in compose_node\n    node = self.compose_mapping_node(anchor)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/yaml/composer.py\", line 133, in compose_mapping_node\n    item_value = self.compose_node(node, item_key)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/yaml/composer.py\", line 84, in compose_node\n    node = self.compose_mapping_node(anchor)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/yaml/composer.py\", line 133, in compose_mapping_node\n    item_value = self.compose_node(node, item_key)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/yaml/composer.py\", line 84, in compose_node\n    node = self.compose_mapping_node(anchor)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/yaml/composer.py\", line 127, in compose_mapping_node\n    while not self.check_event(MappingEndEvent):\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/yaml/parser.py\", line 98, in check_event\n    self.current_event = self.state()\n                         ^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/yaml/parser.py\", line 438, in parse_block_mapping_key\n    raise ParserError(\"while parsing a block mapping\", self.marks[-1],\nyaml.parser.ParserError: while parsing a block mapping\n  in \"<unicode string>\", line 111, column 5:\n        enabled: false\n        ^\nexpected <block end>, but found '-'\n  in \"<unicode string>\", line 132, column 5:\n        - name: \"shared_unembedding\"\n        ^\n"
          ]
        }
      ],
      "source": [
        "CONFIG_PATCH_SCRIPT = textwrap.dedent(\n",
        "    f\"\"\"\n",
        "    import yaml\n",
        "    from pathlib import Path\n",
        "\n",
        "    config_path = Path(r\"{PROD_CONFIG_REMOTE}\")\n",
        "    backup_path = Path(r\"{PROD_CONFIG_BACKUP}\")\n",
        "    if not backup_path.exists():\n",
        "        backup_path.write_text(config_path.read_text(encoding='utf-8'), encoding='utf-8')\n",
        "\n",
        "    data = yaml.safe_load(config_path.read_text(encoding='utf-8'))\n",
        "    rc = data.get('residual_compare', {{}})\n",
        "    rc['prompt_variants'] = ['identity']\n",
        "    rc['prompt_variant_options'] = {{}}\n",
        "    rc['embedding_variants'] = [\n",
        "        {{\n",
        "            'name': 'default',\n",
        "            'base': {{'embedding_source': 'base', 'unembedding_source': 'base'}},\n",
        "            'sft': {{'embedding_source': 'sft', 'unembedding_source': 'sft'}},\n",
        "        }}\n",
        "    ]\n",
        "    data['residual_compare'] = rc\n",
        "    config_path.write_text(yaml.safe_dump(data, sort_keys=False), encoding='utf-8')\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "CONFIG_RESTORE_SCRIPT = textwrap.dedent(\n",
        "    f\"\"\"\n",
        "    from pathlib import Path\n",
        "\n",
        "    config_path = Path(r\"{PROD_CONFIG_REMOTE}\")\n",
        "    backup_path = Path(r\"{PROD_CONFIG_BACKUP}\")\n",
        "    if backup_path.exists():\n",
        "        config_path.write_text(backup_path.read_text(encoding='utf-8'), encoding='utf-8')\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "\n",
        "def apply_config_override() -> None:\n",
        "    ensure_remote_repo()\n",
        "    run_remote(\n",
        "        f\"cd {REMOTE_REPO} && {REMOTE_PYTHON} - <<'PY'\\n{CONFIG_PATCH_SCRIPT}\\nPY\",\n",
        "        desc=\"Apply no-fuzz config override\",\n",
        "    )\n",
        "\n",
        "\n",
        "def restore_config_override() -> None:\n",
        "    ensure_remote_repo()\n",
        "    run_remote(\n",
        "        f\"cd {REMOTE_REPO} && {REMOTE_PYTHON} - <<'PY'\\n{CONFIG_RESTORE_SCRIPT}\\nPY\",\n",
        "        desc=\"Restore original prod_config.yaml\",\n",
        "        check=False,\n",
        "    )\n",
        "\n",
        "\n",
        "apply_config_override()\n",
        "tqdm.write(\"prod_config.yaml patched (prompt_variants=identity, embedding_variants=default).\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def prefetch_models(specs: Iterable[ModelSpec]) -> None:\n",
        "    repos = set()\n",
        "    for spec in specs:\n",
        "        repos.update([spec.base, spec.sft, spec.tokenizer])\n",
        "    for repo in tqdm(sorted(repos), desc=\"Prefetching HF weights\", unit=\"repo\"):\n",
        "        script = textwrap.dedent(\n",
        "            f\"\"\"\n",
        "            from huggingface_hub import snapshot_download\n",
        "            snapshot_download('{repo}', repo_type='model', resume_download=True)\n",
        "            \"\"\"\n",
        "        )\n",
        "        run_remote(\n",
        "            f\"cd {REMOTE_REPO} && {REMOTE_PYTHON} - <<'PY'\\n{script}\\nPY\",\n",
        "            desc=f\"Cache {repo}\",\n",
        "            check=False,\n",
        "        )\n",
        "\n",
        "\n",
        "prefetch_models(MODEL_SPECS)\n",
        "tqdm.write(\"All requested models/tokenizers have been cached on the H200.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "BASE_ENV = {\n",
        "    \"DEV_MODE\": \"False\",\n",
        "    \"PYTHONUNBUFFERED\": \"1\",\n",
        "}\n",
        "\n",
        "\n",
        "def build_env(spec: ModelSpec, prompt_label: str) -> Dict[str, str]:\n",
        "    env = dict(BASE_ENV)\n",
        "    env.update(\n",
        "        {\n",
        "            \"RESIDUAL_BASE_MODEL\": spec.base,\n",
        "            \"RESIDUAL_SFT_MODEL\": spec.sft,\n",
        "            \"RESIDUAL_TOKENIZER\": spec.tokenizer,\n",
        "            \"RESIDUAL_DTYPE\": spec.dtype,\n",
        "            \"RESIDUAL_DEVICE\": spec.device,\n",
        "            \"RESIDUAL_PROMPT_FILE\": REMOTE_PROMPT_FILES[prompt_label],\n",
        "        }\n",
        "    )\n",
        "    return env\n",
        "\n",
        "\n",
        "def execute_residual_attempt(spec: ModelSpec, prompt_label: str) -> Dict[str, object]:\n",
        "    env_line = format_env(build_env(spec, prompt_label))\n",
        "    command = f\"cd {REMOTE_REPO} && {env_line} {REMOTE_PYTHON} experiments/base_vs_sft_residual.py\"\n",
        "    result = run_remote(command, desc=f\"{spec.name} [{prompt_label}]\", check=False)\n",
        "    match = JSON_PATH_RE.search(result.stdout)\n",
        "    json_path = match.group(1).strip() if match else None\n",
        "    return {\n",
        "        \"completed_process\": result,\n",
        "        \"json_path\": json_path,\n",
        "        \"prompt_label\": prompt_label,\n",
        "    }\n",
        "\n",
        "\n",
        "def download_artifacts(spec: ModelSpec, prompt_label: str, json_remote: str, result: subprocess.CompletedProcess) -> Dict[str, str]:\n",
        "    model_dir = LOCAL_OUTPUT_ROOT / spec.name\n",
        "    model_dir.mkdir(parents=True, exist_ok=True)\n",
        "    local_json = model_dir / f\"{Path(json_remote).name}\" if json_remote else model_dir / f\"{spec.name}_missing.json\"\n",
        "    scp_from_remote(json_remote, local_json, desc=f\"JSON → {spec.name}\")\n",
        "    log_path = model_dir / f\"{local_json.stem}_{prompt_label}.log\"\n",
        "    write_log(log_path, result.stdout, result.stderr)\n",
        "    return {\"json_local\": str(local_json), \"log_local\": str(log_path)}\n",
        "\n",
        "\n",
        "def run_with_fallbacks(spec: ModelSpec, prompts: Iterable[str] = PROMPT_VARIANT_ORDER) -> Dict[str, str]:\n",
        "    attempts = list(prompts)\n",
        "    for label in attempts:\n",
        "        outcome = execute_residual_attempt(spec, label)\n",
        "        result = outcome[\"completed_process\"]\n",
        "        if result.returncode == 0 and outcome[\"json_path\"]:\n",
        "            artifacts = download_artifacts(spec, label, outcome[\"json_path\"], result)\n",
        "            tqdm.write(f\"✅ {spec.name} succeeded with {label} prompts\")\n",
        "            return {\n",
        "                \"model\": spec.name,\n",
        "                \"prompt_variant\": label,\n",
        "                \"remote_json\": outcome[\"json_path\"],\n",
        "                **artifacts,\n",
        "            }\n",
        "        tqdm.write(\n",
        "            f\"⚠️ {spec.name} failed with {label} prompts (rc={result.returncode}). Trying next fallback...\"\n",
        "        )\n",
        "    raise RuntimeError(f\"All prompt variants failed for {spec.name}. Check remote logs for details.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "run_records: List[Dict[str, str]] = []\n",
        "\n",
        "for spec in tqdm(MODEL_SPECS, desc=\"Residual sweep\", unit=\"model\"):\n",
        "    try:\n",
        "        outcome = run_with_fallbacks(spec)\n",
        "        outcome[\"status\"] = \"ok\"\n",
        "    except Exception as exc:  # pylint: disable=broad-except\n",
        "        tqdm.write(f\"❌ {spec.name} failed: {exc}\")\n",
        "        outcome = {\"model\": spec.name, \"status\": \"failed\", \"error\": str(exc)}\n",
        "    run_records.append(outcome)\n",
        "\n",
        "summary_df = pd.DataFrame(run_records)\n",
        "summary_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run this cell after all experiments finish to restore prod_config.yaml\n",
        "restore_config_override()\n",
        "tqdm.write(\"prod_config.yaml restored from backup.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.insert(0, str(REPO_ROOT / \"src\"))\n",
        "\n",
        "from src.analysis.residual_results import iter_results, summarize_file  # type: ignore\n",
        "\n",
        "json_paths = sorted(LOCAL_OUTPUT_ROOT.rglob(\"residual_compare_*.json\"))\n",
        "if not json_paths:\n",
        "    raise RuntimeError(f\"No residual_compare JSON files found in {LOCAL_OUTPUT_ROOT}\")\n",
        "\n",
        "summaries = []\n",
        "for path in json_paths:\n",
        "    summary = summarize_file(path)\n",
        "    summaries.append(\n",
        "        {\n",
        "            \"model\": path.parent.name,\n",
        "            \"json_path\": str(path),\n",
        "            \"num_results\": summary.num_results,\n",
        "            \"avg_tokens\": summary.avg_tokens,\n",
        "        }\n",
        "    )\n",
        "\n",
        "summary_table = pd.DataFrame(summaries)\n",
        "summary_table\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "vastai-ssh-jupyter-pytorch-env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
